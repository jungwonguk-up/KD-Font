apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: korea-diffusion-train-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.16, pipelines.kubeflow.org/pipeline_compilation_time: '2023-11-13T11:53:06.149700',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "An example pipeline that
      performs arithmetic calculations.", "name": "korea diffusion train pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.16}
spec:
  entrypoint: korea-diffusion-train-pipeline
  templates:
  - name: creating-marfile
    container:
      args: [-c, 'cd pvc/torch_model; pip install torchserve torch-model-archiver
          torch-workflow-archiver; torch-model-archiver --model-name torch-model --version
          1.0 --serialized-file pytorch_model.bin --handler handler.py --extra-files
          config.json,vocab.txt --force; mkdir model-store; mv -f torch-model.mar
          model-store']
      command: [/bin/sh]
      image: python:3.9
      volumeMounts:
      - {mountPath: pvc, name: test-lee}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Make Mar file for torchserve}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.16
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
    volumes:
    - name: test-lee
      persistentVolumeClaim: {claimName: diffusion}
  - name: korea-diffusion-train-pipeline
    dag:
      tasks:
      - name: creating-marfile
        template: creating-marfile
        dependencies: [make-mar-handle]
      - {name: make-mar-handle, template: make-mar-handle}
      - name: serve-a-model-with-kserve
        template: serve-a-model-with-kserve
        dependencies: [creating-marfile]
  - name: make-mar-handle
    container:
      args: []
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def make_mar_handle():
            import os
            import json
            os.mkdir('model-store')
            config_json = json.dumps({\
                    "diffusion_serve_test": {\
                        "1.0": {\
                            "defaultVersion": true,\
                            "marName": "diffusion_serve_test.mar",\
                            "minWorkers": 2,\
                            "maxWorkers": 12,\
                            "batchSize": 25,\
                            "maxBatchDelay": 1000,\
                            "responseTimeout": 1200\
                            }\
                        }\
                    }
                )
            config = {
                "inference_address":"http://192.168.0.80:9334",
                "management_address":"http://192.168.0.80:9335",
                "metrics_address":"http://192.168.0.80:9336",
                "enable_envvars_config":True,
                "model_store":"/home/hojun/Documents/code/kubeflow/model-store",
                "install_py_dep_per_model":True,
                "models": config_json
            }
            if not os.path.exists("pvc/model-store/config"):
                os.mkdir("pvc/model-store/config")
            with open("pvc/model-store/config/config.properties", "w") as f:
                for i, j in config.items():
                    f.write(f"{i}={j}\n")
                f.close()
            x = '''
            from ts.torch_handler.base_handler import BaseHandler
            import os
            import tqdm
            import math
            import random
            import json
            import pandas as pd
            import numpy as np
            from PIL import Image

            import torch, torchvision
            from torch import optim
            import torch.nn as nn
            import torch.nn.functional as F
            from torch.utils.data import DataLoader, TensorDataset,Dataset
            from torchvision.transforms.functional import to_pil_image

            from model import UNet, Diffusion, CharAttar
            from functools import partial
            from utils import load_yaml

            from PIL import Image

            os.environ['CUDA_VISIBLE_DEVICES'] = str(0)
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

            class DiffusionFontGenerateHandler(BaseHandler):#why use BaseHandler and abc
                def __init__(self):
                    super(DiffusionFontGenerateHandler,self).__init__()
                    self.config = load_yaml("config.yaml")
                    self.initialized = False
                    self.device = f"cuda:{self.config['gpu_num']}"

                def initialize(self,context):
                    input_size = 64
                    self.manifest = context.manifest
                    properties = context.system_properties
                    model_dir = properties.get("model_dir")
                    serialized_file = self.manifest['model']['serializedFile']
                    model_pt_path = os.path.join(model_dir, serialized_file)
                    if not os.path.isfile(model_pt_path):
                        raise RuntimeError("Missing the model.pt file")

                    init_model = UNet().to(self.device)
                    ckpt = torch.load(model_pt_path)
                    init_model.load_state_dict(ckpt)
                    self.model = init_model

                    self.diffusion = Diffusion(first_beta=1e-4,
                                        end_beta=0.02,
                                        noise_step=1000,
                                        beta_schedule_type='linear',
                                        img_size=input_size,
                                        device=self.device)

                    self.initialized = True
                def preprocess(self,sample_img_path,contents_ch):
                    transforms = torchvision.transforms.Compose([
                        # torchvision.transforms.Resize((input_size,input_size)),
                        torchvision.transforms.Grayscale(num_output_channels=1),
                        torchvision.transforms.ToTensor(),
                        torchvision.transforms.Normalize((0.5), (0.5))
                    ])
                    sampleImage_len = len(contents_ch)

                    # print(data)
                    sample_img = Image.open(sample_img_path)
                    sample_img = transforms(sample_img).to(self.device)
                    sample_img = torch.unsqueeze(sample_img,1)
                    sample_img = sample_img.repeat(sampleImage_len, 1, 1, 1)
                    # print(len(sample_img))
                    return sample_img

                def inference(self,sample_img,contents_ch,id):
                    save_path = "./data"

                    charAttar = CharAttar(num_classes=self.config['num_classes'],device=self.device,style_path=self.config['style_path'])
                    x = self.diffusion.portion_sampling(model=self.model,sampling_chars=contents_ch,charAttar=charAttar,sample_img=sample_img,batch_size=4)
                    os.makedirs(save_path,exist_ok=True)
                    for img,ch in zip(x,contents_ch):
                        pillow_img = to_pil_image(img)
                        pillow_img.save(os.path.join(save_path,id)+f"_{ch}.png")

                    return x

            _service = DiffusionFontGenerateHandler()

            def handle(data,context):
                try:
                    if not _service.initialized:
                        _service.initialize(context)
                    if data is None:
                        return None
                    print(data)
                    print(data[0]['body'])
                    data = data[0]['body']['inputs']
                    sample_img_path = data["cropped_img_path"]
                    id = data["id"]
                    contents_ch = data["text"]

                    sample_img = _service.preprocess(sample_img_path=sample_img_path,contents_ch=contents_ch)
                    data = _service.inference(sample_img,contents_ch,id)
                    return [data.tolist()]

                except Exception as e:
                    raise e

            '''
            with open("pvc/model-store/handler.py", "w") as f:
                f.write(x)
            f.close()
            print("Saving handler.py complete !!")

        import argparse
        _parser = argparse.ArgumentParser(prog='Make mar handle', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = make_mar_handle(**_parsed_args)
      image: python:3.10
      volumeMounts:
      - {mountPath: pvc, name: test-lee}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Make A hanlder file
          & config.properties file, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": [], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def make_mar_handle():\n    import os\n    import json\n    os.mkdir(''model-store'')\n    config_json
          = json.dumps({\\\n            \"diffusion_serve_test\": {\\\n                \"1.0\":
          {\\\n                    \"defaultVersion\": true,\\\n                    \"marName\":
          \"diffusion_serve_test.mar\",\\\n                    \"minWorkers\": 2,\\\n                    \"maxWorkers\":
          12,\\\n                    \"batchSize\": 25,\\\n                    \"maxBatchDelay\":
          1000,\\\n                    \"responseTimeout\": 1200\\\n                    }\\\n                }\\\n            }\n        )\n    config
          = {\n        \"inference_address\":\"http://192.168.0.80:9334\",\n        \"management_address\":\"http://192.168.0.80:9335\",\n        \"metrics_address\":\"http://192.168.0.80:9336\",\n        \"enable_envvars_config\":True,\n        \"model_store\":\"/home/hojun/Documents/code/kubeflow/model-store\",\n        \"install_py_dep_per_model\":True,\n        \"models\":
          config_json\n    }\n    if not os.path.exists(\"pvc/model-store/config\"):\n        os.mkdir(\"pvc/model-store/config\")\n    with
          open(\"pvc/model-store/config/config.properties\", \"w\") as f:\n        for
          i, j in config.items():\n            f.write(f\"{i}={j}\\n\")\n        f.close()\n    x
          = ''''''\n    from ts.torch_handler.base_handler import BaseHandler\n    import
          os\n    import tqdm\n    import math\n    import random\n    import json\n    import
          pandas as pd\n    import numpy as np\n    from PIL import Image\n\n    import
          torch, torchvision\n    from torch import optim\n    import torch.nn as
          nn\n    import torch.nn.functional as F\n    from torch.utils.data import
          DataLoader, TensorDataset,Dataset\n    from torchvision.transforms.functional
          import to_pil_image\n\n    from model import UNet, Diffusion, CharAttar\n    from
          functools import partial\n    from utils import load_yaml\n\n    from PIL
          import Image\n\n    os.environ[''CUDA_VISIBLE_DEVICES''] = str(0)\n    device
          = torch.device(''cuda'' if torch.cuda.is_available() else ''cpu'')\n\n    class
          DiffusionFontGenerateHandler(BaseHandler):#why use BaseHandler and abc\n        def
          __init__(self):\n            super(DiffusionFontGenerateHandler,self).__init__()\n            self.config
          = load_yaml(\"config.yaml\")\n            self.initialized = False\n            self.device
          = f\"cuda:{self.config[''gpu_num'']}\"\n\n        def initialize(self,context):\n            input_size
          = 64\n            self.manifest = context.manifest\n            properties
          = context.system_properties\n            model_dir = properties.get(\"model_dir\")\n            serialized_file
          = self.manifest[''model''][''serializedFile'']\n            model_pt_path
          = os.path.join(model_dir, serialized_file)\n            if not os.path.isfile(model_pt_path):\n                raise
          RuntimeError(\"Missing the model.pt file\")\n\n            init_model =
          UNet().to(self.device)\n            ckpt = torch.load(model_pt_path)\n            init_model.load_state_dict(ckpt)\n            self.model
          = init_model\n\n            self.diffusion = Diffusion(first_beta=1e-4,\n                                end_beta=0.02,\n                                noise_step=1000,\n                                beta_schedule_type=''linear'',\n                                img_size=input_size,\n                                device=self.device)\n\n            self.initialized
          = True\n        def preprocess(self,sample_img_path,contents_ch):\n            transforms
          = torchvision.transforms.Compose([\n                # torchvision.transforms.Resize((input_size,input_size)),\n                torchvision.transforms.Grayscale(num_output_channels=1),\n                torchvision.transforms.ToTensor(),\n                torchvision.transforms.Normalize((0.5),
          (0.5))\n            ])\n            sampleImage_len = len(contents_ch)\n\n            #
          print(data)\n            sample_img = Image.open(sample_img_path)\n            sample_img
          = transforms(sample_img).to(self.device)\n            sample_img = torch.unsqueeze(sample_img,1)\n            sample_img
          = sample_img.repeat(sampleImage_len, 1, 1, 1)\n            # print(len(sample_img))\n            return
          sample_img\n\n        def inference(self,sample_img,contents_ch,id):\n            save_path
          = \"./data\"\n\n            charAttar = CharAttar(num_classes=self.config[''num_classes''],device=self.device,style_path=self.config[''style_path''])\n            x
          = self.diffusion.portion_sampling(model=self.model,sampling_chars=contents_ch,charAttar=charAttar,sample_img=sample_img,batch_size=4)\n            os.makedirs(save_path,exist_ok=True)\n            for
          img,ch in zip(x,contents_ch):\n                pillow_img = to_pil_image(img)\n                pillow_img.save(os.path.join(save_path,id)+f\"_{ch}.png\")\n\n            return
          x\n\n    _service = DiffusionFontGenerateHandler()\n\n    def handle(data,context):\n        try:\n            if
          not _service.initialized:\n                _service.initialize(context)\n            if
          data is None:\n                return None\n            print(data)\n            print(data[0][''body''])\n            data
          = data[0][''body''][''inputs'']\n            sample_img_path = data[\"cropped_img_path\"]\n            id
          = data[\"id\"]\n            contents_ch = data[\"text\"]\n\n            sample_img
          = _service.preprocess(sample_img_path=sample_img_path,contents_ch=contents_ch)\n            data
          = _service.inference(sample_img,contents_ch,id)\n            return [data.tolist()]\n\n        except
          Exception as e:\n            raise e\n\n    ''''''\n    with open(\"pvc/model-store/handler.py\",
          \"w\") as f:\n        f.write(x)\n    f.close()\n    print(\"Saving handler.py
          complete !!\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Make
          mar handle'', description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = make_mar_handle(**_parsed_args)\n"], "image": "python:3.10"}}, "name":
          "Make mar handle"}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.16
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
    volumes:
    - name: test-lee
      persistentVolumeClaim: {claimName: diffusion}
  - name: serve-a-model-with-kserve
    container:
      args:
      - -u
      - kservedeployer.py
      - --action
      - apply
      - --model-name
      - diffusion_serve
      - --model-uri
      - pvc://diffusion/model-store
      - --canary-traffic-percent
      - '100'
      - --namespace
      - kubeflow-user-example-com
      - --framework
      - pytorch
      - --runtime-version
      - latest
      - --resource-requests
      - '{"cpu": "0.5", "memory": "512Mi"}'
      - --resource-limits
      - '{"cpu": "1", "memory": "1Gi"}'
      - --custom-model-spec
      - '{}'
      - --autoscaling-target
      - '0'
      - --service-account
      - ''
      - --enable-istio-sidecar
      - "True"
      - --output-path
      - /tmp/outputs/InferenceService_Status/data
      - --inferenceservice-yaml
      - '{}'
      - --watch-timeout
      - '300'
      - --min-replicas
      - '-1'
      - --max-replicas
      - '-1'
      - --request-timeout
      - '60'
      - --enable-isvc-status
      - "True"
      command: [python]
      image: quay.io/aipipeline/kserve-component:v0.11.1
      volumeMounts:
      - {mountPath: pvc, name: test-lee}
    outputs:
      artifacts:
      - {name: serve-a-model-with-kserve-InferenceService-Status, path: /tmp/outputs/InferenceService_Status/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.16
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Serve
          Models using KServe", "implementation": {"container": {"args": ["-u", "kservedeployer.py",
          "--action", {"inputValue": "Action"}, "--model-name", {"inputValue": "Model
          Name"}, "--model-uri", {"inputValue": "Model URI"}, "--canary-traffic-percent",
          {"inputValue": "Canary Traffic Percent"}, "--namespace", {"inputValue":
          "Namespace"}, "--framework", {"inputValue": "Framework"}, "--runtime-version",
          {"inputValue": "Runtime Version"}, "--resource-requests", {"inputValue":
          "Resource Requests"}, "--resource-limits", {"inputValue": "Resource Limits"},
          "--custom-model-spec", {"inputValue": "Custom Model Spec"}, "--autoscaling-target",
          {"inputValue": "Autoscaling Target"}, "--service-account", {"inputValue":
          "Service Account"}, "--enable-istio-sidecar", {"inputValue": "Enable Istio
          Sidecar"}, "--output-path", {"outputPath": "InferenceService Status"}, "--inferenceservice-yaml",
          {"inputValue": "InferenceService YAML"}, "--watch-timeout", {"inputValue":
          "Watch Timeout"}, "--min-replicas", {"inputValue": "Min Replicas"}, "--max-replicas",
          {"inputValue": "Max Replicas"}, "--request-timeout", {"inputValue": "Request
          Timeout"}, "--enable-isvc-status", {"inputValue": "Enable ISVC Status"}],
          "command": ["python"], "image": "quay.io/aipipeline/kserve-component:v0.11.1"}},
          "inputs": [{"default": "create", "description": "Action to execute on KServe",
          "name": "Action", "type": "String"}, {"default": "", "description": "Name
          to give to the deployed model", "name": "Model Name", "type": "String"},
          {"default": "", "description": "Path of the S3 or GCS compatible directory
          containing the model.", "name": "Model URI", "type": "String"}, {"default":
          "100", "description": "The traffic split percentage between the candidate
          model and the last ready model", "name": "Canary Traffic Percent", "type":
          "String"}, {"default": "", "description": "Kubernetes namespace where the
          KServe service is deployed.", "name": "Namespace", "type": "String"}, {"default":
          "", "description": "Machine Learning Framework for Model Serving.", "name":
          "Framework", "type": "String"}, {"default": "latest", "description": "Runtime
          Version of Machine Learning Framework", "name": "Runtime Version", "type":
          "String"}, {"default": "{\"cpu\": \"0.5\", \"memory\": \"512Mi\"}", "description":
          "CPU and Memory requests for Model Serving", "name": "Resource Requests",
          "type": "String"}, {"default": "{\"cpu\": \"1\", \"memory\": \"1Gi\"}",
          "description": "CPU and Memory limits for Model Serving", "name": "Resource
          Limits", "type": "String"}, {"default": "{}", "description": "Custom model
          runtime container spec in JSON", "name": "Custom Model Spec", "type": "String"},
          {"default": "0", "description": "Autoscaling Target Number", "name": "Autoscaling
          Target", "type": "String"}, {"default": "", "description": "ServiceAccount
          to use to run the InferenceService pod", "name": "Service Account", "type":
          "String"}, {"default": "True", "description": "Whether to enable istio sidecar
          injection", "name": "Enable Istio Sidecar", "type": "Bool"}, {"default":
          "{}", "description": "Raw InferenceService serialized YAML for deployment",
          "name": "InferenceService YAML", "type": "String"}, {"default": "300", "description":
          "Timeout seconds for watching until InferenceService becomes ready.", "name":
          "Watch Timeout", "type": "String"}, {"default": "-1", "description": "Minimum
          number of InferenceService replicas", "name": "Min Replicas", "type": "String"},
          {"default": "-1", "description": "Maximum number of InferenceService replicas",
          "name": "Max Replicas", "type": "String"}, {"default": "60", "description":
          "Specifies the number of seconds to wait before timing out a request to
          the component.", "name": "Request Timeout", "type": "String"}, {"default":
          "True", "description": "Specifies whether to store the inference service
          status as the output parameter", "name": "Enable ISVC Status", "type": "Bool"}],
          "name": "Serve a model with KServe", "outputs": [{"description": "Status
          JSON output of InferenceService", "name": "InferenceService Status", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest": "b0379af21c170410b8b1a9606cb6cad63f99b0af53c2a5c1f0af397b53c81cd7",
          "url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kserve/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"Action": "apply", "Autoscaling
          Target": "0", "Canary Traffic Percent": "100", "Custom Model Spec": "{}",
          "Enable ISVC Status": "True", "Enable Istio Sidecar": "True", "Framework":
          "pytorch", "InferenceService YAML": "{}", "Max Replicas": "-1", "Min Replicas":
          "-1", "Model Name": "diffusion_serve", "Model URI": "pvc://diffusion/model-store",
          "Namespace": "kubeflow-user-example-com", "Request Timeout": "60", "Resource
          Limits": "{\"cpu\": \"1\", \"memory\": \"1Gi\"}", "Resource Requests": "{\"cpu\":
          \"0.5\", \"memory\": \"512Mi\"}", "Runtime Version": "latest", "Service
          Account": "", "Watch Timeout": "300"}'}
    volumes:
    - name: test-lee
      persistentVolumeClaim: {claimName: diffusion}
  arguments:
    parameters: []
  serviceAccountName: pipeline-runner
