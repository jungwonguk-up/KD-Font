apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: korea-diffusion-train-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.16, pipelines.kubeflow.org/pipeline_compilation_time: '2023-11-13T11:53:03.873529',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "An example pipeline that
      performs arithmetic calculations.", "inputs": [{"name": "image_size", "type":
      "Integer"}, {"name": "result_path", "type": "String"}, {"name": "csv_path",
      "type": "String"}, {"name": "gpu_num", "type": "Integer"}, {"name": "batch_size",
      "type": "Integer"}, {"name": "n_epochs", "type": "Integer"}, {"name": "num_classes",
      "type": "Integer"}, {"name": "mode", "type": "Integer"}, {"name": "lr", "type":
      "Float"}, {"name": "result_model_path", "type": "String"}, {"name": "style_path",
      "type": "String"}], "name": "korea diffusion train pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.16}
spec:
  entrypoint: korea-diffusion-train-pipeline
  templates:
  - name: condition-Best-Model-1
    dag:
      tasks:
      - name: creating-marfile
        template: creating-marfile
        dependencies: [make-handle]
      - {name: make-handle, template: make-handle}
      - name: serve-a-model-with-kserve
        template: serve-a-model-with-kserve
        dependencies: [creating-marfile]
  - name: condition-Not-a-Best-Model-2
    dag:
      tasks:
      - {name: continous-learning-end, template: continous-learning-end}
  - name: continous-learning-end
    container:
      args: []
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def continous_learning_end():
            pass

        import argparse
        _parser = argparse.ArgumentParser(prog='Continous learning end', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = continous_learning_end(**_parsed_args)
      image: python:3.10
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.16
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          continous_learning_end():\n    pass\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Continous
          learning end'', description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = continous_learning_end(**_parsed_args)\n"], "image": "python:3.10"}},
          "name": "Continous learning end"}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: creating-marfile
    container:
      args: [-c, 'cd pvc/torch_model; pip install torchserve torch-model-archiver
          torch-workflow-archiver; torch-model-archiver --model-name torch-model --version
          1.0 --serialized-file pytorch_model.bin --handler handler.py --extra-files
          config.json,vocab.txt --force; mkdir model-store; mv -f torch-model.mar
          model-store']
      command: [/bin/sh]
      image: python:3.9
      volumeMounts:
      - {mountPath: pvc, name: test-lee}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Make Mar file for torchserve}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.16
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
    volumes:
    - name: test-lee
      persistentVolumeClaim: {claimName: diffusion}
  - name: korea-diffusion-train-pipeline
    inputs:
      parameters:
      - {name: batch_size}
      - {name: csv_path}
      - {name: gpu_num}
      - {name: image_size}
      - {name: lr}
      - {name: mode}
      - {name: n_epochs}
      - {name: num_classes}
      - {name: result_model_path}
      - {name: result_path}
      - {name: style_path}
    dag:
      tasks:
      - name: condition-Best-Model-1
        template: condition-Best-Model-1
        when: '"{{tasks.model-evalute.outputs.parameters.model-evalute-Output}}" ==
          "True"'
        dependencies: [model-evalute]
      - name: condition-Not-a-Best-Model-2
        template: condition-Not-a-Best-Model-2
        when: '"{{tasks.model-evalute.outputs.parameters.model-evalute-Output}}" ==
          "False"'
        dependencies: [model-evalute]
      - name: make-dataset-csv
        template: make-dataset-csv
        arguments:
          parameters:
          - {name: csv_path, value: '{{inputs.parameters.csv_path}}'}
          - {name: result_path, value: '{{inputs.parameters.result_path}}'}
      - name: model-evalute
        template: model-evalute
        dependencies: [model-training]
        arguments:
          parameters:
          - {name: model-training-Output, value: '{{tasks.model-training.outputs.parameters.model-training-Output}}'}
          - {name: result_model_path, value: '{{inputs.parameters.result_model_path}}'}
      - name: model-training
        template: model-training
        dependencies: [make-dataset-csv]
        arguments:
          parameters:
          - {name: batch_size, value: '{{inputs.parameters.batch_size}}'}
          - {name: gpu_num, value: '{{inputs.parameters.gpu_num}}'}
          - {name: image_size, value: '{{inputs.parameters.image_size}}'}
          - {name: lr, value: '{{inputs.parameters.lr}}'}
          - {name: mode, value: '{{inputs.parameters.mode}}'}
          - {name: n_epochs, value: '{{inputs.parameters.n_epochs}}'}
          - {name: num_classes, value: '{{inputs.parameters.num_classes}}'}
          - {name: result_model_path, value: '{{inputs.parameters.result_model_path}}'}
          - {name: style_path, value: '{{inputs.parameters.style_path}}'}
  - name: make-dataset-csv
    container:
      args: [--csv-path, '{{inputs.parameters.csv_path}}', --file-folder-path, '{{inputs.parameters.result_path}}']
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==2.0.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==2.0.0' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def make_dataset_csv(csv_path, file_folder_path):
            import os
            import pandas as pd
            file_list = os.listdir(file_folder_path)
            file_information = []
            for file_name in file_list:
                file_ch = file_name.split("_")
                file_path = os.path.join(file_folder_path)
                file_information.append([file_name,file_path,file_ch])
            file_pd = pd.DataFrame(file_information)
            file_pd.to_csv(csv_path)

        import argparse
        _parser = argparse.ArgumentParser(prog='Make dataset csv', description='')
        _parser.add_argument("--csv-path", dest="csv_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--file-folder-path", dest="file_folder_path", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = make_dataset_csv(**_parsed_args)
      image: python:3.10
      volumeMounts:
      - {mountPath: pvc, name: test-lee}
    inputs:
      parameters:
      - {name: csv_path}
      - {name: result_path}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Make a train.csv file,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--csv-path", {"inputValue": "csv_path"}, "--file-folder-path", {"inputValue":
          "file_folder_path"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==2.0.0''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pandas==2.0.0'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def make_dataset_csv(csv_path, file_folder_path):\n    import os\n    import
          pandas as pd\n    file_list = os.listdir(file_folder_path)\n    file_information
          = []\n    for file_name in file_list:\n        file_ch = file_name.split(\"_\")\n        file_path
          = os.path.join(file_folder_path)\n        file_information.append([file_name,file_path,file_ch])\n    file_pd
          = pd.DataFrame(file_information)\n    file_pd.to_csv(csv_path)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Make dataset csv'', description='''')\n_parser.add_argument(\"--csv-path\",
          dest=\"csv_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-folder-path\",
          dest=\"file_folder_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = make_dataset_csv(**_parsed_args)\n"],
          "image": "python:3.10"}}, "inputs": [{"name": "csv_path", "type": "String"},
          {"name": "file_folder_path", "type": "String"}], "name": "Make dataset csv"}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"csv_path":
          "{{inputs.parameters.csv_path}}", "file_folder_path": "{{inputs.parameters.result_path}}"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.16
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
    volumes:
    - name: test-lee
      persistentVolumeClaim: {claimName: diffusion}
  - name: make-handle
    container:
      args: []
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def make_handle():
            import os
            import json
            os.mkdir('model-store')
            config_json = json.dumps({\
                    "diffusion_serve_test": {\
                        "1.0": {\
                            "defaultVersion": true,\
                            "marName": "diffusion_serve_test.mar",\
                            "minWorkers": 2,\
                            "maxWorkers": 12,\
                            "batchSize": 25,\
                            "maxBatchDelay": 1000,\
                            "responseTimeout": 1200\
                            }\
                        }\
                    }
                )
            config = {
                "inference_address":"http://192.168.0.80:9334",
                "management_address":"http://192.168.0.80:9335",
                "metrics_address":"http://192.168.0.80:9336",
                "enable_envvars_config":True,
                "model_store":"/home/hojun/Documents/code/kubeflow/model-store",
                "install_py_dep_per_model":True,
                "models": config_json
            }
            if not os.path.exists("pvc/model-store/config"):
                os.mkdir("pvc/model-store/config")
            with open("pvc/model-store/config/config.properties", "w") as f:
                for i, j in config.items():
                    f.write(f"{i}={j}\n")
                f.close()
            x = '''
            from ts.torch_handler.base_handler import BaseHandler
            import os
            import tqdm
            import math
            import random
            import json
            import pandas as pd
            import numpy as np
            from PIL import Image

            import torch, torchvision
            from torch import optim
            import torch.nn as nn
            import torch.nn.functional as F
            from torch.utils.data import DataLoader, TensorDataset,Dataset
            from torchvision.transforms.functional import to_pil_image

            from model import UNet, Diffusion, CharAttar
            from functools import partial
            from utils import load_yaml

            from PIL import Image

            os.environ['CUDA_VISIBLE_DEVICES'] = str(0)
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

            class DiffusionFontGenerateHandler(BaseHandler):#why use BaseHandler and abc
                def __init__(self):
                    super(DiffusionFontGenerateHandler,self).__init__()
                    self.config = load_yaml("config.yaml")
                    self.initialized = False
                    self.device = f"cuda:{self.config['gpu_num']}"

                def initialize(self,context):
                    input_size = 64
                    self.manifest = context.manifest
                    properties = context.system_properties
                    model_dir = properties.get("model_dir")
                    serialized_file = self.manifest['model']['serializedFile']
                    model_pt_path = os.path.join(model_dir, serialized_file)
                    if not os.path.isfile(model_pt_path):
                        raise RuntimeError("Missing the model.pt file")

                    init_model = UNet().to(self.device)
                    ckpt = torch.load(model_pt_path)
                    init_model.load_state_dict(ckpt)
                    self.model = init_model

                    self.diffusion = Diffusion(first_beta=1e-4,
                                        end_beta=0.02,
                                        noise_step=1000,
                                        beta_schedule_type='linear',
                                        img_size=input_size,
                                        device=self.device)

                    self.initialized = True
                def preprocess(self,sample_img_path,contents_ch):
                    transforms = torchvision.transforms.Compose([
                        # torchvision.transforms.Resize((input_size,input_size)),
                        torchvision.transforms.Grayscale(num_output_channels=1),
                        torchvision.transforms.ToTensor(),
                        torchvision.transforms.Normalize((0.5), (0.5))
                    ])
                    sampleImage_len = len(contents_ch)

                    # print(data)
                    sample_img = Image.open(sample_img_path)
                    sample_img = transforms(sample_img).to(self.device)
                    sample_img = torch.unsqueeze(sample_img,1)
                    sample_img = sample_img.repeat(sampleImage_len, 1, 1, 1)
                    # print(len(sample_img))
                    return sample_img

                def inference(self,sample_img,contents_ch,id):
                    save_path = "./data"

                    charAttar = CharAttar(num_classes=self.config['num_classes'],device=self.device,style_path=self.config['style_path'])
                    x = self.diffusion.portion_sampling(model=self.model,sampling_chars=contents_ch,charAttar=charAttar,sample_img=sample_img,batch_size=4)
                    os.makedirs(save_path,exist_ok=True)
                    for img,ch in zip(x,contents_ch):
                        pillow_img = to_pil_image(img)
                        pillow_img.save(os.path.join(save_path,id)+f"_{ch}.png")

                    return x

            _service = DiffusionFontGenerateHandler()

            def handle(data,context):
                try:
                    if not _service.initialized:
                        _service.initialize(context)
                    if data is None:
                        return None
                    print(data)
                    print(data[0]['body'])
                    data = data[0]['body']['inputs']
                    sample_img_path = data["cropped_img_path"]
                    id = data["id"]
                    contents_ch = data["text"]

                    sample_img = _service.preprocess(sample_img_path=sample_img_path,contents_ch=contents_ch)
                    data = _service.inference(sample_img,contents_ch,id)
                    return [data.tolist()]

                except Exception as e:
                    raise e

            '''
            with open("pvc/model-store/handler.py", "w") as f:
                f.write(x)
            f.close()
            print("Saving handler.py complete !!")

        import argparse
        _parser = argparse.ArgumentParser(prog='Make handle', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = make_handle(**_parsed_args)
      image: python:3.10
      volumeMounts:
      - {mountPath: pvc, name: test-lee}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Make A hanlder file
          & config.properties file, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": [], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def make_handle():\n    import os\n    import json\n    os.mkdir(''model-store'')\n    config_json
          = json.dumps({\\\n            \"diffusion_serve_test\": {\\\n                \"1.0\":
          {\\\n                    \"defaultVersion\": true,\\\n                    \"marName\":
          \"diffusion_serve_test.mar\",\\\n                    \"minWorkers\": 2,\\\n                    \"maxWorkers\":
          12,\\\n                    \"batchSize\": 25,\\\n                    \"maxBatchDelay\":
          1000,\\\n                    \"responseTimeout\": 1200\\\n                    }\\\n                }\\\n            }\n        )\n    config
          = {\n        \"inference_address\":\"http://192.168.0.80:9334\",\n        \"management_address\":\"http://192.168.0.80:9335\",\n        \"metrics_address\":\"http://192.168.0.80:9336\",\n        \"enable_envvars_config\":True,\n        \"model_store\":\"/home/hojun/Documents/code/kubeflow/model-store\",\n        \"install_py_dep_per_model\":True,\n        \"models\":
          config_json\n    }\n    if not os.path.exists(\"pvc/model-store/config\"):\n        os.mkdir(\"pvc/model-store/config\")\n    with
          open(\"pvc/model-store/config/config.properties\", \"w\") as f:\n        for
          i, j in config.items():\n            f.write(f\"{i}={j}\\n\")\n        f.close()\n    x
          = ''''''\n    from ts.torch_handler.base_handler import BaseHandler\n    import
          os\n    import tqdm\n    import math\n    import random\n    import json\n    import
          pandas as pd\n    import numpy as np\n    from PIL import Image\n\n    import
          torch, torchvision\n    from torch import optim\n    import torch.nn as
          nn\n    import torch.nn.functional as F\n    from torch.utils.data import
          DataLoader, TensorDataset,Dataset\n    from torchvision.transforms.functional
          import to_pil_image\n\n    from model import UNet, Diffusion, CharAttar\n    from
          functools import partial\n    from utils import load_yaml\n\n    from PIL
          import Image\n\n    os.environ[''CUDA_VISIBLE_DEVICES''] = str(0)\n    device
          = torch.device(''cuda'' if torch.cuda.is_available() else ''cpu'')\n\n    class
          DiffusionFontGenerateHandler(BaseHandler):#why use BaseHandler and abc\n        def
          __init__(self):\n            super(DiffusionFontGenerateHandler,self).__init__()\n            self.config
          = load_yaml(\"config.yaml\")\n            self.initialized = False\n            self.device
          = f\"cuda:{self.config[''gpu_num'']}\"\n\n        def initialize(self,context):\n            input_size
          = 64\n            self.manifest = context.manifest\n            properties
          = context.system_properties\n            model_dir = properties.get(\"model_dir\")\n            serialized_file
          = self.manifest[''model''][''serializedFile'']\n            model_pt_path
          = os.path.join(model_dir, serialized_file)\n            if not os.path.isfile(model_pt_path):\n                raise
          RuntimeError(\"Missing the model.pt file\")\n\n            init_model =
          UNet().to(self.device)\n            ckpt = torch.load(model_pt_path)\n            init_model.load_state_dict(ckpt)\n            self.model
          = init_model\n\n            self.diffusion = Diffusion(first_beta=1e-4,\n                                end_beta=0.02,\n                                noise_step=1000,\n                                beta_schedule_type=''linear'',\n                                img_size=input_size,\n                                device=self.device)\n\n            self.initialized
          = True\n        def preprocess(self,sample_img_path,contents_ch):\n            transforms
          = torchvision.transforms.Compose([\n                # torchvision.transforms.Resize((input_size,input_size)),\n                torchvision.transforms.Grayscale(num_output_channels=1),\n                torchvision.transforms.ToTensor(),\n                torchvision.transforms.Normalize((0.5),
          (0.5))\n            ])\n            sampleImage_len = len(contents_ch)\n\n            #
          print(data)\n            sample_img = Image.open(sample_img_path)\n            sample_img
          = transforms(sample_img).to(self.device)\n            sample_img = torch.unsqueeze(sample_img,1)\n            sample_img
          = sample_img.repeat(sampleImage_len, 1, 1, 1)\n            # print(len(sample_img))\n            return
          sample_img\n\n        def inference(self,sample_img,contents_ch,id):\n            save_path
          = \"./data\"\n\n            charAttar = CharAttar(num_classes=self.config[''num_classes''],device=self.device,style_path=self.config[''style_path''])\n            x
          = self.diffusion.portion_sampling(model=self.model,sampling_chars=contents_ch,charAttar=charAttar,sample_img=sample_img,batch_size=4)\n            os.makedirs(save_path,exist_ok=True)\n            for
          img,ch in zip(x,contents_ch):\n                pillow_img = to_pil_image(img)\n                pillow_img.save(os.path.join(save_path,id)+f\"_{ch}.png\")\n\n            return
          x\n\n    _service = DiffusionFontGenerateHandler()\n\n    def handle(data,context):\n        try:\n            if
          not _service.initialized:\n                _service.initialize(context)\n            if
          data is None:\n                return None\n            print(data)\n            print(data[0][''body''])\n            data
          = data[0][''body''][''inputs'']\n            sample_img_path = data[\"cropped_img_path\"]\n            id
          = data[\"id\"]\n            contents_ch = data[\"text\"]\n\n            sample_img
          = _service.preprocess(sample_img_path=sample_img_path,contents_ch=contents_ch)\n            data
          = _service.inference(sample_img,contents_ch,id)\n            return [data.tolist()]\n\n        except
          Exception as e:\n            raise e\n\n    ''''''\n    with open(\"pvc/model-store/handler.py\",
          \"w\") as f:\n        f.write(x)\n    f.close()\n    print(\"Saving handler.py
          complete !!\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Make
          handle'', description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = make_handle(**_parsed_args)\n"], "image": "python:3.10"}}, "name": "Make
          handle"}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.16
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
    volumes:
    - name: test-lee
      persistentVolumeClaim: {claimName: diffusion}
  - name: model-evalute
    container:
      args: [--model-value, '{{inputs.parameters.model-training-Output}}', --model-evalute-csv-path,
        pvc/model.csv, --model-path, '{{inputs.parameters.result_model_path}}', '----output-paths',
        /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def model_evalute(model_value,model_evalute_csv_path,model_path):
            import pandas as pd
            best_model_value = pd.read_csv(model_evalute_csv_path)[1]

            if best_model_value < model_value:
                model_info = [model_path,model_value]
                pd.DataFrame(model_info).to_csv(model_evalute_csv_path)
                return True
            else:
                return False

        def _serialize_bool(bool_value: bool) -> str:
            if isinstance(bool_value, str):
                return bool_value
            if not isinstance(bool_value, bool):
                raise TypeError('Value "{}" has type "{}" instead of bool.'.format(
                    str(bool_value), str(type(bool_value))))
            return str(bool_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Model evalute', description='')
        _parser.add_argument("--model-value", dest="model_value", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-evalute-csv-path", dest="model_evalute_csv_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = model_evalute(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_bool,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: python:3.10
    inputs:
      parameters:
      - {name: model-training-Output}
      - {name: result_model_path}
    outputs:
      parameters:
      - name: model-evalute-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: model-evalute-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.16
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--model-value", {"inputValue": "model_value"}, "--model-evalute-csv-path",
          {"inputValue": "model_evalute_csv_path"}, "--model-path", {"inputValue":
          "model_path"}, "----output-paths", {"outputPath": "Output"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def model_evalute(model_value,model_evalute_csv_path,model_path):\n    import
          pandas as pd\n    best_model_value = pd.read_csv(model_evalute_csv_path)[1]\n\n    if
          best_model_value < model_value:\n        model_info = [model_path,model_value]\n        pd.DataFrame(model_info).to_csv(model_evalute_csv_path)\n        return
          True\n    else:\n        return False\n\ndef _serialize_bool(bool_value:
          bool) -> str:\n    if isinstance(bool_value, str):\n        return bool_value\n    if
          not isinstance(bool_value, bool):\n        raise TypeError(''Value \"{}\"
          has type \"{}\" instead of bool.''.format(\n            str(bool_value),
          str(type(bool_value))))\n    return str(bool_value)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Model evalute'', description='''')\n_parser.add_argument(\"--model-value\",
          dest=\"model_value\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-evalute-csv-path\",
          dest=\"model_evalute_csv_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-path\",
          dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = model_evalute(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_bool,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "python:3.10"}}, "inputs": [{"name": "model_value", "type": "Float"},
          {"name": "model_evalute_csv_path", "type": "String"}, {"name": "model_path"}],
          "name": "Model evalute", "outputs": [{"name": "Output", "type": "Boolean"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"model_evalute_csv_path":
          "pvc/model.csv", "model_path": "{{inputs.parameters.result_model_path}}",
          "model_value": "{{inputs.parameters.model-training-Output}}"}'}
  - name: model-training
    container:
      args: [--gpu-num, '{{inputs.parameters.gpu_num}}', --batch-size, '{{inputs.parameters.batch_size}}',
        --input-size, '{{inputs.parameters.image_size}}', --n-epochs, '{{inputs.parameters.n_epochs}}',
        --num-classes, '{{inputs.parameters.num_classes}}', --mode, '{{inputs.parameters.mode}}',
        --lr, '{{inputs.parameters.lr}}', --result-model-path, '{{inputs.parameters.result_model_path}}',
        --style-path, '{{inputs.parameters.style_path}}', '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'tqdm==4.65.0' 'pandas==2.0.0' ' pillow==9.4.0' 'torch==2.0.0+cu118' 'torchvision==0.15.1+cu118'
        || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'tqdm==4.65.0' 'pandas==2.0.0' ' pillow==9.4.0' 'torch==2.0.0+cu118' 'torchvision==0.15.1+cu118'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def model_training(gpu_num, batch_size, input_size, n_epochs, num_classes,\
        \ mode, lr, result_model_path, style_path):\n    import os\n    import tqdm\n\
        \    import math\n    import random\n    import pandas as pd\n    import numpy\
        \ as np\n    from PIL import Image\n\n    import torch, torchvision\n    from\
        \ torch import optim\n    import torch.nn as nn\n    import torch.nn.functional\
        \ as F\n    from torch.utils.data import DataLoader, TensorDataset,Dataset\n\
        \n    from functools import partial\n\n    from PIL import Image\n    class\
        \ DiffusionDataset(Dataset):\n        def __init__(self, csv_path, transform\
        \ =None):\n            self.transform = transform\n            self.csv_data\
        \ = pd.read_csv(os.path.join(csv_path,\"diffusion_font_train.csv\"))\n   \
        \         self.x_file_name = self.csv_data.iloc[:,0]\n            self.x_path\
        \ = self.csv_data.iloc[:,1]\n            self.y = self.csv_data.iloc[:,2]\n\
        \            self.labels = np.unique(self.y)\n            self.y_to_label\
        \ = self.make_y_to_label()\n            self.label_to_y = self.make_label_to_y()\n\
        \            self.y_labels = self.make_y_labels()\n\n        def make_y_to_label(self):\n\
        \            y_to_label_dict = {}\n            for label, value in enumerate(self.labels):\n\
        \                y_to_label_dict[value] = label\n            return y_to_label_dict\n\
        \n        def make_label_to_y(self):\n            label_to_y_dict = {}\n \
        \           for label, value in enumerate(self.labels):\n                label_to_y_dict[label]\
        \ = value\n            return label_to_y_dict\n\n        def make_y_labels(self):\n\
        \            y_labels = []\n            for y_ch in self.y:\n            \
        \    y_labels.append(self.y_to_label[y_ch])\n            return y_labels\n\
        \n        def __len__(self):\n            return len(self.x_path)\n\n    \
        \    def __getitem__(self, id_):\n            filename = self.x_file_name[id_]\n\
        \            x = Image.open(self.x_path[id_])\n            transform_x = self.transform(x)\n\
        \            label = self.y_labels[id_]\n\n            return transform_x,\
        \ label, filename\n\n    class Diffusion:    \n        def __init__(self,\
        \ first_beta, end_beta, beta_schedule_type, noise_step, img_size, device):\n\
        \            self.first_beta = first_beta\n            self.end_beta = end_beta\n\
        \            self.beta_schedule_type = beta_schedule_type\n\n            self.noise_step\
        \ = noise_step\n\n            self.beta_list = self.beta_schedule().to(device)\n\
        \n            self.alphas =  1. - self.beta_list\n            self.alpha_bars\
        \ = torch.cumprod(self.alphas, dim = 0)\n\n            self.img_size = img_size\n\
        \            self.device = device\n\n        def sample_t(self, batch_size):\n\
        \            return torch.randint(1,self.noise_step,(batch_size,))\n\n   \
        \     def beta_schedule(self):\n            if self.beta_schedule_type ==\
        \ \"linear\":\n                return torch.linspace(self.first_beta, self.end_beta,\
        \ self.noise_step)\n            elif self.beta_schedule_type == \"cosine\"\
        :\n                steps = self.noise_step + 1\n                s = 0.008\n\
        \                x = torch.linspace(0, self.noise_step, steps)\n         \
        \       alphas_cumprod = torch.cos(((x / self.noise_step) + s) / (1 + s) *\
        \ torch.pi * 0.5) ** 2\n                alphas_cumprod = alphas_cumprod /\
        \ alphas_cumprod[0]\n                betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n\
        \                return torch.clip(betas, 0.0001, 0.9999)\n            elif\
        \ self.beta_schedule_type == \"quadratic\":\n                return torch.linspace(self.first_beta\
        \ ** 0.5, self.end_beta ** 0.5, self.noise_step) ** 2\n            elif self.beta_schedule_type\
        \ == \"sigmoid\":\n                beta = torch.linspace(-6,-6,self.noise_step)\n\
        \                return torch.sigmoid(beta) * (self.end_beta - self.first_beta)\
        \ + self.first_beta\n\n        def alpha_t(self, t):\n            return self.alphas[t][:,\
        \ None, None, None]\n\n        def alpha_bar_t (self,t):\n            return\
        \ self.alpha_bars[t][:, None, None, None]\n\n        def one_minus_alpha_bar(self,t):\n\
        \            return (1. - self.alpha_bars[t])[:, None, None, None]\n\n   \
        \     def beta_t(self,t):\n            return self.beta_list[t][:, None, None,\
        \ None]\n\n        def noise_images(self,x,t):\n            epsilon = torch.randn_like(x)\n\
        \            return torch.sqrt(self.alpha_bar_t(t)) * x + torch.sqrt(self.one_minus_alpha_bar(t))\
        \ * epsilon , epsilon\n\n        def indexToChar(self,y):\n            return\
        \ chr(44032+y)\n        def portion_sampling(self, model, n,sampleImage_len,dataset,mode,charAttar,sample_img,\
        \ cfg_scale=3):\n            example_images = []\n            model.eval()\n\
        \            with torch.no_grad():\n                x_list = torch.randn((sampleImage_len,\
        \ 1, self.img_size, self.img_size)).to(self.device)\n\n                y_idx\
        \ = list(range(n))[::math.floor(n/sampleImage_len)][:sampleImage_len]\n  \
        \              contents_index = torch.IntTensor(y_idx)\n                contents\
        \ = [dataset.label_to_y[int(content_index)] for content_index in contents_index]\n\
        \                charAttr_list = charAttar.make_charAttr(sample_img, contents_index,\
        \ contents,mode=3).to(self.device)\n\n                pbar = tqdm(list(reversed(range(1,\
        \ self.noise_step))),desc=\"sampling\")\n                for i in pbar:\n\
        \                    dataset = TensorDataset(x_list,charAttr_list)\n     \
        \               batch_size= 18\n                    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=False)\n\
        \                    predicted_noise = torch.tensor([]).to(self.device)\n\
        \                    uncond_predicted_noise = torch.tensor([]).to(self.device)\n\
        \                    for batch_x, batch_conditions in dataloader:\n      \
        \                  batch_t = (torch.ones(len(batch_x)) * i).long().to(self.device)\n\
        \                        batch_noise = model(batch_x, batch_t, batch_conditions)\n\
        \                        predicted_noise = torch.cat([predicted_noise,batch_noise],dim=0)\n\
        \                        #uncodition\n                        uncond_batch_noise\
        \ = model(batch_x, batch_t, torch.zeros_like(batch_conditions))\n        \
        \                uncond_predicted_noise = torch.cat([uncond_predicted_noise,uncond_batch_noise],dim\
        \ = 0)\n\n                    if cfg_scale > 0:\n                        predicted_noise\
        \ = torch.lerp(uncond_predicted_noise, predicted_noise, cfg_scale)\n\n   \
        \                 t = (torch.ones(sampleImage_len) * i).long()\n         \
        \           a_t = self.alpha_t(t)\n                    aBar_t = self.alpha_bar_t(t)\n\
        \                    b_t = self.beta_t(t)\n\n                    if i > 1:\n\
        \                        noise = torch.randn_like(x_list)\n              \
        \      else:\n                        noise = torch.zeros_like(x_list)\n\n\
        \                    x_list = 1 / torch.sqrt(a_t) * (\n                  \
        \          x_list - ((1 - a_t) / (torch.sqrt(1 - aBar_t))) * predicted_noise)\
        \ + torch.sqrt(\n                        b_t) * noise\n            model.train()\n\
        \            x_list = (x_list.clamp(-1, 1) + 1) / 2\n            x_list =\
        \ (x_list * 255).type(torch.uint8)\n            return x_list\n\n        def\
        \ test_sampling(self, model,sampleImage_len,charAttr_list,cfg_scale=3):\n\
        \            example_images = []\n            model.eval()\n            with\
        \ torch.no_grad():\n                x_list = torch.randn((sampleImage_len,\
        \ 3, self.img_size, self.img_size)).to(self.device)\n                pbar\
        \ = tqdm(list(reversed(range(1, self.noise_step))),desc=\"sampling\")\n  \
        \              for i in pbar:\n                    dataset = TensorDataset(x_list,charAttr_list)\n\
        \                    batch_size= 4\n                    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=False)\n\
        \                    predicted_noise = torch.tensor([]).to(self.device)\n\
        \                    uncond_predicted_noise = torch.tensor([]).to(self.device)\n\
        \                    for batch_x, batch_conditions in dataloader:\n      \
        \                  batch_t = (torch.ones(len(batch_x)) * i).long().to(self.device)\n\
        \                        batch_noise = model(batch_x, batch_t, batch_conditions)\n\
        \                        predicted_noise = torch.cat([predicted_noise,batch_noise],dim=0)\n\
        \                        #uncodition\n                        uncond_batch_noise\
        \ = model(batch_x, batch_t, torch.zeros_like(batch_conditions))\n        \
        \                uncond_predicted_noise = torch.cat([uncond_predicted_noise,uncond_batch_noise],dim\
        \ = 0)\n\n                    if cfg_scale > 0:\n                        predicted_noise\
        \ = torch.lerp(uncond_predicted_noise, predicted_noise, cfg_scale)\n\n   \
        \                 t = (torch.ones(sampleImage_len) * i).long()\n         \
        \           a_t = self.alpha_t(t)\n                    aBar_t = self.alpha_bar_t(t)\n\
        \                    b_t = self.beta_t(t)\n\n                    if i > 1:\n\
        \                        noise = torch.randn_like(x_list)\n              \
        \      else:\n                        noise = torch.zeros_like(x_list)\n\n\
        \                    x_list = 1 / torch.sqrt(a_t) * (\n                  \
        \          x_list - ((1 - a_t) / (torch.sqrt(1 - aBar_t))) * predicted_noise)\
        \ + torch.sqrt(\n                        b_t) * noise\n            x_list\
        \ = (x_list.clamp(-1, 1) + 1) / 2\n            x_list = (x_list * 255).type(torch.uint8)\n\
        \            return x_list\n\n    class SelfAttention(nn.Module):\n      \
        \  def __init__(self, channels):\n            super(SelfAttention, self).__init__()\n\
        \            self.channels = channels\n            self.mha = nn.MultiheadAttention(channels,\
        \ 4, batch_first=True)\n            self.ln = nn.LayerNorm([channels])\n \
        \           self.ff_self = nn.Sequential(\n                nn.LayerNorm([channels]),\n\
        \                nn.Linear(channels, channels),\n                nn.GELU(),\n\
        \                nn.Linear(channels, channels),\n            )\n\n       \
        \ def forward(self, x):\n            size = x.shape[-1]\n            x = x.view(-1,\
        \ self.channels, size * size).swapaxes(1, 2)\n            x_ln = self.ln(x)\n\
        \            attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n           \
        \ attention_value = attention_value + x\n            attention_value = self.ff_self(attention_value)\
        \ + attention_value\n            return attention_value.swapaxes(2, 1).view(-1,\
        \ self.channels, size, size)\n\n    class DoubleConv(nn.Module):\n       \
        \ def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n\
        \            super().__init__()\n            self.residual = residual\n  \
        \          if not mid_channels:\n                mid_channels = out_channels\n\
        \            self.double_conv = nn.Sequential(\n                nn.Conv2d(in_channels,\
        \ mid_channels, kernel_size=3, padding=1, bias=False),\n                nn.GroupNorm(1,\
        \ mid_channels),\n                nn.GELU(),\n                nn.Conv2d(mid_channels,\
        \ out_channels, kernel_size=3, padding=1, bias=False),\n                nn.GroupNorm(1,\
        \ out_channels),\n            )\n\n        def forward(self, x):\n       \
        \     if self.residual:\n                return F.gelu(x + self.double_conv(x))\n\
        \            else:\n                return self.double_conv(x)\n\n    class\
        \ Down(nn.Module):\n        def __init__(self, in_channels, out_channels,\
        \ time_dim=256, charAttr_dim=12456):\n            super().__init__()\n   \
        \         self.maxpool_conv = nn.Sequential(\n                nn.MaxPool2d(2),\n\
        \                DoubleConv(in_channels, in_channels, residual=True),\n  \
        \              DoubleConv(in_channels, out_channels),\n            )\n\n \
        \           self.time_layer = nn.Sequential(\n                nn.SiLU(),\n\
        \                nn.Linear(\n                    time_dim,\n             \
        \       out_channels\n                ),\n            )\n\n            self.condition_layer\
        \ = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(\n\
        \                    charAttr_dim,\n                    out_channels\n   \
        \             ),\n            )\n\n        def forward(self, x, t,charAttr):\n\
        \            x = self.maxpool_conv(x)\n            emb = self.time_layer(t)[:,\
        \ :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n            charAttr_emb\
        \ = self.condition_layer(charAttr)[:, :, None, None].repeat(1, 1, x.shape[-2],\
        \ x.shape[-1])\n            return x + emb + charAttr_emb\n\n    class Up(nn.Module):\n\
        \        def __init__(self, in_channels, out_channels, time_dim=256, charAttr_dim=12456):\n\
        \            super().__init__()\n\n            self.up = nn.Upsample(scale_factor=2,\
        \ mode=\"bilinear\", align_corners=True)\n            self.conv = nn.Sequential(\n\
        \                DoubleConv(in_channels, in_channels, residual=True),\n  \
        \              DoubleConv(in_channels, out_channels, in_channels // 2),\n\
        \            )\n\n            self.time_layer = nn.Sequential(\n         \
        \       nn.SiLU(),\n                nn.Linear(\n                    time_dim,\n\
        \                    out_channels\n                ),\n            )\n   \
        \         self.condition_layer = nn.Sequential(\n                nn.SiLU(),\n\
        \                nn.Linear(\n                    charAttr_dim,\n         \
        \           out_channels\n                ),\n            )\n\n        def\
        \ forward(self, x, skip_x, t, charAttr):\n            x = self.up(x)\n   \
        \         x = torch.cat([skip_x, x], dim=1)\n            x = self.conv(x)\n\
        \            time_emb = self.time_layer(t)[:, :, None, None].repeat(1, 1,\
        \ x.shape[-2], x.shape[-1])\n            charAttr_emb = self.condition_layer(charAttr)[:,\
        \ :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n            return\
        \ x + time_emb + charAttr_emb\n\n    class UNet(nn.Module):\n        def __init__(self,\
        \ c_in=1, c_out=1, time_dim=256, charAttr_dim = 296, device=\"cuda\"):\n \
        \           super().__init__()\n            self.device = device\n       \
        \     self.time_dim = time_dim\n            self.charAttr_dim = charAttr_dim\n\
        \n            self.inc = DoubleConv(c_in, 64)\n            self.down1 = Down(64,\
        \ 128, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n         \
        \   self.sa1 = SelfAttention(128)\n            self.down2 = Down(128, 256,time_dim=self.time_dim,\
        \ charAttr_dim=self.charAttr_dim)\n            self.sa2 = SelfAttention(256)\n\
        \            self.down3 = Down(256, 256, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n\
        \            self.sa3 = SelfAttention(256)\n\n            self.bot1 = DoubleConv(256,\
        \ 512)\n            self.bot2 = DoubleConv(512, 512)\n            self.bot3\
        \ = DoubleConv(512, 256)\n\n            self.up1 = Up(512, 128, time_dim=self.time_dim,\
        \ charAttr_dim=self.charAttr_dim)\n            self.sa4 = SelfAttention(128)\n\
        \            self.up2 = Up(256, 64, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n\
        \            self.sa5 = SelfAttention(64)\n            self.up3 = Up(128,\
        \ 64, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n          \
        \  self.sa6 = SelfAttention(64)\n            self.outc = nn.Conv2d(64, c_out,\
        \ kernel_size=1)\n\n        def pos_encoding(self, t, channels):\n       \
        \     inv_freq = 1.0 / (\n                10000\n                ** (torch.arange(0,\
        \ channels, 2, device=self.device).float() / channels)\n            )\n  \
        \          pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n\
        \            pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n\
        \            pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n       \
        \     return pos_enc\n\n        def forward(self, x, time, charAttr):\n  \
        \          time = time.unsqueeze(-1).type(torch.float)\n            time =\
        \ self.pos_encoding(time, self.time_dim)\n\n            # if y is not None:\n\
        \            #     time += self.contents_emb(y)\n\n            x1 = self.inc(x)\n\
        \            x2 = self.down1(x1, time, charAttr)\n            x2 = self.sa1(x2)\n\
        \            x3 = self.down2(x2, time, charAttr)\n            x3 = self.sa2(x3)\n\
        \            x4 = self.down3(x3, time, charAttr)\n            x4 = self.sa3(x4)\n\
        \n            x4 = self.bot1(x4)\n            x4 = self.bot2(x4)\n       \
        \     x4 = self.bot3(x4)\n\n            x = self.up1(x4, x3, time, charAttr)\n\
        \            x = self.sa4(x)\n            x = self.up2(x, x2, time, charAttr)\n\
        \            x = self.sa5(x)\n            x = self.up3(x, x1, time, charAttr)\n\
        \            x = self.sa6(x)\n            output = self.outc(x)\n        \
        \    return output\n\n    class GlobalContext(nn.Module):\n        \"\"\"\
        \ Global-context \"\"\"\n        def __init__(self, C, bottleneck_ratio=0.25,\
        \ w_norm='none'):\n            super().__init__()\n            C_bottleneck\
        \ = int(C * bottleneck_ratio)\n            w_norm = w_norm_dispatch(w_norm)\n\
        \            self.k_proj = w_norm(nn.Conv2d(C, 1, 1))\n            self.transform\
        \ = nn.Sequential(\n                w_norm(nn.Linear(C, C_bottleneck)),\n\
        \                nn.LayerNorm(C_bottleneck),\n                nn.ReLU(),\n\
        \                w_norm(nn.Linear(C_bottleneck, C))\n            )\n\n   \
        \     def forward(self, x):\n            # x: [B, C, H, W]\n            context_logits\
        \ = self.k_proj(x)  # [B, 1, H, W]\n            context_weights = F.softmax(context_logits.flatten(1),\
        \ dim=1)  # [B, HW]\n            context = torch.einsum('bci,bi->bc', x.flatten(2),\
        \ context_weights)\n            out = self.transform(context)\n\n        \
        \    return out[..., None, None]\n\n    class GCBlock(nn.Module):\n      \
        \  \"\"\" Global-context block \"\"\"\n        def __init__(self, C, bottleneck_ratio=0.25,\
        \ w_norm='none'):\n            super().__init__()\n            self.gc = GlobalContext(C,\
        \ bottleneck_ratio, w_norm)\n\n        def forward(self, x):\n           \
        \ gc = self.gc(x)\n            return x + gc\n\n    class TLU(nn.Module):\n\
        \        \"\"\" Thresholded Linear Unit \"\"\"\n        def __init__(self,\
        \ num_features):\n            super().__init__()\n            self.num_features\
        \ = num_features\n            self.tau = nn.Parameter(torch.zeros(1, num_features,\
        \ 1, 1))\n\n        def forward(self, x):\n            return torch.max(x,\
        \ self.tau)\n\n        def extra_repr(self):\n            return 'num_features={}'.format(self.num_features)\n\
        \n    # NOTE generalized version\n    class FilterResponseNorm(nn.Module):\n\
        \        \"\"\" Filter Response Normalization \"\"\"\n        def __init__(self,\
        \ num_features, ndim, eps=None, learnable_eps=False):\n            \"\"\"\n\
        \            Args:\n                num_features\n                ndim\n \
        \               eps: if None is given, use the paper value as default.\n \
        \                   from paper, fixed_eps=1e-6 and learnable_eps_init=1e-4.\n\
        \                learnable_eps: turn eps to learnable parameter, which is\
        \ recommended on\n                    fully-connected or 1x1 activation map.\n\
        \            \"\"\"\n            super().__init__()\n            if eps is\
        \ None:\n                if learnable_eps:\n                    eps = 1e-4\n\
        \                else:\n                    eps = 1e-6\n\n            self.num_features\
        \ = num_features\n            self.init_eps = eps\n            self.learnable_eps\
        \ = learnable_eps\n            self.ndim = ndim\n\n            self.mean_dims\
        \ = list(range(2, 2+ndim))\n\n            self.weight = nn.Parameter(torch.ones([1,\
        \ num_features] + [1]*ndim))\n            self.bias = nn.Parameter(torch.zeros([1,\
        \ num_features] + [1]*ndim))\n            if learnable_eps:\n            \
        \    self.eps = nn.Parameter(torch.as_tensor(eps))\n            else:\n  \
        \              self.register_buffer('eps', torch.as_tensor(eps))\n\n     \
        \   def forward(self, x):\n            # normalize\n            nu2 = x.pow(2).mean(self.mean_dims,\
        \ keepdim=True)\n            x = x * torch.rsqrt(nu2 + self.eps.abs())\n\n\
        \            # modulation\n            x = x * self.weight + self.bias\n\n\
        \            return x\n\n        def extra_repr(self):\n            return\
        \ 'num_features={}, init_eps={}, ndim={}'.format(\n                    self.num_features,\
        \ self.init_eps, self.ndim)\n\n    FilterResponseNorm1d = partial(FilterResponseNorm,\
        \ ndim=1, learnable_eps=True)\n    FilterResponseNorm2d = partial(FilterResponseNorm,\
        \ ndim=2)\n\n    def split_dim(x, dim, n_chunks):\n        shape = x.shape\n\
        \        assert shape[dim] % n_chunks == 0\n        return x.view(*shape[:dim],\
        \ n_chunks, shape[dim] // n_chunks, *shape[dim+1:])\n\n    def weights_init(init_type='default'):\n\
        \        \"\"\" Adopted from FUNIT \"\"\"\n        def init_fun(m):\n    \
        \        classname = m.__class__.__name__\n            if (classname.find('Conv')\
        \ == 0 or classname.find('Linear') == 0) and hasattr(m, 'weight'):\n     \
        \           if init_type == 'gaussian':\n                    nn.init.normal_(m.weight.data,\
        \ 0.0, 0.02)\n                elif init_type == 'xavier':\n              \
        \      nn.init.xavier_normal_(m.weight.data, gain=2**0.5)\n              \
        \  elif init_type == 'kaiming':\n                    nn.init.kaiming_normal_(m.weight.data,\
        \ a=0, mode='fan_in')\n                elif init_type == 'orthogonal':\n \
        \                   nn.init.orthogonal_(m.weight.data, gain=2**0.5)\n    \
        \            elif init_type == 'default':\n                    pass\n    \
        \            else:\n                    assert 0, \"Unsupported initialization:\
        \ {}\".format(init_type)\n\n                if hasattr(m, 'bias') and m.bias\
        \ is not None:\n                    nn.init.constant_(m.bias.data, 0.0)\n\n\
        \        return init_fun\n\n    def spectral_norm(module):\n        \"\"\"\
        \ init & apply spectral norm \"\"\"\n        nn.init.xavier_uniform_(module.weight,\
        \ 2 ** 0.5)\n        if hasattr(module, 'bias') and module.bias is not None:\n\
        \            module.bias.data.zero_()\n\n        return nn.utils.spectral_norm(module)\n\
        \n    class BasicConv(nn.Module):\n        def __init__(self, in_planes, out_planes,\
        \ kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True,\
        \ bias=False):\n            super(BasicConv, self).__init__()\n          \
        \  self.out_channels = out_planes\n            self.conv = nn.Conv2d(in_planes,\
        \ out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation,\
        \ groups=groups, bias=bias)\n            self.bn = nn.BatchNorm2d(out_planes,\
        \ eps=1e-5, momentum=0.01, affine=True) if bn else None\n            self.relu\
        \ = nn.ReLU() if relu else None\n\n        def forward(self, x):\n       \
        \     x = self.conv(x)\n            if self.bn is not None:\n            \
        \    x = self.bn(x)\n            if self.relu is not None:\n             \
        \   x = self.relu(x)\n            return x\n\n    class Flatten(nn.Module):\n\
        \n        def forward(self, x):\n            return x.view(x.size(0), -1)\n\
        \n    class ChannelGate(nn.Module):\n        def __init__(self, gate_channels,\
        \ reduction_ratio=16, pool_types=['avg', 'max']):\n            super(ChannelGate,\
        \ self).__init__()\n            self.gate_channels = gate_channels\n     \
        \       self.mlp = nn.Sequential(\n                Flatten(),\n          \
        \      nn.Linear(gate_channels, gate_channels // reduction_ratio),\n     \
        \           nn.ReLU(),\n                nn.Linear(gate_channels // reduction_ratio,\
        \ gate_channels)\n                )\n            self.pool_types = pool_types\n\
        \n        def forward(self, x):\n            channel_att_sum = None\n    \
        \        for pool_type in self.pool_types:\n                if pool_type ==\
        \ 'avg':\n                    avg_pool = F.avg_pool2d(x, (x.size(2), x.size(3)),\
        \ stride=(x.size(2), x.size(3)))\n                    channel_att_raw = self.mlp(avg_pool)\n\
        \                elif pool_type == 'max':\n                    max_pool =\
        \ F.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n\
        \                    channel_att_raw = self.mlp(max_pool)\n              \
        \  elif pool_type == 'lp':\n                    lp_pool = F.lp_pool2d(x, 2,\
        \ (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n               \
        \     channel_att_raw = self.mlp(lp_pool)\n                elif pool_type\
        \ == 'lse':\n                    # LSE pool only\n                    lse_pool\
        \ = logsumexp_2d(x)\n                    channel_att_raw = self.mlp(lse_pool)\n\
        \n                if channel_att_sum is None:\n                    channel_att_sum\
        \ = channel_att_raw\n                else:\n                    channel_att_sum\
        \ = channel_att_sum + channel_att_raw\n\n            scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n\
        \            return x * scale\n\n    def logsumexp_2d(tensor):\n        tensor_flatten\
        \ = tensor.view(tensor.size(0), tensor.size(1), -1)\n        s, _ = torch.max(tensor_flatten,\
        \ dim=2, keepdim=True)\n        outputs = s + (tensor_flatten - s).exp().sum(dim=2,\
        \ keepdim=True).log()\n        return outputs\n\n    class ChannelPool(nn.Module):\n\
        \        def forward(self, x):\n            return torch.cat((torch.max(x,\
        \ 1)[0].unsqueeze(1), torch.mean(x, 1).unsqueeze(1)), dim=1)\n\n    class\
        \ SpatialGate(nn.Module):\n        def __init__(self):\n            super(SpatialGate,\
        \ self).__init__()\n            kernel_size = 7\n            self.compress\
        \ = ChannelPool()\n            self.spatial = BasicConv(2, 1, kernel_size,\
        \ stride=1, padding=(kernel_size-1) // 2, relu=False)\n\n        def forward(self,\
        \ x):\n            x_compress = self.compress(x)\n            x_out = self.spatial(x_compress)\n\
        \            scale = torch.sigmoid(x_out)  # broadcasting\n            return\
        \ x * scale\n\n    class CBAM(nn.Module):\n        def __init__(self, gate_channels,\
        \ reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n    \
        \        super(CBAM, self).__init__()\n            self.ChannelGate = ChannelGate(gate_channels,\
        \ reduction_ratio, pool_types)\n            self.no_spatial = no_spatial\n\
        \            if not no_spatial:\n                self.SpatialGate = SpatialGate()\n\
        \n        def forward(self, x):\n            x_out = self.ChannelGate(x)\n\
        \            if not self.no_spatial:\n                x_out = self.SpatialGate(x_out)\n\
        \            return x_out\n\n    class Flatten(nn.Module):\n        def __init__(self,\
        \ start_dim=1, end_dim=-1):\n            super(Flatten, self).__init__()\n\
        \            self.start_dim = start_dim\n            self.end_dim = end_dim\n\
        \n        def forward(self, input):\n            return input.flatten(self.start_dim,\
        \ self.end_dim)\n\n    def dispatcher(dispatch_fn):\n        def decorated(key,\
        \ *args):\n            if callable(key):\n                return key\n\n \
        \           if key is None:\n                key = 'none'\n\n            return\
        \ dispatch_fn(key, *args)\n        return decorated\n\n    @dispatcher\n \
        \   def norm_dispatch(norm):\n        return {\n            'none': nn.Identity,\n\
        \            'in': partial(nn.InstanceNorm2d, affine=False),  # false as default\n\
        \            'bn': nn.BatchNorm2d,\n            'frn': FilterResponseNorm2d\n\
        \        }[norm.lower()]\n\n    @dispatcher\n    def w_norm_dispatch(w_norm):\n\
        \        # NOTE Unlike other dispatcher, w_norm is function, not class.\n\
        \        return {\n            'spectral': spectral_norm,\n            'none':\
        \ lambda x: x\n        }[w_norm.lower()]\n\n    @dispatcher\n    def activ_dispatch(activ,\
        \ norm=None):\n        if norm_dispatch(norm) == FilterResponseNorm2d:\n \
        \           # use TLU for FRN\n            activ = 'tlu'\n\n        return\
        \ {\n            \"none\": nn.Identity,\n            \"relu\": nn.ReLU,\n\
        \            \"lrelu\": partial(nn.LeakyReLU, negative_slope=0.2),\n     \
        \       \"tlu\": TLU\n        }[activ.lower()]\n\n    @dispatcher\n    def\
        \ pad_dispatch(pad_type):\n        return {\n            \"zero\": nn.ZeroPad2d,\n\
        \            \"replicate\": nn.ReplicationPad2d,\n            \"reflect\"\
        : nn.ReflectionPad2d\n        }[pad_type.lower()]\n\n    class ParamBlock(nn.Module):\n\
        \        def __init__(self, C_out, shape):\n            super().__init__()\n\
        \            w = torch.randn((C_out, *shape))\n            b = torch.randn((C_out,))\n\
        \            self.shape = shape\n            self.w = nn.Parameter(w)\n  \
        \          self.b = nn.Parameter(b)\n\n        def forward(self, x):\n   \
        \         b = self.b.reshape((1, *self.b.shape, 1, 1, 1)).repeat(x.size(0),\
        \ 1, *self.shape)\n            return self.w*x + b\n\n    class LinearBlock(nn.Module):\n\
        \        \"\"\" pre-active linear block \"\"\"\n        def __init__(self,\
        \ C_in, C_out, norm='none', activ='relu', bias=True, w_norm='none',\n    \
        \                dropout=0.):\n            super().__init__()\n          \
        \  activ = activ_dispatch(activ, norm)\n            if norm.lower() == 'bn':\n\
        \                norm = nn.BatchNorm1d\n            elif norm.lower() == 'frn':\n\
        \                norm = FilterResponseNorm1d\n            elif norm.lower()\
        \ == 'none':\n                norm = nn.Identity\n            else:\n    \
        \            raise ValueError(f\"LinearBlock supports BN only (but {norm}\
        \ is given)\")\n            w_norm = w_norm_dispatch(w_norm)\n           \
        \ self.norm = norm(C_in)\n            self.activ = activ()\n            if\
        \ dropout > 0.:\n                self.dropout = nn.Dropout(p=dropout)\n  \
        \          self.linear = w_norm(nn.Linear(C_in, C_out, bias))\n\n        def\
        \ forward(self, x):\n            x = self.norm(x)\n            x = self.activ(x)\n\
        \            if hasattr(self, 'dropout'):\n                x = self.dropout(x)\n\
        \            return self.linear(x)\n\n    class ConvBlock(nn.Module):\n  \
        \      \"\"\" pre-active conv block \"\"\"\n        def __init__(self, C_in,\
        \ C_out, kernel_size=3, stride=1, padding=1, norm='none',\n              \
        \      activ='relu', bias=True, upsample=False, downsample=False, w_norm='none',\n\
        \                    pad_type='zero', dropout=0., size=None):\n          \
        \  # 1x1 conv assertion\n            if kernel_size == 1:\n              \
        \  assert padding == 0\n            super().__init__()\n            self.C_in\
        \ = C_in\n            self.C_out = C_out\n\n            activ = activ_dispatch(activ,\
        \ norm)\n            norm = norm_dispatch(norm)\n            w_norm = w_norm_dispatch(w_norm)\n\
        \            pad = pad_dispatch(pad_type)\n            self.upsample = upsample\n\
        \            self.downsample = downsample\n\n            assert ((norm ==\
        \ FilterResponseNorm2d) == (activ == TLU)), \"Use FRN and TLU together\"\n\
        \n            if norm == FilterResponseNorm2d and size == 1:\n           \
        \     self.norm = norm(C_in, learnable_eps=True)\n            else:\n    \
        \            self.norm = norm(C_in)\n            if activ == TLU:\n      \
        \          self.activ = activ(C_in)\n            else:\n                self.activ\
        \ = activ()\n            if dropout > 0.:\n                self.dropout =\
        \ nn.Dropout2d(p=dropout)\n            self.pad = pad(padding)\n         \
        \   self.conv = w_norm(nn.Conv2d(C_in, C_out, kernel_size, stride, bias=bias))\n\
        \n        def forward(self, x):\n            x = self.norm(x)\n          \
        \  x = self.activ(x)\n            if self.upsample:\n                x = F.interpolate(x,\
        \ scale_factor=2)\n            if hasattr(self, 'dropout'):\n            \
        \    x = self.dropout(x)\n            x = self.conv(self.pad(x))\n       \
        \     if self.downsample:\n                x = F.avg_pool2d(x, 2)\n      \
        \      return x\n\n    class ResBlock(nn.Module):\n        \"\"\" Pre-activate\
        \ ResBlock with spectral normalization \"\"\"\n        def __init__(self,\
        \ C_in, C_out, kernel_size=3, padding=1, upsample=False, downsample=False,\n\
        \                    norm='none', w_norm='none', activ='relu', pad_type='zero',\
        \ dropout=0.,\n                    scale_var=False):\n            assert not\
        \ (upsample and downsample)\n            super().__init__()\n            w_norm\
        \ = w_norm_dispatch(w_norm)\n            self.C_in = C_in\n            self.C_out\
        \ = C_out\n            self.upsample = upsample\n            self.downsample\
        \ = downsample\n            self.scale_var = scale_var\n\n            self.conv1\
        \ = ConvBlock(C_in, C_out, kernel_size, 1, padding, norm, activ,\n       \
        \                         upsample=upsample, w_norm=w_norm, pad_type=pad_type,\n\
        \                                dropout=dropout)\n            self.conv2\
        \ = ConvBlock(C_out, C_out, kernel_size, 1, padding, norm, activ,\n      \
        \                          w_norm=w_norm, pad_type=pad_type, dropout=dropout)\n\
        \n            # XXX upsample / downsample needs skip conv?\n            if\
        \ C_in != C_out or upsample or downsample:\n                self.skip = w_norm(nn.Conv2d(C_in,\
        \ C_out, 1))\n\n        def forward(self, x):\n            \"\"\"\n      \
        \      normal: pre-activ + convs + skip-con\n            upsample: pre-activ\
        \ + upsample + convs + skip-con\n            downsample: pre-activ + convs\
        \ + downsample + skip-con\n            => pre-activ + (upsample) + convs +\
        \ (downsample) + skip-con\n            \"\"\"\n            out = x\n\n   \
        \         out = self.conv1(out)\n            out = self.conv2(out)\n\n   \
        \         if self.downsample:\n                out = F.avg_pool2d(out, 2)\n\
        \n            # skip-con\n            if hasattr(self, 'skip'):\n        \
        \        if self.upsample:\n                    x = F.interpolate(x, scale_factor=2)\n\
        \                x = self.skip(x)\n                if self.downsample:\n \
        \                   x = F.avg_pool2d(x, 2)\n\n            out = out + x\n\
        \            if self.scale_var:\n                out = out / np.sqrt(2)\n\
        \            return out\n\n    class Upsample1x1(nn.Module):\n        \"\"\
        \"Upsample 1x1 to 2x2 using Linear\"\"\"\n        def __init__(self, C_in,\
        \ C_out, norm='none', activ='relu', w_norm='none'):\n            assert norm.lower()\
        \ != 'in', 'Do not use instance norm for 1x1 spatial size'\n            super().__init__()\n\
        \            self.C_in = C_in\n            self.C_out = C_out\n          \
        \  self.proj = ConvBlock(\n                C_in, C_out*4, 1, 1, 0, norm=norm,\
        \ activ=activ, w_norm=w_norm\n            )\n\n        def forward(self, x):\n\
        \            # x: [B, C_in, 1, 1]\n            x = self.proj(x)  # [B, C_out*4,\
        \ 1, 1]\n            B, C = x.shape[:2]\n            return x.view(B, C//4,\
        \ 2, 2)\n\n    class HourGlass(nn.Module):\n        \"\"\"U-net like hourglass\
        \ module\"\"\"\n        def __init__(self, C_in, C_max, size, n_downs, n_mids=1,\
        \ norm='none', activ='relu',\n                    w_norm='none', pad_type='zero'):\n\
        \            \"\"\"\n            Args:\n                C_max: maximum C_out\
        \ of left downsampling block's output\n            \"\"\"\n            super().__init__()\n\
        \            assert size == n_downs ** 2, \"HGBlock assume that the spatial\
        \ size is downsampled to 1x1.\"\n            self.C_in = C_in\n\n        \
        \    ConvBlk = partial(ConvBlock, norm=norm, activ=activ, w_norm=w_norm, pad_type=pad_type)\n\
        \n            self.lefts = nn.ModuleList()\n            c_in = C_in\n    \
        \        for i in range(n_downs):\n                c_out = min(c_in*2, C_max)\n\
        \                self.lefts.append(ConvBlk(c_in, c_out, downsample=True))\n\
        \                c_in = c_out\n\n            # 1x1 conv for mids\n       \
        \     self.mids = nn.Sequential(\n                *[\n                   \
        \ ConvBlk(c_in, c_out, kernel_size=1, padding=0)\n                    for\
        \ _ in range(n_mids)\n                ]\n            )\n\n            self.rights\
        \ = nn.ModuleList()\n            for i, lb in enumerate(self.lefts[::-1]):\n\
        \                c_out = lb.C_in\n                c_in = lb.C_out\n      \
        \          channel_in = c_in*2 if i else c_in  # for channel concat\n    \
        \            if i == 0:\n                    block = Upsample1x1(channel_in,\
        \ c_out, norm=norm, activ=activ, w_norm=w_norm)\n                else:\n \
        \                   block = ConvBlk(channel_in, c_out, upsample=True)\n  \
        \              self.rights.append(block)\n\n        def forward(self, x):\n\
        \            features = []\n            for lb in self.lefts:\n          \
        \      x = lb(x)\n                features.append(x)\n\n            assert\
        \ x.shape[-2:] == torch.Size((1, 1))\n\n            for i, (rb, lf) in enumerate(zip(self.rights,\
        \ features[::-1])):\n                if i:\n                    x = torch.cat([x,\
        \ lf], dim=1)\n                x = rb(x)\n\n            return x\n\n    class\
        \ StyleEncoder(nn.Module):\n        def __init__(self, layers, out_shape):\n\
        \            super().__init__()\n\n            self.layers = nn.Sequential(*layers)\n\
        \            self.out_shape = out_shape\n\n        def forward(self, x):\n\
        \            style_feat = self.layers(x)\n\n            return style_feat\n\
        \n    def style_enc_builder(C_in, C, norm='none', activ='relu', pad_type='reflect',\
        \ skip_scale_var=False):\n\n        ConvBlk = partial(ConvBlock, norm=norm,\
        \ activ=activ, pad_type=pad_type)\n\n        layers = [\n            ConvBlk(C_in,\
        \ C, 3, 1, 1, norm='none', activ='none'),\n            ConvBlk(C*1, C*2, 3,\
        \ 1, 1, downsample=True),\n            GCBlock(C*2),\n            ConvBlk(C*2,\
        \ C*4, 3, 1, 1, downsample=True),\n            CBAM(C*4)\n        ]\n\n  \
        \      out_shape = (C*4, 32, 32)\n\n        return StyleEncoder(layers, out_shape)\n\
        \n    class CharAttar:\n        def __init__(self,num_classes,device,style_path):\n\
        \            self.num_classes = num_classes\n            self.device = device\n\
        \            self.contents_dim = 100\n            self.contents_emb = nn.Embedding(num_classes,\
        \ self.contents_dim)\n            self.style_enc = self.make_style_enc(os.path.join(style_path,\"\
        style_enc.pth\"))\n            self.style_conv = nn.Sequential(\n        \
        \                        nn.Conv2d(128,128,16),\n                        \
        \        nn.SiLU(),\n                            ).to(device)\n\n        def\
        \ make_stroke(self,contents):\n            strokes_list = []\n           \
        \ for content in contents:\n                content_code = ord(content)\n\
        \                first_letter_code = 44032\n                stroke = [0] *\
        \ 68\n                first_consonant_letter = int((content_code - first_letter_code)\
        \ / 588)\n                middle_consonant_letter = int(((content_code - first_letter_code)\
        \ - (first_consonant_letter * 588)) / 28)\n                last_consonant_letter\
        \ = int((content_code - first_letter_code) - (first_consonant_letter * 588)\
        \ - (middle_consonant_letter * 28))\n                stroke[first_consonant_letter]\
        \ = 1\n                stroke[middle_consonant_letter + 19] = 1\n        \
        \        stroke[last_consonant_letter + 19 + 21] = 1\n                strokes_list.append(stroke)\n\
        \            return strokes_list\n\n        def make_style_enc(self,style_enc_path):\n\
        \            C ,C_in = 32, 1\n            sty_encoder = style_enc_builder(C_in,\
        \ C)\n            checkpoint = torch.load(style_enc_path, map_location=self.device)\n\
        \            tmp_dict = {}\n            for k, v in checkpoint.items():\n\
        \                if k in sty_encoder.state_dict():\n                    tmp_dict[k]\
        \ = v\n            sty_encoder.load_state_dict(tmp_dict)\n            # frozen\
        \ sty_encoder\n            for p in sty_encoder.parameters():\n          \
        \      p.requires_grad = False\n            return sty_encoder.to(self.device)\n\
        \n        # def set_charAttr_dim(mode):\n        #     pass\n        def make_charAttr(self,images,contents_index,\
        \ contents,mode):\n            input_length = images.shape[0]\n          \
        \  # contents_index = [int(content_index) for content_index in contents_index]\n\
        \            # style_encoder = style_enc_builder(1,32).to(self.device)\n\n\
        \            contents_emb = None\n            stroke =  None\n           \
        \ style = None\n            contents_p, stroke_p = random.random(), random.random()\n\
        \            if mode == 1:\n\n                if contents_p < 0.3:\n     \
        \               contents_emb = torch.zeros(input_length,self.contents_dim)\n\
        \                else:\n                    contents_emb = torch.FloatTensor(self.contents_emb(contents_index))\n\
        \n                if stroke_p < 0.3:\n                    stroke = torch.zeros(input_length,68)\n\
        \                else:\n                    stroke =  torch.FloatTensor(self.make_stroke(contents))\n\
        \n                style = torch.zeros(input_length,128)\n\n            elif\
        \ mode == 2:\n\n                if contents_p < 0.3:\n                   \
        \ contents_emb = torch.zeros(input_length,self.contents_dim)\n           \
        \     else:\n                    contents_emb = torch.FloatTensor(self.contents_emb(contents_index))\n\
        \n                if stroke_p < 0.3:\n                    stroke = torch.zeros(input_length,68)\n\
        \                else:\n                    stroke = torch.FloatTensor(self.make_stroke(contents))\n\
        \n                if contents_p < 0.3 and stroke_p < 0.3:\n              \
        \      style = torch.zeros(input_length,128)\n                else:\n    \
        \                style = self.style_enc(images)\n                    # style\
        \ = F.adaptive_avg_pool2d(style, (1, 1))\n                    style = self.style_conv(style)\n\
        \                    style = style.view(input_length, -1).cpu()\n\n      \
        \      elif mode == 3: #test\n                contents_emb = torch.FloatTensor(self.contents_emb(contents_index))\n\
        \                stroke = torch.FloatTensor(self.make_stroke(contents))\n\
        \                style = self.style_enc(images)\n                # style =\
        \ F.adaptive_avg_pool2d(style, (1, 1))\n                style = self.style_conv(style)\n\
        \                style = style.view(input_length, -1).cpu()\n\n          \
        \  elif mode == 4:\n                contents_emb = torch.FloatTensor(self.contents_emb(contents_index))\n\
        \                stroke = torch.FloatTensor(self.make_stroke(contents))\n\
        \                style = torch.zeros(input_length,128)\n\n            return\
        \ torch.cat([contents_emb,stroke,style],dim=1)\n\n    if os.path.isdir(result_model_path):\n\
        \        pass\n    else:\n        os.mkdir(result_model_path)\n    os.environ['CUDA_VISIBLE_DEVICES']\
        \ = str(gpu_num)\n    device = torch.device('cuda' if torch.cuda.is_available()\
        \ else 'cpu')\n    transforms = torchvision.transforms.Compose([\n       \
        \     # torchvision.transforms.Resize((input_size,input_size)),\n        \
        \    torchvision.transforms.Grayscale(num_output_channels=1),\n          \
        \  torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize((0.5),\
        \ (0.5))\n        ])\n    dataset = DiffusionDataset(\"diffusion_font_train.csv\"\
        ,transform=transforms)\n    dataloader = DataLoader(dataset, batch_size=batch_size,\
        \ shuffle=True,num_workers=12)\n    model = UNet().to(device)\n    optimizer\
        \ = optim.AdamW(model.parameters(), lr=lr)\n    loss_func = nn.MSELoss()\n\
        \    diffusion = Diffusion(first_beta=1e-4,\n                            end_beta=0.02,\n\
        \                            noise_step=1000,\n                          \
        \  beta_schedule_type='linear',\n                            img_size=input_size,\n\
        \                            device=device)\n\n    model_value = 0\n    for\
        \ epoch_id in range(n_epochs):\n        for i, (images, contents_label,filename)\
        \ in enumerate(dataloader):\n            images = images.to(device)\n\n  \
        \          contents = [dataset.label_to_y[int(content_index)] for content_index\
        \ in contents_label]\n            charAttar = CharAttar(num_classes=num_classes,device=device,style_path=style_path)\n\
        \            print(epoch_id)\n\n            charAttr_list = charAttar.make_charAttr(images,\
        \ contents_label,contents,mode=mode).to(device)\n\n            t = diffusion.sample_t(images.shape[0]).to(device)\n\
        \            x_t, noise = diffusion.noise_images(images, t)\n\n          \
        \  predicted_noise = model(x_t, t, charAttr_list)\n            loss = loss_func(noise,\
        \ predicted_noise)\n\n            optimizer.zero_grad()\n            loss.backward()\n\
        \            optimizer.step()\n        if epoch_id % 10 == 0:\n          \
        \  labels = torch.arange(num_classes).long().to(device)\n            # sampled_images\
        \ = diffusion.portion_sampling(model, n=len(dataset.labels),sampleImage_len\
        \ = sampleImage_len,dataset=dataset,mode =mode,charAttar=charAttar,sample_img=sample_img)\n\
        \            # plot_images(sampled_images)\n            torch.save(model,os.path.join(result_model_path,f\"\
        model_{epoch_id}.pt\"))\n            torch.save(model.state_dict(), os.path.join(result_model_path,\
        \ f\"ckpt_{epoch_id}.pt\"))\n            torch.save(optimizer.state_dict(),\
        \ os.path.join(result_model_path, f\"optim_{epoch_id}.pt\"))\n    return model_value\n\
        \ndef _serialize_float(float_value: float) -> str:\n    if isinstance(float_value,\
        \ str):\n        return float_value\n    if not isinstance(float_value, (float,\
        \ int)):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of\
        \ float.'.format(\n            str(float_value), str(type(float_value))))\n\
        \    return str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Model\
        \ training', description='')\n_parser.add_argument(\"--gpu-num\", dest=\"\
        gpu_num\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --batch-size\", dest=\"batch_size\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--input-size\", dest=\"input_size\", type=int, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--n-epochs\", dest=\"\
        n_epochs\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --num-classes\", dest=\"num_classes\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--mode\", dest=\"mode\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--lr\", dest=\"lr\", type=float, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--result-model-path\", dest=\"result_model_path\",\
        \ type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --style-path\", dest=\"style_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = model_training(**_parsed_args)\n\n_outputs\
        \ = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\n\
        import os\nfor idx, output_file in enumerate(_output_files):\n    try:\n \
        \       os.makedirs(os.path.dirname(output_file))\n    except OSError:\n \
        \       pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: python:3.10
      resources:
        limits: {cpu: '8', memory: 16G}
      volumeMounts:
      - {mountPath: pvc, name: test-lee}
    inputs:
      parameters:
      - {name: batch_size}
      - {name: gpu_num}
      - {name: image_size}
      - {name: lr}
      - {name: mode}
      - {name: n_epochs}
      - {name: num_classes}
      - {name: result_model_path}
      - {name: style_path}
    outputs:
      parameters:
      - name: model-training-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: model-training-Output, path: /tmp/outputs/Output/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Training Korean Diffusion
          Model, pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--gpu-num", {"inputValue": "gpu_num"}, "--batch-size", {"inputValue":
          "batch_size"}, "--input-size", {"inputValue": "input_size"}, "--n-epochs",
          {"inputValue": "n_epochs"}, "--num-classes", {"inputValue": "num_classes"},
          "--mode", {"inputValue": "mode"}, "--lr", {"inputValue": "lr"}, "--result-model-path",
          {"inputValue": "result_model_path"}, "--style-path", {"inputValue": "style_path"},
          "----output-paths", {"outputPath": "Output"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''tqdm==4.65.0''
          ''pandas==2.0.0'' '' pillow==9.4.0'' ''torch==2.0.0+cu118'' ''torchvision==0.15.1+cu118''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''tqdm==4.65.0'' ''pandas==2.0.0'' '' pillow==9.4.0'' ''torch==2.0.0+cu118''
          ''torchvision==0.15.1+cu118'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def model_training(gpu_num, batch_size, input_size, n_epochs, num_classes,
          mode, lr, result_model_path, style_path):\n    import os\n    import tqdm\n    import
          math\n    import random\n    import pandas as pd\n    import numpy as np\n    from
          PIL import Image\n\n    import torch, torchvision\n    from torch import
          optim\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from
          torch.utils.data import DataLoader, TensorDataset,Dataset\n\n    from functools
          import partial\n\n    from PIL import Image\n    class DiffusionDataset(Dataset):\n        def
          __init__(self, csv_path, transform =None):\n            self.transform =
          transform\n            self.csv_data = pd.read_csv(os.path.join(csv_path,\"diffusion_font_train.csv\"))\n            self.x_file_name
          = self.csv_data.iloc[:,0]\n            self.x_path = self.csv_data.iloc[:,1]\n            self.y
          = self.csv_data.iloc[:,2]\n            self.labels = np.unique(self.y)\n            self.y_to_label
          = self.make_y_to_label()\n            self.label_to_y = self.make_label_to_y()\n            self.y_labels
          = self.make_y_labels()\n\n        def make_y_to_label(self):\n            y_to_label_dict
          = {}\n            for label, value in enumerate(self.labels):\n                y_to_label_dict[value]
          = label\n            return y_to_label_dict\n\n        def make_label_to_y(self):\n            label_to_y_dict
          = {}\n            for label, value in enumerate(self.labels):\n                label_to_y_dict[label]
          = value\n            return label_to_y_dict\n\n        def make_y_labels(self):\n            y_labels
          = []\n            for y_ch in self.y:\n                y_labels.append(self.y_to_label[y_ch])\n            return
          y_labels\n\n        def __len__(self):\n            return len(self.x_path)\n\n        def
          __getitem__(self, id_):\n            filename = self.x_file_name[id_]\n            x
          = Image.open(self.x_path[id_])\n            transform_x = self.transform(x)\n            label
          = self.y_labels[id_]\n\n            return transform_x, label, filename\n\n    class
          Diffusion:    \n        def __init__(self, first_beta, end_beta, beta_schedule_type,
          noise_step, img_size, device):\n            self.first_beta = first_beta\n            self.end_beta
          = end_beta\n            self.beta_schedule_type = beta_schedule_type\n\n            self.noise_step
          = noise_step\n\n            self.beta_list = self.beta_schedule().to(device)\n\n            self.alphas
          =  1. - self.beta_list\n            self.alpha_bars = torch.cumprod(self.alphas,
          dim = 0)\n\n            self.img_size = img_size\n            self.device
          = device\n\n        def sample_t(self, batch_size):\n            return
          torch.randint(1,self.noise_step,(batch_size,))\n\n        def beta_schedule(self):\n            if
          self.beta_schedule_type == \"linear\":\n                return torch.linspace(self.first_beta,
          self.end_beta, self.noise_step)\n            elif self.beta_schedule_type
          == \"cosine\":\n                steps = self.noise_step + 1\n                s
          = 0.008\n                x = torch.linspace(0, self.noise_step, steps)\n                alphas_cumprod
          = torch.cos(((x / self.noise_step) + s) / (1 + s) * torch.pi * 0.5) ** 2\n                alphas_cumprod
          = alphas_cumprod / alphas_cumprod[0]\n                betas = 1 - (alphas_cumprod[1:]
          / alphas_cumprod[:-1])\n                return torch.clip(betas, 0.0001,
          0.9999)\n            elif self.beta_schedule_type == \"quadratic\":\n                return
          torch.linspace(self.first_beta ** 0.5, self.end_beta ** 0.5, self.noise_step)
          ** 2\n            elif self.beta_schedule_type == \"sigmoid\":\n                beta
          = torch.linspace(-6,-6,self.noise_step)\n                return torch.sigmoid(beta)
          * (self.end_beta - self.first_beta) + self.first_beta\n\n        def alpha_t(self,
          t):\n            return self.alphas[t][:, None, None, None]\n\n        def
          alpha_bar_t (self,t):\n            return self.alpha_bars[t][:, None, None,
          None]\n\n        def one_minus_alpha_bar(self,t):\n            return (1.
          - self.alpha_bars[t])[:, None, None, None]\n\n        def beta_t(self,t):\n            return
          self.beta_list[t][:, None, None, None]\n\n        def noise_images(self,x,t):\n            epsilon
          = torch.randn_like(x)\n            return torch.sqrt(self.alpha_bar_t(t))
          * x + torch.sqrt(self.one_minus_alpha_bar(t)) * epsilon , epsilon\n\n        def
          indexToChar(self,y):\n            return chr(44032+y)\n        def portion_sampling(self,
          model, n,sampleImage_len,dataset,mode,charAttar,sample_img, cfg_scale=3):\n            example_images
          = []\n            model.eval()\n            with torch.no_grad():\n                x_list
          = torch.randn((sampleImage_len, 1, self.img_size, self.img_size)).to(self.device)\n\n                y_idx
          = list(range(n))[::math.floor(n/sampleImage_len)][:sampleImage_len]\n                contents_index
          = torch.IntTensor(y_idx)\n                contents = [dataset.label_to_y[int(content_index)]
          for content_index in contents_index]\n                charAttr_list = charAttar.make_charAttr(sample_img,
          contents_index, contents,mode=3).to(self.device)\n\n                pbar
          = tqdm(list(reversed(range(1, self.noise_step))),desc=\"sampling\")\n                for
          i in pbar:\n                    dataset = TensorDataset(x_list,charAttr_list)\n                    batch_size=
          18\n                    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=False)\n                    predicted_noise
          = torch.tensor([]).to(self.device)\n                    uncond_predicted_noise
          = torch.tensor([]).to(self.device)\n                    for batch_x, batch_conditions
          in dataloader:\n                        batch_t = (torch.ones(len(batch_x))
          * i).long().to(self.device)\n                        batch_noise = model(batch_x,
          batch_t, batch_conditions)\n                        predicted_noise = torch.cat([predicted_noise,batch_noise],dim=0)\n                        #uncodition\n                        uncond_batch_noise
          = model(batch_x, batch_t, torch.zeros_like(batch_conditions))\n                        uncond_predicted_noise
          = torch.cat([uncond_predicted_noise,uncond_batch_noise],dim = 0)\n\n                    if
          cfg_scale > 0:\n                        predicted_noise = torch.lerp(uncond_predicted_noise,
          predicted_noise, cfg_scale)\n\n                    t = (torch.ones(sampleImage_len)
          * i).long()\n                    a_t = self.alpha_t(t)\n                    aBar_t
          = self.alpha_bar_t(t)\n                    b_t = self.beta_t(t)\n\n                    if
          i > 1:\n                        noise = torch.randn_like(x_list)\n                    else:\n                        noise
          = torch.zeros_like(x_list)\n\n                    x_list = 1 / torch.sqrt(a_t)
          * (\n                            x_list - ((1 - a_t) / (torch.sqrt(1 - aBar_t)))
          * predicted_noise) + torch.sqrt(\n                        b_t) * noise\n            model.train()\n            x_list
          = (x_list.clamp(-1, 1) + 1) / 2\n            x_list = (x_list * 255).type(torch.uint8)\n            return
          x_list\n\n        def test_sampling(self, model,sampleImage_len,charAttr_list,cfg_scale=3):\n            example_images
          = []\n            model.eval()\n            with torch.no_grad():\n                x_list
          = torch.randn((sampleImage_len, 3, self.img_size, self.img_size)).to(self.device)\n                pbar
          = tqdm(list(reversed(range(1, self.noise_step))),desc=\"sampling\")\n                for
          i in pbar:\n                    dataset = TensorDataset(x_list,charAttr_list)\n                    batch_size=
          4\n                    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=False)\n                    predicted_noise
          = torch.tensor([]).to(self.device)\n                    uncond_predicted_noise
          = torch.tensor([]).to(self.device)\n                    for batch_x, batch_conditions
          in dataloader:\n                        batch_t = (torch.ones(len(batch_x))
          * i).long().to(self.device)\n                        batch_noise = model(batch_x,
          batch_t, batch_conditions)\n                        predicted_noise = torch.cat([predicted_noise,batch_noise],dim=0)\n                        #uncodition\n                        uncond_batch_noise
          = model(batch_x, batch_t, torch.zeros_like(batch_conditions))\n                        uncond_predicted_noise
          = torch.cat([uncond_predicted_noise,uncond_batch_noise],dim = 0)\n\n                    if
          cfg_scale > 0:\n                        predicted_noise = torch.lerp(uncond_predicted_noise,
          predicted_noise, cfg_scale)\n\n                    t = (torch.ones(sampleImage_len)
          * i).long()\n                    a_t = self.alpha_t(t)\n                    aBar_t
          = self.alpha_bar_t(t)\n                    b_t = self.beta_t(t)\n\n                    if
          i > 1:\n                        noise = torch.randn_like(x_list)\n                    else:\n                        noise
          = torch.zeros_like(x_list)\n\n                    x_list = 1 / torch.sqrt(a_t)
          * (\n                            x_list - ((1 - a_t) / (torch.sqrt(1 - aBar_t)))
          * predicted_noise) + torch.sqrt(\n                        b_t) * noise\n            x_list
          = (x_list.clamp(-1, 1) + 1) / 2\n            x_list = (x_list * 255).type(torch.uint8)\n            return
          x_list\n\n    class SelfAttention(nn.Module):\n        def __init__(self,
          channels):\n            super(SelfAttention, self).__init__()\n            self.channels
          = channels\n            self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n            self.ln
          = nn.LayerNorm([channels])\n            self.ff_self = nn.Sequential(\n                nn.LayerNorm([channels]),\n                nn.Linear(channels,
          channels),\n                nn.GELU(),\n                nn.Linear(channels,
          channels),\n            )\n\n        def forward(self, x):\n            size
          = x.shape[-1]\n            x = x.view(-1, self.channels, size * size).swapaxes(1,
          2)\n            x_ln = self.ln(x)\n            attention_value, _ = self.mha(x_ln,
          x_ln, x_ln)\n            attention_value = attention_value + x\n            attention_value
          = self.ff_self(attention_value) + attention_value\n            return attention_value.swapaxes(2,
          1).view(-1, self.channels, size, size)\n\n    class DoubleConv(nn.Module):\n        def
          __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n            super().__init__()\n            self.residual
          = residual\n            if not mid_channels:\n                mid_channels
          = out_channels\n            self.double_conv = nn.Sequential(\n                nn.Conv2d(in_channels,
          mid_channels, kernel_size=3, padding=1, bias=False),\n                nn.GroupNorm(1,
          mid_channels),\n                nn.GELU(),\n                nn.Conv2d(mid_channels,
          out_channels, kernel_size=3, padding=1, bias=False),\n                nn.GroupNorm(1,
          out_channels),\n            )\n\n        def forward(self, x):\n            if
          self.residual:\n                return F.gelu(x + self.double_conv(x))\n            else:\n                return
          self.double_conv(x)\n\n    class Down(nn.Module):\n        def __init__(self,
          in_channels, out_channels, time_dim=256, charAttr_dim=12456):\n            super().__init__()\n            self.maxpool_conv
          = nn.Sequential(\n                nn.MaxPool2d(2),\n                DoubleConv(in_channels,
          in_channels, residual=True),\n                DoubleConv(in_channels, out_channels),\n            )\n\n            self.time_layer
          = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(\n                    time_dim,\n                    out_channels\n                ),\n            )\n\n            self.condition_layer
          = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(\n                    charAttr_dim,\n                    out_channels\n                ),\n            )\n\n        def
          forward(self, x, t,charAttr):\n            x = self.maxpool_conv(x)\n            emb
          = self.time_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n            charAttr_emb
          = self.condition_layer(charAttr)[:, :, None, None].repeat(1, 1, x.shape[-2],
          x.shape[-1])\n            return x + emb + charAttr_emb\n\n    class Up(nn.Module):\n        def
          __init__(self, in_channels, out_channels, time_dim=256, charAttr_dim=12456):\n            super().__init__()\n\n            self.up
          = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n            self.conv
          = nn.Sequential(\n                DoubleConv(in_channels, in_channels, residual=True),\n                DoubleConv(in_channels,
          out_channels, in_channels // 2),\n            )\n\n            self.time_layer
          = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(\n                    time_dim,\n                    out_channels\n                ),\n            )\n            self.condition_layer
          = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(\n                    charAttr_dim,\n                    out_channels\n                ),\n            )\n\n        def
          forward(self, x, skip_x, t, charAttr):\n            x = self.up(x)\n            x
          = torch.cat([skip_x, x], dim=1)\n            x = self.conv(x)\n            time_emb
          = self.time_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n            charAttr_emb
          = self.condition_layer(charAttr)[:, :, None, None].repeat(1, 1, x.shape[-2],
          x.shape[-1])\n            return x + time_emb + charAttr_emb\n\n    class
          UNet(nn.Module):\n        def __init__(self, c_in=1, c_out=1, time_dim=256,
          charAttr_dim = 296, device=\"cuda\"):\n            super().__init__()\n            self.device
          = device\n            self.time_dim = time_dim\n            self.charAttr_dim
          = charAttr_dim\n\n            self.inc = DoubleConv(c_in, 64)\n            self.down1
          = Down(64, 128, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n            self.sa1
          = SelfAttention(128)\n            self.down2 = Down(128, 256,time_dim=self.time_dim,
          charAttr_dim=self.charAttr_dim)\n            self.sa2 = SelfAttention(256)\n            self.down3
          = Down(256, 256, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n            self.sa3
          = SelfAttention(256)\n\n            self.bot1 = DoubleConv(256, 512)\n            self.bot2
          = DoubleConv(512, 512)\n            self.bot3 = DoubleConv(512, 256)\n\n            self.up1
          = Up(512, 128, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n            self.sa4
          = SelfAttention(128)\n            self.up2 = Up(256, 64, time_dim=self.time_dim,
          charAttr_dim=self.charAttr_dim)\n            self.sa5 = SelfAttention(64)\n            self.up3
          = Up(128, 64, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n            self.sa6
          = SelfAttention(64)\n            self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n\n        def
          pos_encoding(self, t, channels):\n            inv_freq = 1.0 / (\n                10000\n                **
          (torch.arange(0, channels, 2, device=self.device).float() / channels)\n            )\n            pos_enc_a
          = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n            pos_enc_b
          = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n            pos_enc
          = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n            return pos_enc\n\n        def
          forward(self, x, time, charAttr):\n            time = time.unsqueeze(-1).type(torch.float)\n            time
          = self.pos_encoding(time, self.time_dim)\n\n            # if y is not None:\n            #     time
          += self.contents_emb(y)\n\n            x1 = self.inc(x)\n            x2
          = self.down1(x1, time, charAttr)\n            x2 = self.sa1(x2)\n            x3
          = self.down2(x2, time, charAttr)\n            x3 = self.sa2(x3)\n            x4
          = self.down3(x3, time, charAttr)\n            x4 = self.sa3(x4)\n\n            x4
          = self.bot1(x4)\n            x4 = self.bot2(x4)\n            x4 = self.bot3(x4)\n\n            x
          = self.up1(x4, x3, time, charAttr)\n            x = self.sa4(x)\n            x
          = self.up2(x, x2, time, charAttr)\n            x = self.sa5(x)\n            x
          = self.up3(x, x1, time, charAttr)\n            x = self.sa6(x)\n            output
          = self.outc(x)\n            return output\n\n    class GlobalContext(nn.Module):\n        \"\"\"
          Global-context \"\"\"\n        def __init__(self, C, bottleneck_ratio=0.25,
          w_norm=''none''):\n            super().__init__()\n            C_bottleneck
          = int(C * bottleneck_ratio)\n            w_norm = w_norm_dispatch(w_norm)\n            self.k_proj
          = w_norm(nn.Conv2d(C, 1, 1))\n            self.transform = nn.Sequential(\n                w_norm(nn.Linear(C,
          C_bottleneck)),\n                nn.LayerNorm(C_bottleneck),\n                nn.ReLU(),\n                w_norm(nn.Linear(C_bottleneck,
          C))\n            )\n\n        def forward(self, x):\n            # x: [B,
          C, H, W]\n            context_logits = self.k_proj(x)  # [B, 1, H, W]\n            context_weights
          = F.softmax(context_logits.flatten(1), dim=1)  # [B, HW]\n            context
          = torch.einsum(''bci,bi->bc'', x.flatten(2), context_weights)\n            out
          = self.transform(context)\n\n            return out[..., None, None]\n\n    class
          GCBlock(nn.Module):\n        \"\"\" Global-context block \"\"\"\n        def
          __init__(self, C, bottleneck_ratio=0.25, w_norm=''none''):\n            super().__init__()\n            self.gc
          = GlobalContext(C, bottleneck_ratio, w_norm)\n\n        def forward(self,
          x):\n            gc = self.gc(x)\n            return x + gc\n\n    class
          TLU(nn.Module):\n        \"\"\" Thresholded Linear Unit \"\"\"\n        def
          __init__(self, num_features):\n            super().__init__()\n            self.num_features
          = num_features\n            self.tau = nn.Parameter(torch.zeros(1, num_features,
          1, 1))\n\n        def forward(self, x):\n            return torch.max(x,
          self.tau)\n\n        def extra_repr(self):\n            return ''num_features={}''.format(self.num_features)\n\n    #
          NOTE generalized version\n    class FilterResponseNorm(nn.Module):\n        \"\"\"
          Filter Response Normalization \"\"\"\n        def __init__(self, num_features,
          ndim, eps=None, learnable_eps=False):\n            \"\"\"\n            Args:\n                num_features\n                ndim\n                eps:
          if None is given, use the paper value as default.\n                    from
          paper, fixed_eps=1e-6 and learnable_eps_init=1e-4.\n                learnable_eps:
          turn eps to learnable parameter, which is recommended on\n                    fully-connected
          or 1x1 activation map.\n            \"\"\"\n            super().__init__()\n            if
          eps is None:\n                if learnable_eps:\n                    eps
          = 1e-4\n                else:\n                    eps = 1e-6\n\n            self.num_features
          = num_features\n            self.init_eps = eps\n            self.learnable_eps
          = learnable_eps\n            self.ndim = ndim\n\n            self.mean_dims
          = list(range(2, 2+ndim))\n\n            self.weight = nn.Parameter(torch.ones([1,
          num_features] + [1]*ndim))\n            self.bias = nn.Parameter(torch.zeros([1,
          num_features] + [1]*ndim))\n            if learnable_eps:\n                self.eps
          = nn.Parameter(torch.as_tensor(eps))\n            else:\n                self.register_buffer(''eps'',
          torch.as_tensor(eps))\n\n        def forward(self, x):\n            # normalize\n            nu2
          = x.pow(2).mean(self.mean_dims, keepdim=True)\n            x = x * torch.rsqrt(nu2
          + self.eps.abs())\n\n            # modulation\n            x = x * self.weight
          + self.bias\n\n            return x\n\n        def extra_repr(self):\n            return
          ''num_features={}, init_eps={}, ndim={}''.format(\n                    self.num_features,
          self.init_eps, self.ndim)\n\n    FilterResponseNorm1d = partial(FilterResponseNorm,
          ndim=1, learnable_eps=True)\n    FilterResponseNorm2d = partial(FilterResponseNorm,
          ndim=2)\n\n    def split_dim(x, dim, n_chunks):\n        shape = x.shape\n        assert
          shape[dim] % n_chunks == 0\n        return x.view(*shape[:dim], n_chunks,
          shape[dim] // n_chunks, *shape[dim+1:])\n\n    def weights_init(init_type=''default''):\n        \"\"\"
          Adopted from FUNIT \"\"\"\n        def init_fun(m):\n            classname
          = m.__class__.__name__\n            if (classname.find(''Conv'') == 0 or
          classname.find(''Linear'') == 0) and hasattr(m, ''weight''):\n                if
          init_type == ''gaussian'':\n                    nn.init.normal_(m.weight.data,
          0.0, 0.02)\n                elif init_type == ''xavier'':\n                    nn.init.xavier_normal_(m.weight.data,
          gain=2**0.5)\n                elif init_type == ''kaiming'':\n                    nn.init.kaiming_normal_(m.weight.data,
          a=0, mode=''fan_in'')\n                elif init_type == ''orthogonal'':\n                    nn.init.orthogonal_(m.weight.data,
          gain=2**0.5)\n                elif init_type == ''default'':\n                    pass\n                else:\n                    assert
          0, \"Unsupported initialization: {}\".format(init_type)\n\n                if
          hasattr(m, ''bias'') and m.bias is not None:\n                    nn.init.constant_(m.bias.data,
          0.0)\n\n        return init_fun\n\n    def spectral_norm(module):\n        \"\"\"
          init & apply spectral norm \"\"\"\n        nn.init.xavier_uniform_(module.weight,
          2 ** 0.5)\n        if hasattr(module, ''bias'') and module.bias is not None:\n            module.bias.data.zero_()\n\n        return
          nn.utils.spectral_norm(module)\n\n    class BasicConv(nn.Module):\n        def
          __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0,
          dilation=1, groups=1, relu=True, bn=True, bias=False):\n            super(BasicConv,
          self).__init__()\n            self.out_channels = out_planes\n            self.conv
          = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,
          padding=padding, dilation=dilation, groups=groups, bias=bias)\n            self.bn
          = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn
          else None\n            self.relu = nn.ReLU() if relu else None\n\n        def
          forward(self, x):\n            x = self.conv(x)\n            if self.bn
          is not None:\n                x = self.bn(x)\n            if self.relu is
          not None:\n                x = self.relu(x)\n            return x\n\n    class
          Flatten(nn.Module):\n\n        def forward(self, x):\n            return
          x.view(x.size(0), -1)\n\n    class ChannelGate(nn.Module):\n        def
          __init__(self, gate_channels, reduction_ratio=16, pool_types=[''avg'', ''max'']):\n            super(ChannelGate,
          self).__init__()\n            self.gate_channels = gate_channels\n            self.mlp
          = nn.Sequential(\n                Flatten(),\n                nn.Linear(gate_channels,
          gate_channels // reduction_ratio),\n                nn.ReLU(),\n                nn.Linear(gate_channels
          // reduction_ratio, gate_channels)\n                )\n            self.pool_types
          = pool_types\n\n        def forward(self, x):\n            channel_att_sum
          = None\n            for pool_type in self.pool_types:\n                if
          pool_type == ''avg'':\n                    avg_pool = F.avg_pool2d(x, (x.size(2),
          x.size(3)), stride=(x.size(2), x.size(3)))\n                    channel_att_raw
          = self.mlp(avg_pool)\n                elif pool_type == ''max'':\n                    max_pool
          = F.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n                    channel_att_raw
          = self.mlp(max_pool)\n                elif pool_type == ''lp'':\n                    lp_pool
          = F.lp_pool2d(x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n                    channel_att_raw
          = self.mlp(lp_pool)\n                elif pool_type == ''lse'':\n                    #
          LSE pool only\n                    lse_pool = logsumexp_2d(x)\n                    channel_att_raw
          = self.mlp(lse_pool)\n\n                if channel_att_sum is None:\n                    channel_att_sum
          = channel_att_raw\n                else:\n                    channel_att_sum
          = channel_att_sum + channel_att_raw\n\n            scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n            return
          x * scale\n\n    def logsumexp_2d(tensor):\n        tensor_flatten = tensor.view(tensor.size(0),
          tensor.size(1), -1)\n        s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n        outputs
          = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n        return
          outputs\n\n    class ChannelPool(nn.Module):\n        def forward(self,
          x):\n            return torch.cat((torch.max(x, 1)[0].unsqueeze(1), torch.mean(x,
          1).unsqueeze(1)), dim=1)\n\n    class SpatialGate(nn.Module):\n        def
          __init__(self):\n            super(SpatialGate, self).__init__()\n            kernel_size
          = 7\n            self.compress = ChannelPool()\n            self.spatial
          = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n\n        def
          forward(self, x):\n            x_compress = self.compress(x)\n            x_out
          = self.spatial(x_compress)\n            scale = torch.sigmoid(x_out)  #
          broadcasting\n            return x * scale\n\n    class CBAM(nn.Module):\n        def
          __init__(self, gate_channels, reduction_ratio=16, pool_types=[''avg'', ''max''],
          no_spatial=False):\n            super(CBAM, self).__init__()\n            self.ChannelGate
          = ChannelGate(gate_channels, reduction_ratio, pool_types)\n            self.no_spatial
          = no_spatial\n            if not no_spatial:\n                self.SpatialGate
          = SpatialGate()\n\n        def forward(self, x):\n            x_out = self.ChannelGate(x)\n            if
          not self.no_spatial:\n                x_out = self.SpatialGate(x_out)\n            return
          x_out\n\n    class Flatten(nn.Module):\n        def __init__(self, start_dim=1,
          end_dim=-1):\n            super(Flatten, self).__init__()\n            self.start_dim
          = start_dim\n            self.end_dim = end_dim\n\n        def forward(self,
          input):\n            return input.flatten(self.start_dim, self.end_dim)\n\n    def
          dispatcher(dispatch_fn):\n        def decorated(key, *args):\n            if
          callable(key):\n                return key\n\n            if key is None:\n                key
          = ''none''\n\n            return dispatch_fn(key, *args)\n        return
          decorated\n\n    @dispatcher\n    def norm_dispatch(norm):\n        return
          {\n            ''none'': nn.Identity,\n            ''in'': partial(nn.InstanceNorm2d,
          affine=False),  # false as default\n            ''bn'': nn.BatchNorm2d,\n            ''frn'':
          FilterResponseNorm2d\n        }[norm.lower()]\n\n    @dispatcher\n    def
          w_norm_dispatch(w_norm):\n        # NOTE Unlike other dispatcher, w_norm
          is function, not class.\n        return {\n            ''spectral'': spectral_norm,\n            ''none'':
          lambda x: x\n        }[w_norm.lower()]\n\n    @dispatcher\n    def activ_dispatch(activ,
          norm=None):\n        if norm_dispatch(norm) == FilterResponseNorm2d:\n            #
          use TLU for FRN\n            activ = ''tlu''\n\n        return {\n            \"none\":
          nn.Identity,\n            \"relu\": nn.ReLU,\n            \"lrelu\": partial(nn.LeakyReLU,
          negative_slope=0.2),\n            \"tlu\": TLU\n        }[activ.lower()]\n\n    @dispatcher\n    def
          pad_dispatch(pad_type):\n        return {\n            \"zero\": nn.ZeroPad2d,\n            \"replicate\":
          nn.ReplicationPad2d,\n            \"reflect\": nn.ReflectionPad2d\n        }[pad_type.lower()]\n\n    class
          ParamBlock(nn.Module):\n        def __init__(self, C_out, shape):\n            super().__init__()\n            w
          = torch.randn((C_out, *shape))\n            b = torch.randn((C_out,))\n            self.shape
          = shape\n            self.w = nn.Parameter(w)\n            self.b = nn.Parameter(b)\n\n        def
          forward(self, x):\n            b = self.b.reshape((1, *self.b.shape, 1,
          1, 1)).repeat(x.size(0), 1, *self.shape)\n            return self.w*x +
          b\n\n    class LinearBlock(nn.Module):\n        \"\"\" pre-active linear
          block \"\"\"\n        def __init__(self, C_in, C_out, norm=''none'', activ=''relu'',
          bias=True, w_norm=''none'',\n                    dropout=0.):\n            super().__init__()\n            activ
          = activ_dispatch(activ, norm)\n            if norm.lower() == ''bn'':\n                norm
          = nn.BatchNorm1d\n            elif norm.lower() == ''frn'':\n                norm
          = FilterResponseNorm1d\n            elif norm.lower() == ''none'':\n                norm
          = nn.Identity\n            else:\n                raise ValueError(f\"LinearBlock
          supports BN only (but {norm} is given)\")\n            w_norm = w_norm_dispatch(w_norm)\n            self.norm
          = norm(C_in)\n            self.activ = activ()\n            if dropout >
          0.:\n                self.dropout = nn.Dropout(p=dropout)\n            self.linear
          = w_norm(nn.Linear(C_in, C_out, bias))\n\n        def forward(self, x):\n            x
          = self.norm(x)\n            x = self.activ(x)\n            if hasattr(self,
          ''dropout''):\n                x = self.dropout(x)\n            return self.linear(x)\n\n    class
          ConvBlock(nn.Module):\n        \"\"\" pre-active conv block \"\"\"\n        def
          __init__(self, C_in, C_out, kernel_size=3, stride=1, padding=1, norm=''none'',\n                    activ=''relu'',
          bias=True, upsample=False, downsample=False, w_norm=''none'',\n                    pad_type=''zero'',
          dropout=0., size=None):\n            # 1x1 conv assertion\n            if
          kernel_size == 1:\n                assert padding == 0\n            super().__init__()\n            self.C_in
          = C_in\n            self.C_out = C_out\n\n            activ = activ_dispatch(activ,
          norm)\n            norm = norm_dispatch(norm)\n            w_norm = w_norm_dispatch(w_norm)\n            pad
          = pad_dispatch(pad_type)\n            self.upsample = upsample\n            self.downsample
          = downsample\n\n            assert ((norm == FilterResponseNorm2d) == (activ
          == TLU)), \"Use FRN and TLU together\"\n\n            if norm == FilterResponseNorm2d
          and size == 1:\n                self.norm = norm(C_in, learnable_eps=True)\n            else:\n                self.norm
          = norm(C_in)\n            if activ == TLU:\n                self.activ =
          activ(C_in)\n            else:\n                self.activ = activ()\n            if
          dropout > 0.:\n                self.dropout = nn.Dropout2d(p=dropout)\n            self.pad
          = pad(padding)\n            self.conv = w_norm(nn.Conv2d(C_in, C_out, kernel_size,
          stride, bias=bias))\n\n        def forward(self, x):\n            x = self.norm(x)\n            x
          = self.activ(x)\n            if self.upsample:\n                x = F.interpolate(x,
          scale_factor=2)\n            if hasattr(self, ''dropout''):\n                x
          = self.dropout(x)\n            x = self.conv(self.pad(x))\n            if
          self.downsample:\n                x = F.avg_pool2d(x, 2)\n            return
          x\n\n    class ResBlock(nn.Module):\n        \"\"\" Pre-activate ResBlock
          with spectral normalization \"\"\"\n        def __init__(self, C_in, C_out,
          kernel_size=3, padding=1, upsample=False, downsample=False,\n                    norm=''none'',
          w_norm=''none'', activ=''relu'', pad_type=''zero'', dropout=0.,\n                    scale_var=False):\n            assert
          not (upsample and downsample)\n            super().__init__()\n            w_norm
          = w_norm_dispatch(w_norm)\n            self.C_in = C_in\n            self.C_out
          = C_out\n            self.upsample = upsample\n            self.downsample
          = downsample\n            self.scale_var = scale_var\n\n            self.conv1
          = ConvBlock(C_in, C_out, kernel_size, 1, padding, norm, activ,\n                                upsample=upsample,
          w_norm=w_norm, pad_type=pad_type,\n                                dropout=dropout)\n            self.conv2
          = ConvBlock(C_out, C_out, kernel_size, 1, padding, norm, activ,\n                                w_norm=w_norm,
          pad_type=pad_type, dropout=dropout)\n\n            # XXX upsample / downsample
          needs skip conv?\n            if C_in != C_out or upsample or downsample:\n                self.skip
          = w_norm(nn.Conv2d(C_in, C_out, 1))\n\n        def forward(self, x):\n            \"\"\"\n            normal:
          pre-activ + convs + skip-con\n            upsample: pre-activ + upsample
          + convs + skip-con\n            downsample: pre-activ + convs + downsample
          + skip-con\n            => pre-activ + (upsample) + convs + (downsample)
          + skip-con\n            \"\"\"\n            out = x\n\n            out =
          self.conv1(out)\n            out = self.conv2(out)\n\n            if self.downsample:\n                out
          = F.avg_pool2d(out, 2)\n\n            # skip-con\n            if hasattr(self,
          ''skip''):\n                if self.upsample:\n                    x = F.interpolate(x,
          scale_factor=2)\n                x = self.skip(x)\n                if self.downsample:\n                    x
          = F.avg_pool2d(x, 2)\n\n            out = out + x\n            if self.scale_var:\n                out
          = out / np.sqrt(2)\n            return out\n\n    class Upsample1x1(nn.Module):\n        \"\"\"Upsample
          1x1 to 2x2 using Linear\"\"\"\n        def __init__(self, C_in, C_out, norm=''none'',
          activ=''relu'', w_norm=''none''):\n            assert norm.lower() != ''in'',
          ''Do not use instance norm for 1x1 spatial size''\n            super().__init__()\n            self.C_in
          = C_in\n            self.C_out = C_out\n            self.proj = ConvBlock(\n                C_in,
          C_out*4, 1, 1, 0, norm=norm, activ=activ, w_norm=w_norm\n            )\n\n        def
          forward(self, x):\n            # x: [B, C_in, 1, 1]\n            x = self.proj(x)  #
          [B, C_out*4, 1, 1]\n            B, C = x.shape[:2]\n            return x.view(B,
          C//4, 2, 2)\n\n    class HourGlass(nn.Module):\n        \"\"\"U-net like
          hourglass module\"\"\"\n        def __init__(self, C_in, C_max, size, n_downs,
          n_mids=1, norm=''none'', activ=''relu'',\n                    w_norm=''none'',
          pad_type=''zero''):\n            \"\"\"\n            Args:\n                C_max:
          maximum C_out of left downsampling block''s output\n            \"\"\"\n            super().__init__()\n            assert
          size == n_downs ** 2, \"HGBlock assume that the spatial size is downsampled
          to 1x1.\"\n            self.C_in = C_in\n\n            ConvBlk = partial(ConvBlock,
          norm=norm, activ=activ, w_norm=w_norm, pad_type=pad_type)\n\n            self.lefts
          = nn.ModuleList()\n            c_in = C_in\n            for i in range(n_downs):\n                c_out
          = min(c_in*2, C_max)\n                self.lefts.append(ConvBlk(c_in, c_out,
          downsample=True))\n                c_in = c_out\n\n            # 1x1 conv
          for mids\n            self.mids = nn.Sequential(\n                *[\n                    ConvBlk(c_in,
          c_out, kernel_size=1, padding=0)\n                    for _ in range(n_mids)\n                ]\n            )\n\n            self.rights
          = nn.ModuleList()\n            for i, lb in enumerate(self.lefts[::-1]):\n                c_out
          = lb.C_in\n                c_in = lb.C_out\n                channel_in =
          c_in*2 if i else c_in  # for channel concat\n                if i == 0:\n                    block
          = Upsample1x1(channel_in, c_out, norm=norm, activ=activ, w_norm=w_norm)\n                else:\n                    block
          = ConvBlk(channel_in, c_out, upsample=True)\n                self.rights.append(block)\n\n        def
          forward(self, x):\n            features = []\n            for lb in self.lefts:\n                x
          = lb(x)\n                features.append(x)\n\n            assert x.shape[-2:]
          == torch.Size((1, 1))\n\n            for i, (rb, lf) in enumerate(zip(self.rights,
          features[::-1])):\n                if i:\n                    x = torch.cat([x,
          lf], dim=1)\n                x = rb(x)\n\n            return x\n\n    class
          StyleEncoder(nn.Module):\n        def __init__(self, layers, out_shape):\n            super().__init__()\n\n            self.layers
          = nn.Sequential(*layers)\n            self.out_shape = out_shape\n\n        def
          forward(self, x):\n            style_feat = self.layers(x)\n\n            return
          style_feat\n\n    def style_enc_builder(C_in, C, norm=''none'', activ=''relu'',
          pad_type=''reflect'', skip_scale_var=False):\n\n        ConvBlk = partial(ConvBlock,
          norm=norm, activ=activ, pad_type=pad_type)\n\n        layers = [\n            ConvBlk(C_in,
          C, 3, 1, 1, norm=''none'', activ=''none''),\n            ConvBlk(C*1, C*2,
          3, 1, 1, downsample=True),\n            GCBlock(C*2),\n            ConvBlk(C*2,
          C*4, 3, 1, 1, downsample=True),\n            CBAM(C*4)\n        ]\n\n        out_shape
          = (C*4, 32, 32)\n\n        return StyleEncoder(layers, out_shape)\n\n    class
          CharAttar:\n        def __init__(self,num_classes,device,style_path):\n            self.num_classes
          = num_classes\n            self.device = device\n            self.contents_dim
          = 100\n            self.contents_emb = nn.Embedding(num_classes, self.contents_dim)\n            self.style_enc
          = self.make_style_enc(os.path.join(style_path,\"style_enc.pth\"))\n            self.style_conv
          = nn.Sequential(\n                                nn.Conv2d(128,128,16),\n                                nn.SiLU(),\n                            ).to(device)\n\n        def
          make_stroke(self,contents):\n            strokes_list = []\n            for
          content in contents:\n                content_code = ord(content)\n                first_letter_code
          = 44032\n                stroke = [0] * 68\n                first_consonant_letter
          = int((content_code - first_letter_code) / 588)\n                middle_consonant_letter
          = int(((content_code - first_letter_code) - (first_consonant_letter * 588))
          / 28)\n                last_consonant_letter = int((content_code - first_letter_code)
          - (first_consonant_letter * 588) - (middle_consonant_letter * 28))\n                stroke[first_consonant_letter]
          = 1\n                stroke[middle_consonant_letter + 19] = 1\n                stroke[last_consonant_letter
          + 19 + 21] = 1\n                strokes_list.append(stroke)\n            return
          strokes_list\n\n        def make_style_enc(self,style_enc_path):\n            C
          ,C_in = 32, 1\n            sty_encoder = style_enc_builder(C_in, C)\n            checkpoint
          = torch.load(style_enc_path, map_location=self.device)\n            tmp_dict
          = {}\n            for k, v in checkpoint.items():\n                if k
          in sty_encoder.state_dict():\n                    tmp_dict[k] = v\n            sty_encoder.load_state_dict(tmp_dict)\n            #
          frozen sty_encoder\n            for p in sty_encoder.parameters():\n                p.requires_grad
          = False\n            return sty_encoder.to(self.device)\n\n        # def
          set_charAttr_dim(mode):\n        #     pass\n        def make_charAttr(self,images,contents_index,
          contents,mode):\n            input_length = images.shape[0]\n            #
          contents_index = [int(content_index) for content_index in contents_index]\n            #
          style_encoder = style_enc_builder(1,32).to(self.device)\n\n            contents_emb
          = None\n            stroke =  None\n            style = None\n            contents_p,
          stroke_p = random.random(), random.random()\n            if mode == 1:\n\n                if
          contents_p < 0.3:\n                    contents_emb = torch.zeros(input_length,self.contents_dim)\n                else:\n                    contents_emb
          = torch.FloatTensor(self.contents_emb(contents_index))\n\n                if
          stroke_p < 0.3:\n                    stroke = torch.zeros(input_length,68)\n                else:\n                    stroke
          =  torch.FloatTensor(self.make_stroke(contents))\n\n                style
          = torch.zeros(input_length,128)\n\n            elif mode == 2:\n\n                if
          contents_p < 0.3:\n                    contents_emb = torch.zeros(input_length,self.contents_dim)\n                else:\n                    contents_emb
          = torch.FloatTensor(self.contents_emb(contents_index))\n\n                if
          stroke_p < 0.3:\n                    stroke = torch.zeros(input_length,68)\n                else:\n                    stroke
          = torch.FloatTensor(self.make_stroke(contents))\n\n                if contents_p
          < 0.3 and stroke_p < 0.3:\n                    style = torch.zeros(input_length,128)\n                else:\n                    style
          = self.style_enc(images)\n                    # style = F.adaptive_avg_pool2d(style,
          (1, 1))\n                    style = self.style_conv(style)\n                    style
          = style.view(input_length, -1).cpu()\n\n            elif mode == 3: #test\n                contents_emb
          = torch.FloatTensor(self.contents_emb(contents_index))\n                stroke
          = torch.FloatTensor(self.make_stroke(contents))\n                style =
          self.style_enc(images)\n                # style = F.adaptive_avg_pool2d(style,
          (1, 1))\n                style = self.style_conv(style)\n                style
          = style.view(input_length, -1).cpu()\n\n            elif mode == 4:\n                contents_emb
          = torch.FloatTensor(self.contents_emb(contents_index))\n                stroke
          = torch.FloatTensor(self.make_stroke(contents))\n                style =
          torch.zeros(input_length,128)\n\n            return torch.cat([contents_emb,stroke,style],dim=1)\n\n    if
          os.path.isdir(result_model_path):\n        pass\n    else:\n        os.mkdir(result_model_path)\n    os.environ[''CUDA_VISIBLE_DEVICES'']
          = str(gpu_num)\n    device = torch.device(''cuda'' if torch.cuda.is_available()
          else ''cpu'')\n    transforms = torchvision.transforms.Compose([\n            #
          torchvision.transforms.Resize((input_size,input_size)),\n            torchvision.transforms.Grayscale(num_output_channels=1),\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize((0.5),
          (0.5))\n        ])\n    dataset = DiffusionDataset(\"diffusion_font_train.csv\",transform=transforms)\n    dataloader
          = DataLoader(dataset, batch_size=batch_size, shuffle=True,num_workers=12)\n    model
          = UNet().to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    loss_func
          = nn.MSELoss()\n    diffusion = Diffusion(first_beta=1e-4,\n                            end_beta=0.02,\n                            noise_step=1000,\n                            beta_schedule_type=''linear'',\n                            img_size=input_size,\n                            device=device)\n\n    model_value
          = 0\n    for epoch_id in range(n_epochs):\n        for i, (images, contents_label,filename)
          in enumerate(dataloader):\n            images = images.to(device)\n\n            contents
          = [dataset.label_to_y[int(content_index)] for content_index in contents_label]\n            charAttar
          = CharAttar(num_classes=num_classes,device=device,style_path=style_path)\n            print(epoch_id)\n\n            charAttr_list
          = charAttar.make_charAttr(images, contents_label,contents,mode=mode).to(device)\n\n            t
          = diffusion.sample_t(images.shape[0]).to(device)\n            x_t, noise
          = diffusion.noise_images(images, t)\n\n            predicted_noise = model(x_t,
          t, charAttr_list)\n            loss = loss_func(noise, predicted_noise)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if
          epoch_id % 10 == 0:\n            labels = torch.arange(num_classes).long().to(device)\n            #
          sampled_images = diffusion.portion_sampling(model, n=len(dataset.labels),sampleImage_len
          = sampleImage_len,dataset=dataset,mode =mode,charAttar=charAttar,sample_img=sample_img)\n            #
          plot_images(sampled_images)\n            torch.save(model,os.path.join(result_model_path,f\"model_{epoch_id}.pt\"))\n            torch.save(model.state_dict(),
          os.path.join(result_model_path, f\"ckpt_{epoch_id}.pt\"))\n            torch.save(optimizer.state_dict(),
          os.path.join(result_model_path, f\"optim_{epoch_id}.pt\"))\n    return model_value\n\ndef
          _serialize_float(float_value: float) -> str:\n    if isinstance(float_value,
          str):\n        return float_value\n    if not isinstance(float_value, (float,
          int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          float.''.format(\n            str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Model
          training'', description='''')\n_parser.add_argument(\"--gpu-num\", dest=\"gpu_num\",
          type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch-size\",
          dest=\"batch_size\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-size\",
          dest=\"input_size\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--n-epochs\",
          dest=\"n_epochs\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--num-classes\",
          dest=\"num_classes\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mode\",
          dest=\"mode\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lr\",
          dest=\"lr\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--result-model-path\",
          dest=\"result_model_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--style-path\",
          dest=\"style_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = model_training(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "python:3.10"}}, "inputs": [{"name": "gpu_num", "type": "Integer"},
          {"name": "batch_size", "type": "Integer"}, {"name": "input_size", "type":
          "Integer"}, {"name": "n_epochs", "type": "Integer"}, {"name": "num_classes",
          "type": "Integer"}, {"name": "mode", "type": "Integer"}, {"name": "lr",
          "type": "Float"}, {"name": "result_model_path", "type": "String"}, {"name":
          "style_path", "type": "String"}], "name": "Model training", "outputs": [{"name":
          "Output", "type": "Float"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"batch_size": "{{inputs.parameters.batch_size}}",
          "gpu_num": "{{inputs.parameters.gpu_num}}", "input_size": "{{inputs.parameters.image_size}}",
          "lr": "{{inputs.parameters.lr}}", "mode": "{{inputs.parameters.mode}}",
          "n_epochs": "{{inputs.parameters.n_epochs}}", "num_classes": "{{inputs.parameters.num_classes}}",
          "result_model_path": "{{inputs.parameters.result_model_path}}", "style_path":
          "{{inputs.parameters.style_path}}"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.16
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
    volumes:
    - name: test-lee
      persistentVolumeClaim: {claimName: diffusion}
  - name: serve-a-model-with-kserve
    container:
      args:
      - -u
      - kservedeployer.py
      - --action
      - apply
      - --model-name
      - diffusion_serve
      - --model-uri
      - pvc://diffusion/model-store
      - --canary-traffic-percent
      - '100'
      - --namespace
      - kubeflow-user-example-com
      - --framework
      - pytorch
      - --runtime-version
      - latest
      - --resource-requests
      - '{"cpu": "0.5", "memory": "512Mi"}'
      - --resource-limits
      - '{"cpu": "1", "memory": "1Gi"}'
      - --custom-model-spec
      - '{}'
      - --autoscaling-target
      - '0'
      - --service-account
      - ''
      - --enable-istio-sidecar
      - "True"
      - --output-path
      - /tmp/outputs/InferenceService_Status/data
      - --inferenceservice-yaml
      - '{}'
      - --watch-timeout
      - '300'
      - --min-replicas
      - '-1'
      - --max-replicas
      - '-1'
      - --request-timeout
      - '60'
      - --enable-isvc-status
      - "True"
      command: [python]
      image: quay.io/aipipeline/kserve-component:v0.11.1
      volumeMounts:
      - {mountPath: pvc, name: test-lee}
    outputs:
      artifacts:
      - {name: serve-a-model-with-kserve-InferenceService-Status, path: /tmp/outputs/InferenceService_Status/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.16
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Serve
          Models using KServe", "implementation": {"container": {"args": ["-u", "kservedeployer.py",
          "--action", {"inputValue": "Action"}, "--model-name", {"inputValue": "Model
          Name"}, "--model-uri", {"inputValue": "Model URI"}, "--canary-traffic-percent",
          {"inputValue": "Canary Traffic Percent"}, "--namespace", {"inputValue":
          "Namespace"}, "--framework", {"inputValue": "Framework"}, "--runtime-version",
          {"inputValue": "Runtime Version"}, "--resource-requests", {"inputValue":
          "Resource Requests"}, "--resource-limits", {"inputValue": "Resource Limits"},
          "--custom-model-spec", {"inputValue": "Custom Model Spec"}, "--autoscaling-target",
          {"inputValue": "Autoscaling Target"}, "--service-account", {"inputValue":
          "Service Account"}, "--enable-istio-sidecar", {"inputValue": "Enable Istio
          Sidecar"}, "--output-path", {"outputPath": "InferenceService Status"}, "--inferenceservice-yaml",
          {"inputValue": "InferenceService YAML"}, "--watch-timeout", {"inputValue":
          "Watch Timeout"}, "--min-replicas", {"inputValue": "Min Replicas"}, "--max-replicas",
          {"inputValue": "Max Replicas"}, "--request-timeout", {"inputValue": "Request
          Timeout"}, "--enable-isvc-status", {"inputValue": "Enable ISVC Status"}],
          "command": ["python"], "image": "quay.io/aipipeline/kserve-component:v0.11.1"}},
          "inputs": [{"default": "create", "description": "Action to execute on KServe",
          "name": "Action", "type": "String"}, {"default": "", "description": "Name
          to give to the deployed model", "name": "Model Name", "type": "String"},
          {"default": "", "description": "Path of the S3 or GCS compatible directory
          containing the model.", "name": "Model URI", "type": "String"}, {"default":
          "100", "description": "The traffic split percentage between the candidate
          model and the last ready model", "name": "Canary Traffic Percent", "type":
          "String"}, {"default": "", "description": "Kubernetes namespace where the
          KServe service is deployed.", "name": "Namespace", "type": "String"}, {"default":
          "", "description": "Machine Learning Framework for Model Serving.", "name":
          "Framework", "type": "String"}, {"default": "latest", "description": "Runtime
          Version of Machine Learning Framework", "name": "Runtime Version", "type":
          "String"}, {"default": "{\"cpu\": \"0.5\", \"memory\": \"512Mi\"}", "description":
          "CPU and Memory requests for Model Serving", "name": "Resource Requests",
          "type": "String"}, {"default": "{\"cpu\": \"1\", \"memory\": \"1Gi\"}",
          "description": "CPU and Memory limits for Model Serving", "name": "Resource
          Limits", "type": "String"}, {"default": "{}", "description": "Custom model
          runtime container spec in JSON", "name": "Custom Model Spec", "type": "String"},
          {"default": "0", "description": "Autoscaling Target Number", "name": "Autoscaling
          Target", "type": "String"}, {"default": "", "description": "ServiceAccount
          to use to run the InferenceService pod", "name": "Service Account", "type":
          "String"}, {"default": "True", "description": "Whether to enable istio sidecar
          injection", "name": "Enable Istio Sidecar", "type": "Bool"}, {"default":
          "{}", "description": "Raw InferenceService serialized YAML for deployment",
          "name": "InferenceService YAML", "type": "String"}, {"default": "300", "description":
          "Timeout seconds for watching until InferenceService becomes ready.", "name":
          "Watch Timeout", "type": "String"}, {"default": "-1", "description": "Minimum
          number of InferenceService replicas", "name": "Min Replicas", "type": "String"},
          {"default": "-1", "description": "Maximum number of InferenceService replicas",
          "name": "Max Replicas", "type": "String"}, {"default": "60", "description":
          "Specifies the number of seconds to wait before timing out a request to
          the component.", "name": "Request Timeout", "type": "String"}, {"default":
          "True", "description": "Specifies whether to store the inference service
          status as the output parameter", "name": "Enable ISVC Status", "type": "Bool"}],
          "name": "Serve a model with KServe", "outputs": [{"description": "Status
          JSON output of InferenceService", "name": "InferenceService Status", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest": "b0379af21c170410b8b1a9606cb6cad63f99b0af53c2a5c1f0af397b53c81cd7",
          "url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kserve/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"Action": "apply", "Autoscaling
          Target": "0", "Canary Traffic Percent": "100", "Custom Model Spec": "{}",
          "Enable ISVC Status": "True", "Enable Istio Sidecar": "True", "Framework":
          "pytorch", "InferenceService YAML": "{}", "Max Replicas": "-1", "Min Replicas":
          "-1", "Model Name": "diffusion_serve", "Model URI": "pvc://diffusion/model-store",
          "Namespace": "kubeflow-user-example-com", "Request Timeout": "60", "Resource
          Limits": "{\"cpu\": \"1\", \"memory\": \"1Gi\"}", "Resource Requests": "{\"cpu\":
          \"0.5\", \"memory\": \"512Mi\"}", "Runtime Version": "latest", "Service
          Account": "", "Watch Timeout": "300"}'}
    volumes:
    - name: test-lee
      persistentVolumeClaim: {claimName: diffusion}
  arguments:
    parameters:
    - {name: image_size}
    - {name: result_path}
    - {name: csv_path}
    - {name: gpu_num}
    - {name: batch_size}
    - {name: n_epochs}
    - {name: num_classes}
    - {name: mode}
    - {name: lr}
    - {name: result_model_path}
    - {name: style_path}
  serviceAccountName: pipeline-runner
