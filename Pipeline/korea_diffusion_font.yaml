apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: korea-diffusion-train-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.16, pipelines.kubeflow.org/pipeline_compilation_time: '2023-10-25T10:50:55.012552',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "An example pipeline that
      performs arithmetic calculations.", "inputs": [{"name": "style_url", "type":
      "String"}, {"name": "fonts_url", "type": "String"}, {"name": "image_size", "type":
      "Integer"}, {"name": "train_string", "type": "String"}, {"name": "fonts_base_path",
      "type": "String"}, {"name": "result_path", "type": "String"}, {"name": "csv_path",
      "type": "String"}, {"name": "gpu_num", "type": "Integer"}, {"name": "batch_size",
      "type": "Integer"}, {"name": "n_epochs", "type": "Integer"}, {"name": "num_classes",
      "type": "Integer"}, {"name": "mode", "type": "Integer"}, {"name": "lr", "type":
      "Float"}, {"name": "result_model_path", "type": "String"}, {"name": "style_path",
      "type": "String"}], "name": "korea diffusion train pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.16}
spec:
  entrypoint: korea-diffusion-train-pipeline
  templates:
  - name: korea-diffusion-train-pipeline
    inputs:
      parameters:
      - {name: batch_size}
      - {name: csv_path}
      - {name: fonts_base_path}
      - {name: fonts_url}
      - {name: gpu_num}
      - {name: image_size}
      - {name: lr}
      - {name: mode}
      - {name: n_epochs}
      - {name: num_classes}
      - {name: result_model_path}
      - {name: result_path}
      - {name: style_path}
      - {name: style_url}
      - {name: train_string}
    dag:
      tasks:
      - name: load-fonts
        template: load-fonts
        arguments:
          parameters:
          - {name: fonts_base_path, value: '{{inputs.parameters.fonts_base_path}}'}
          - {name: fonts_url, value: '{{inputs.parameters.fonts_url}}'}
          - {name: style_path, value: '{{inputs.parameters.style_path}}'}
          - {name: style_url, value: '{{inputs.parameters.style_url}}'}
      - name: make-image-csv
        template: make-image-csv
        dependencies: [load-fonts]
        arguments:
          parameters:
          - {name: csv_path, value: '{{inputs.parameters.csv_path}}'}
          - {name: fonts_base_path, value: '{{inputs.parameters.fonts_base_path}}'}
          - {name: image_size, value: '{{inputs.parameters.image_size}}'}
          - {name: result_path, value: '{{inputs.parameters.result_path}}'}
          - {name: train_string, value: '{{inputs.parameters.train_string}}'}
      - name: model-training
        template: model-training
        dependencies: [make-image-csv]
        arguments:
          parameters:
          - {name: batch_size, value: '{{inputs.parameters.batch_size}}'}
          - {name: gpu_num, value: '{{inputs.parameters.gpu_num}}'}
          - {name: image_size, value: '{{inputs.parameters.image_size}}'}
          - {name: lr, value: '{{inputs.parameters.lr}}'}
          - {name: mode, value: '{{inputs.parameters.mode}}'}
          - {name: n_epochs, value: '{{inputs.parameters.n_epochs}}'}
          - {name: num_classes, value: '{{inputs.parameters.num_classes}}'}
          - {name: result_model_path, value: '{{inputs.parameters.result_model_path}}'}
          - {name: style_path, value: '{{inputs.parameters.style_path}}'}
  - name: load-fonts
    container:
      args: [--style-url, '{{inputs.parameters.style_url}}', --sytle-path, '{{inputs.parameters.style_path}}',
        --fonts-url, '{{inputs.parameters.fonts_url}}', --font-path, '{{inputs.parameters.fonts_base_path}}']
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'urllib3==1.26.14' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'urllib3==1.26.14' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def load_fonts(style_url,sytle_path,fonts_url, font_path):
            import tarfile
            import os
            import urllib.request
            if os.path.isdir(font_path):
                pass
            else:
                os.mkdir(font_path)
                with urllib.request.urlopen(fonts_url) as res:
                    tarfile.open(fileobj=res, mode="r|gz").extractall(font_path)
            if os.path.isdir(sytle_path):
                pass
            else:
                os.mkdir(sytle_path)
            with urllib.request.urlopen(style_url) as res:
                    tarfile.open(fileobj=res, mode="r|gz").extractall(sytle_path)

        import argparse
        _parser = argparse.ArgumentParser(prog='Load fonts', description='')
        _parser.add_argument("--style-url", dest="style_url", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--sytle-path", dest="sytle_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--fonts-url", dest="fonts_url", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--font-path", dest="font_path", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = load_fonts(**_parsed_args)
      image: python:3.10
      volumeMounts:
      - {mountPath: pvc, name: test-lee}
    inputs:
      parameters:
      - {name: fonts_base_path}
      - {name: fonts_url}
      - {name: style_path}
      - {name: style_url}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.16
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--style-url", {"inputValue": "style_url"}, "--sytle-path", {"inputValue":
          "sytle_path"}, "--fonts-url", {"inputValue": "fonts_url"}, "--font-path",
          {"inputValue": "font_path"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''urllib3==1.26.14''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''urllib3==1.26.14'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def load_fonts(style_url,sytle_path,fonts_url, font_path):\n    import
          tarfile\n    import os\n    import urllib.request\n    if os.path.isdir(font_path):\n        pass\n    else:\n        os.mkdir(font_path)\n        with
          urllib.request.urlopen(fonts_url) as res:\n            tarfile.open(fileobj=res,
          mode=\"r|gz\").extractall(font_path)\n    if os.path.isdir(sytle_path):\n        pass\n    else:\n        os.mkdir(sytle_path)\n    with
          urllib.request.urlopen(style_url) as res:\n            tarfile.open(fileobj=res,
          mode=\"r|gz\").extractall(sytle_path)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Load
          fonts'', description='''')\n_parser.add_argument(\"--style-url\", dest=\"style_url\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--sytle-path\",
          dest=\"sytle_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--fonts-url\",
          dest=\"fonts_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--font-path\",
          dest=\"font_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = load_fonts(**_parsed_args)\n"],
          "image": "python:3.10"}}, "inputs": [{"name": "style_url", "type": "String"},
          {"name": "sytle_path", "type": "String"}, {"name": "fonts_url", "type":
          "String"}, {"name": "font_path", "type": "String"}], "name": "Load fonts"}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"font_path":
          "{{inputs.parameters.fonts_base_path}}", "fonts_url": "{{inputs.parameters.fonts_url}}",
          "style_url": "{{inputs.parameters.style_url}}", "sytle_path": "{{inputs.parameters.style_path}}"}'}
    volumes:
    - name: test-lee
      persistentVolumeClaim: {claimName: diffusion}
  - name: make-image-csv
    container:
      args: [--image-size, '{{inputs.parameters.image_size}}', --train-string, '{{inputs.parameters.train_string}}',
        --fonts-base-path, '{{inputs.parameters.fonts_base_path}}', --result-path,
        '{{inputs.parameters.result_path}}', --csv-path, '{{inputs.parameters.csv_path}}']
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==2.0.0' 'pillow==9.4.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas==2.0.0' 'pillow==9.4.0'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def make_image_csv(image_size, train_string, fonts_base_path, result_path,\
        \ csv_path):\n    import os\n    import pandas as pd\n    from PIL import\
        \ Image, ImageDraw, ImageFont, ImageOps\n\n    if os.path.isdir(result_path):\n\
        \        pass\n    else:\n        os.mkdir(result_path)\n\n    if os.path.isdir(csv_path):\n\
        \        pass\n    else:\n        os.mkdir(csv_path)\n\n    fonts_folder_list\
        \ = os.listdir(fonts_base_path)\n    fonts_folder_list = [x for x in fonts_folder_list\
        \ if x[0] != \".\"]\n    hangul_codePoint = [format(ord(ch),'X') for ch in\
        \ train_string]\n\n    font_size = int(image_size *0.8)\n\n    train_files\
        \ = []\n\n    for uni in hangul_codePoint:\n        unicodeChars = chr(int(uni,\
        \ 16))\n        for fonts_folder in fonts_folder_list:\n                fonts\
        \ = os.listdir(os.path.join(fonts_base_path,fonts_folder))\n             \
        \   fonts = [x for x in fonts if x[0] != \".\"]\n                for ttf in\
        \ fonts:\n                    # Get Font image\n                    font =\
        \ ImageFont.truetype(font=os.path.join(fonts_base_path, fonts_folder, ttf),\
        \ size=font_size)\n\n                    # Get font image size and bbox\n\
        \                    x,y = font.getsize(unicodeChars)\n                  \
        \  left, top, right, bottom = font.getbbox(unicodeChars)\n\n             \
        \       # Check font image is empty / If font image is empty, do not create\
        \ image\n                    if x == 0 or y == 0 or (right-left) == 0 or (bottom-top)\
        \ == 0:\n                        continue\n\n                    # make base\
        \ image\n                    font_image = Image.new('RGB', (image_size, image_size),\
        \ color='white')\n\n                    # Draw font image on base image\n\
        \                    draw_image = ImageDraw.Draw(font_image)\n           \
        \         draw_image.text(((image_size-x)/2, (image_size-y)/2), unicodeChars,\
        \ font=font, fill='black')\n\n                    # Set file name\n      \
        \              # file_name = os.path.join(result_path, ttf[:-4]+\"_\"+unicodeChars)\n\
        \                    ttf_name = ttf[:-4].replace(\" \",\"\")\n           \
        \         # print(ttf[:-4],ttf_name)\n                    file_name = result_path\
        \ + \"/\" + ttf_name + \"_\" + unicodeChars + \".png\"\n\n               \
        \     # Convert the image to grayscale\n                    font_image = ImageOps.grayscale(font_image)\n\
        \n                    # Save image\n                    font_image.save('{}.png'.format(file_name))\
        \ \n                    train_files.append([ttf_name + \"_\" + unicodeChars,file_name,unicodeChars])\n\
        \    train_csv = pd.DataFrame(train_files)\n    train_csv.to_csv(os.path.join(csv_path,\"\
        diffusion_font_train.csv\"),index=False)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Make\
        \ image csv', description='')\n_parser.add_argument(\"--image-size\", dest=\"\
        image_size\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --train-string\", dest=\"train_string\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--fonts-base-path\", dest=\"fonts_base_path\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--result-path\"\
        , dest=\"result_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--csv-path\", dest=\"csv_path\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = make_image_csv(**_parsed_args)\n"
      image: python:3.10
      volumeMounts:
      - {mountPath: pvc, name: test-lee}
    inputs:
      parameters:
      - {name: csv_path}
      - {name: fonts_base_path}
      - {name: image_size}
      - {name: result_path}
      - {name: train_string}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.16
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--image-size", {"inputValue": "image_size"}, "--train-string",
          {"inputValue": "train_string"}, "--fonts-base-path", {"inputValue": "fonts_base_path"},
          "--result-path", {"inputValue": "result_path"}, "--csv-path", {"inputValue":
          "csv_path"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==2.0.0''
          ''pillow==9.4.0'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
          --quiet --no-warn-script-location ''pandas==2.0.0'' ''pillow==9.4.0'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def make_image_csv(image_size,
          train_string, fonts_base_path, result_path, csv_path):\n    import os\n    import
          pandas as pd\n    from PIL import Image, ImageDraw, ImageFont, ImageOps\n\n    if
          os.path.isdir(result_path):\n        pass\n    else:\n        os.mkdir(result_path)\n\n    if
          os.path.isdir(csv_path):\n        pass\n    else:\n        os.mkdir(csv_path)\n\n    fonts_folder_list
          = os.listdir(fonts_base_path)\n    fonts_folder_list = [x for x in fonts_folder_list
          if x[0] != \".\"]\n    hangul_codePoint = [format(ord(ch),''X'') for ch
          in train_string]\n\n    font_size = int(image_size *0.8)\n\n    train_files
          = []\n\n    for uni in hangul_codePoint:\n        unicodeChars = chr(int(uni,
          16))\n        for fonts_folder in fonts_folder_list:\n                fonts
          = os.listdir(os.path.join(fonts_base_path,fonts_folder))\n                fonts
          = [x for x in fonts if x[0] != \".\"]\n                for ttf in fonts:\n                    #
          Get Font image\n                    font = ImageFont.truetype(font=os.path.join(fonts_base_path,
          fonts_folder, ttf), size=font_size)\n\n                    # Get font image
          size and bbox\n                    x,y = font.getsize(unicodeChars)\n                    left,
          top, right, bottom = font.getbbox(unicodeChars)\n\n                    #
          Check font image is empty / If font image is empty, do not create image\n                    if
          x == 0 or y == 0 or (right-left) == 0 or (bottom-top) == 0:\n                        continue\n\n                    #
          make base image\n                    font_image = Image.new(''RGB'', (image_size,
          image_size), color=''white'')\n\n                    # Draw font image on
          base image\n                    draw_image = ImageDraw.Draw(font_image)\n                    draw_image.text(((image_size-x)/2,
          (image_size-y)/2), unicodeChars, font=font, fill=''black'')\n\n                    #
          Set file name\n                    # file_name = os.path.join(result_path,
          ttf[:-4]+\"_\"+unicodeChars)\n                    ttf_name = ttf[:-4].replace(\"
          \",\"\")\n                    # print(ttf[:-4],ttf_name)\n                    file_name
          = result_path + \"/\" + ttf_name + \"_\" + unicodeChars + \".png\"\n\n                    #
          Convert the image to grayscale\n                    font_image = ImageOps.grayscale(font_image)\n\n                    #
          Save image\n                    font_image.save(''{}.png''.format(file_name))
          \n                    train_files.append([ttf_name + \"_\" + unicodeChars,file_name,unicodeChars])\n    train_csv
          = pd.DataFrame(train_files)\n    train_csv.to_csv(os.path.join(csv_path,\"diffusion_font_train.csv\"),index=False)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Make image csv'', description='''')\n_parser.add_argument(\"--image-size\",
          dest=\"image_size\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-string\",
          dest=\"train_string\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--fonts-base-path\",
          dest=\"fonts_base_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--result-path\",
          dest=\"result_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--csv-path\",
          dest=\"csv_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = make_image_csv(**_parsed_args)\n"],
          "image": "python:3.10"}}, "inputs": [{"name": "image_size", "type": "Integer"},
          {"name": "train_string", "type": "String"}, {"name": "fonts_base_path",
          "type": "String"}, {"name": "result_path", "type": "String"}, {"name": "csv_path",
          "type": "String"}], "name": "Make image csv"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"csv_path": "{{inputs.parameters.csv_path}}",
          "fonts_base_path": "{{inputs.parameters.fonts_base_path}}", "image_size":
          "{{inputs.parameters.image_size}}", "result_path": "{{inputs.parameters.result_path}}",
          "train_string": "{{inputs.parameters.train_string}}"}'}
    volumes:
    - name: test-lee
      persistentVolumeClaim: {claimName: diffusion}
  - name: model-training
    container:
      args: [--gpu-num, '{{inputs.parameters.gpu_num}}', --batch-size, '{{inputs.parameters.batch_size}}',
        --input-size, '{{inputs.parameters.image_size}}', --n-epochs, '{{inputs.parameters.n_epochs}}',
        --num-classes, '{{inputs.parameters.num_classes}}', --mode, '{{inputs.parameters.mode}}',
        --lr, '{{inputs.parameters.lr}}', --result-model-path, '{{inputs.parameters.result_model_path}}',
        --style-path, '{{inputs.parameters.style_path}}']
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'tqdm==4.65.0' 'pandas==2.0.0' ' pillow==9.4.0' 'torch==2.0.0+cu118' 'torchvision==0.15.1+cu118'
        || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'tqdm==4.65.0' 'pandas==2.0.0' ' pillow==9.4.0' 'torch==2.0.0+cu118' 'torchvision==0.15.1+cu118'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def model_training(gpu_num, batch_size, input_size, n_epochs, num_classes,\
        \ mode, lr, result_model_path, style_path):\n    import os\n    import tqdm\n\
        \    import math\n    import random\n    import pandas as pd\n    import numpy\
        \ as np\n    from PIL import Image\n\n    import torch, torchvision\n    from\
        \ torch import optim\n    import torch.nn as nn\n    import torch.nn.functional\
        \ as F\n    from torch.utils.data import DataLoader, TensorDataset,Dataset\n\
        \n    from functools import partial\n\n    from PIL import Image\n    class\
        \ DiffusionDataset(Dataset):\n        def __init__(self, csv_path, transform\
        \ =None):\n            self.transform = transform\n            self.csv_data\
        \ = pd.read_csv(os.path.join(csv_path,\"diffusion_font_train.csv\"))\n   \
        \         self.x_file_name = self.csv_data.iloc[:,0]\n            self.x_path\
        \ = self.csv_data.iloc[:,1]\n            self.y = self.csv_data.iloc[:,2]\n\
        \            self.labels = np.unique(self.y)\n            self.y_to_label\
        \ = self.make_y_to_label()\n            self.label_to_y = self.make_label_to_y()\n\
        \            self.y_labels = self.make_y_labels()\n\n        def make_y_to_label(self):\n\
        \            y_to_label_dict = {}\n            for label, value in enumerate(self.labels):\n\
        \                y_to_label_dict[value] = label\n            return y_to_label_dict\n\
        \n        def make_label_to_y(self):\n            label_to_y_dict = {}\n \
        \           for label, value in enumerate(self.labels):\n                label_to_y_dict[label]\
        \ = value\n            return label_to_y_dict\n\n        def make_y_labels(self):\n\
        \            y_labels = []\n            for y_ch in self.y:\n            \
        \    y_labels.append(self.y_to_label[y_ch])\n            return y_labels\n\
        \n        def __len__(self):\n            return len(self.x_path)\n\n    \
        \    def __getitem__(self, id_):\n            filename = self.x_file_name[id_]\n\
        \            x = Image.open(self.x_path[id_])\n            transform_x = self.transform(x)\n\
        \            label = self.y_labels[id_]\n\n            return transform_x,\
        \ label, filename\n\n    class Diffusion:    \n        def __init__(self,\
        \ first_beta, end_beta, beta_schedule_type, noise_step, img_size, device):\n\
        \            self.first_beta = first_beta\n            self.end_beta = end_beta\n\
        \            self.beta_schedule_type = beta_schedule_type\n\n            self.noise_step\
        \ = noise_step\n\n            self.beta_list = self.beta_schedule().to(device)\n\
        \n            self.alphas =  1. - self.beta_list\n            self.alpha_bars\
        \ = torch.cumprod(self.alphas, dim = 0)\n\n            self.img_size = img_size\n\
        \            self.device = device\n\n        def sample_t(self, batch_size):\n\
        \            return torch.randint(1,self.noise_step,(batch_size,))\n\n   \
        \     def beta_schedule(self):\n            if self.beta_schedule_type ==\
        \ \"linear\":\n                return torch.linspace(self.first_beta, self.end_beta,\
        \ self.noise_step)\n            elif self.beta_schedule_type == \"cosine\"\
        :\n                steps = self.noise_step + 1\n                s = 0.008\n\
        \                x = torch.linspace(0, self.noise_step, steps)\n         \
        \       alphas_cumprod = torch.cos(((x / self.noise_step) + s) / (1 + s) *\
        \ torch.pi * 0.5) ** 2\n                alphas_cumprod = alphas_cumprod /\
        \ alphas_cumprod[0]\n                betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n\
        \                return torch.clip(betas, 0.0001, 0.9999)\n            elif\
        \ self.beta_schedule_type == \"quadratic\":\n                return torch.linspace(self.first_beta\
        \ ** 0.5, self.end_beta ** 0.5, self.noise_step) ** 2\n            elif self.beta_schedule_type\
        \ == \"sigmoid\":\n                beta = torch.linspace(-6,-6,self.noise_step)\n\
        \                return torch.sigmoid(beta) * (self.end_beta - self.first_beta)\
        \ + self.first_beta\n\n        def alpha_t(self, t):\n            return self.alphas[t][:,\
        \ None, None, None]\n\n        def alpha_bar_t (self,t):\n            return\
        \ self.alpha_bars[t][:, None, None, None]\n\n        def one_minus_alpha_bar(self,t):\n\
        \            return (1. - self.alpha_bars[t])[:, None, None, None]\n\n   \
        \     def beta_t(self,t):\n            return self.beta_list[t][:, None, None,\
        \ None]\n\n        def noise_images(self,x,t):\n            epsilon = torch.randn_like(x)\n\
        \            return torch.sqrt(self.alpha_bar_t(t)) * x + torch.sqrt(self.one_minus_alpha_bar(t))\
        \ * epsilon , epsilon\n\n        def indexToChar(self,y):\n            return\
        \ chr(44032+y)\n        def portion_sampling(self, model, n,sampleImage_len,dataset,mode,charAttar,sample_img,\
        \ cfg_scale=3):\n            example_images = []\n            model.eval()\n\
        \            with torch.no_grad():\n                x_list = torch.randn((sampleImage_len,\
        \ 1, self.img_size, self.img_size)).to(self.device)\n\n                y_idx\
        \ = list(range(n))[::math.floor(n/sampleImage_len)][:sampleImage_len]\n  \
        \              contents_index = torch.IntTensor(y_idx)\n                contents\
        \ = [dataset.label_to_y[int(content_index)] for content_index in contents_index]\n\
        \                charAttr_list = charAttar.make_charAttr(sample_img, contents_index,\
        \ contents,mode=3).to(self.device)\n\n                pbar = tqdm(list(reversed(range(1,\
        \ self.noise_step))),desc=\"sampling\")\n                for i in pbar:\n\
        \                    dataset = TensorDataset(x_list,charAttr_list)\n     \
        \               batch_size= 18\n                    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=False)\n\
        \                    predicted_noise = torch.tensor([]).to(self.device)\n\
        \                    uncond_predicted_noise = torch.tensor([]).to(self.device)\n\
        \                    for batch_x, batch_conditions in dataloader:\n      \
        \                  batch_t = (torch.ones(len(batch_x)) * i).long().to(self.device)\n\
        \                        batch_noise = model(batch_x, batch_t, batch_conditions)\n\
        \                        predicted_noise = torch.cat([predicted_noise,batch_noise],dim=0)\n\
        \                        #uncodition\n                        uncond_batch_noise\
        \ = model(batch_x, batch_t, torch.zeros_like(batch_conditions))\n        \
        \                uncond_predicted_noise = torch.cat([uncond_predicted_noise,uncond_batch_noise],dim\
        \ = 0)\n\n                    if cfg_scale > 0:\n                        predicted_noise\
        \ = torch.lerp(uncond_predicted_noise, predicted_noise, cfg_scale)\n\n   \
        \                 t = (torch.ones(sampleImage_len) * i).long()\n         \
        \           a_t = self.alpha_t(t)\n                    aBar_t = self.alpha_bar_t(t)\n\
        \                    b_t = self.beta_t(t)\n\n                    if i > 1:\n\
        \                        noise = torch.randn_like(x_list)\n              \
        \      else:\n                        noise = torch.zeros_like(x_list)\n\n\
        \                    x_list = 1 / torch.sqrt(a_t) * (\n                  \
        \          x_list - ((1 - a_t) / (torch.sqrt(1 - aBar_t))) * predicted_noise)\
        \ + torch.sqrt(\n                        b_t) * noise\n            model.train()\n\
        \            x_list = (x_list.clamp(-1, 1) + 1) / 2\n            x_list =\
        \ (x_list * 255).type(torch.uint8)\n            return x_list\n\n        def\
        \ test_sampling(self, model,sampleImage_len,charAttr_list,cfg_scale=3):\n\
        \            example_images = []\n            model.eval()\n            with\
        \ torch.no_grad():\n                x_list = torch.randn((sampleImage_len,\
        \ 3, self.img_size, self.img_size)).to(self.device)\n                pbar\
        \ = tqdm(list(reversed(range(1, self.noise_step))),desc=\"sampling\")\n  \
        \              for i in pbar:\n                    dataset = TensorDataset(x_list,charAttr_list)\n\
        \                    batch_size= 4\n                    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=False)\n\
        \                    predicted_noise = torch.tensor([]).to(self.device)\n\
        \                    uncond_predicted_noise = torch.tensor([]).to(self.device)\n\
        \                    for batch_x, batch_conditions in dataloader:\n      \
        \                  batch_t = (torch.ones(len(batch_x)) * i).long().to(self.device)\n\
        \                        batch_noise = model(batch_x, batch_t, batch_conditions)\n\
        \                        predicted_noise = torch.cat([predicted_noise,batch_noise],dim=0)\n\
        \                        #uncodition\n                        uncond_batch_noise\
        \ = model(batch_x, batch_t, torch.zeros_like(batch_conditions))\n        \
        \                uncond_predicted_noise = torch.cat([uncond_predicted_noise,uncond_batch_noise],dim\
        \ = 0)\n\n                    if cfg_scale > 0:\n                        predicted_noise\
        \ = torch.lerp(uncond_predicted_noise, predicted_noise, cfg_scale)\n\n   \
        \                 t = (torch.ones(sampleImage_len) * i).long()\n         \
        \           a_t = self.alpha_t(t)\n                    aBar_t = self.alpha_bar_t(t)\n\
        \                    b_t = self.beta_t(t)\n\n                    if i > 1:\n\
        \                        noise = torch.randn_like(x_list)\n              \
        \      else:\n                        noise = torch.zeros_like(x_list)\n\n\
        \                    x_list = 1 / torch.sqrt(a_t) * (\n                  \
        \          x_list - ((1 - a_t) / (torch.sqrt(1 - aBar_t))) * predicted_noise)\
        \ + torch.sqrt(\n                        b_t) * noise\n            model.train()\n\
        \            x_list = (x_list.clamp(-1, 1) + 1) / 2\n            x_list =\
        \ (x_list * 255).type(torch.uint8)\n            return x_list\n\n    class\
        \ SelfAttention(nn.Module):\n        def __init__(self, channels):\n     \
        \       super(SelfAttention, self).__init__()\n            self.channels =\
        \ channels\n            self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n\
        \            self.ln = nn.LayerNorm([channels])\n            self.ff_self\
        \ = nn.Sequential(\n                nn.LayerNorm([channels]),\n          \
        \      nn.Linear(channels, channels),\n                nn.GELU(),\n      \
        \          nn.Linear(channels, channels),\n            )\n\n        def forward(self,\
        \ x):\n            size = x.shape[-1]\n            x = x.view(-1, self.channels,\
        \ size * size).swapaxes(1, 2)\n            x_ln = self.ln(x)\n           \
        \ attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n            attention_value\
        \ = attention_value + x\n            attention_value = self.ff_self(attention_value)\
        \ + attention_value\n            return attention_value.swapaxes(2, 1).view(-1,\
        \ self.channels, size, size)\n\n    class DoubleConv(nn.Module):\n       \
        \ def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n\
        \            super().__init__()\n            self.residual = residual\n  \
        \          if not mid_channels:\n                mid_channels = out_channels\n\
        \            self.double_conv = nn.Sequential(\n                nn.Conv2d(in_channels,\
        \ mid_channels, kernel_size=3, padding=1, bias=False),\n                nn.GroupNorm(1,\
        \ mid_channels),\n                nn.GELU(),\n                nn.Conv2d(mid_channels,\
        \ out_channels, kernel_size=3, padding=1, bias=False),\n                nn.GroupNorm(1,\
        \ out_channels),\n            )\n\n        def forward(self, x):\n       \
        \     if self.residual:\n                return F.gelu(x + self.double_conv(x))\n\
        \            else:\n                return self.double_conv(x)\n\n    class\
        \ Down(nn.Module):\n        def __init__(self, in_channels, out_channels,\
        \ time_dim=256, charAttr_dim=12456):\n            super().__init__()\n   \
        \         self.maxpool_conv = nn.Sequential(\n                nn.MaxPool2d(2),\n\
        \                DoubleConv(in_channels, in_channels, residual=True),\n  \
        \              DoubleConv(in_channels, out_channels),\n            )\n\n \
        \           self.time_layer = nn.Sequential(\n                nn.SiLU(),\n\
        \                nn.Linear(\n                    time_dim,\n             \
        \       out_channels\n                ),\n            )\n\n            self.condition_layer\
        \ = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(\n\
        \                    charAttr_dim,\n                    out_channels\n   \
        \             ),\n            )\n\n        def forward(self, x, t,charAttr):\n\
        \            x = self.maxpool_conv(x)\n            emb = self.time_layer(t)[:,\
        \ :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n            charAttr_emb\
        \ = self.condition_layer(charAttr)[:, :, None, None].repeat(1, 1, x.shape[-2],\
        \ x.shape[-1])\n            return x + emb + charAttr_emb\n\n    class Up(nn.Module):\n\
        \        def __init__(self, in_channels, out_channels, time_dim=256, charAttr_dim=12456):\n\
        \            super().__init__()\n\n            self.up = nn.Upsample(scale_factor=2,\
        \ mode=\"bilinear\", align_corners=True)\n            self.conv = nn.Sequential(\n\
        \                DoubleConv(in_channels, in_channels, residual=True),\n  \
        \              DoubleConv(in_channels, out_channels, in_channels // 2),\n\
        \            )\n\n            self.time_layer = nn.Sequential(\n         \
        \       nn.SiLU(),\n                nn.Linear(\n                    time_dim,\n\
        \                    out_channels\n                ),\n            )\n   \
        \         self.condition_layer = nn.Sequential(\n                nn.SiLU(),\n\
        \                nn.Linear(\n                    charAttr_dim,\n         \
        \           out_channels\n                ),\n            )\n\n        def\
        \ forward(self, x, skip_x, t, charAttr):\n            x = self.up(x)\n   \
        \         x = torch.cat([skip_x, x], dim=1)\n            x = self.conv(x)\n\
        \            time_emb = self.time_layer(t)[:, :, None, None].repeat(1, 1,\
        \ x.shape[-2], x.shape[-1])\n            charAttr_emb = self.condition_layer(charAttr)[:,\
        \ :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n            return\
        \ x + time_emb + charAttr_emb\n\n    class UNet(nn.Module):\n        def __init__(self,\
        \ c_in=1, c_out=1, time_dim=256, charAttr_dim = 296, device=\"cuda\"):\n \
        \           super().__init__()\n            self.device = device\n       \
        \     self.time_dim = time_dim\n            self.charAttr_dim = charAttr_dim\n\
        \n            self.inc = DoubleConv(c_in, 64)\n            self.down1 = Down(64,\
        \ 128, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n         \
        \   self.sa1 = SelfAttention(128)\n            self.down2 = Down(128, 256,time_dim=self.time_dim,\
        \ charAttr_dim=self.charAttr_dim)\n            self.sa2 = SelfAttention(256)\n\
        \            self.down3 = Down(256, 256, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n\
        \            self.sa3 = SelfAttention(256)\n\n            self.bot1 = DoubleConv(256,\
        \ 512)\n            self.bot2 = DoubleConv(512, 512)\n            self.bot3\
        \ = DoubleConv(512, 256)\n\n            self.up1 = Up(512, 128, time_dim=self.time_dim,\
        \ charAttr_dim=self.charAttr_dim)\n            self.sa4 = SelfAttention(128)\n\
        \            self.up2 = Up(256, 64, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n\
        \            self.sa5 = SelfAttention(64)\n            self.up3 = Up(128,\
        \ 64, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n          \
        \  self.sa6 = SelfAttention(64)\n            self.outc = nn.Conv2d(64, c_out,\
        \ kernel_size=1)\n\n        def pos_encoding(self, t, channels):\n       \
        \     inv_freq = 1.0 / (\n                10000\n                ** (torch.arange(0,\
        \ channels, 2, device=self.device).float() / channels)\n            )\n  \
        \          pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n\
        \            pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n\
        \            pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n       \
        \     return pos_enc\n\n        def forward(self, x, time, charAttr):\n  \
        \          time = time.unsqueeze(-1).type(torch.float)\n            time =\
        \ self.pos_encoding(time, self.time_dim)\n\n            # if y is not None:\n\
        \            #     time += self.contents_emb(y)\n\n            x1 = self.inc(x)\n\
        \            x2 = self.down1(x1, time, charAttr)\n            x2 = self.sa1(x2)\n\
        \            x3 = self.down2(x2, time, charAttr)\n            x3 = self.sa2(x3)\n\
        \            x4 = self.down3(x3, time, charAttr)\n            x4 = self.sa3(x4)\n\
        \n            x4 = self.bot1(x4)\n            x4 = self.bot2(x4)\n       \
        \     x4 = self.bot3(x4)\n\n            x = self.up1(x4, x3, time, charAttr)\n\
        \            x = self.sa4(x)\n            x = self.up2(x, x2, time, charAttr)\n\
        \            x = self.sa5(x)\n            x = self.up3(x, x1, time, charAttr)\n\
        \            x = self.sa6(x)\n            output = self.outc(x)\n        \
        \    return output\n\n    class GlobalContext(nn.Module):\n        \"\"\"\
        \ Global-context \"\"\"\n        def __init__(self, C, bottleneck_ratio=0.25,\
        \ w_norm='none'):\n            super().__init__()\n            C_bottleneck\
        \ = int(C * bottleneck_ratio)\n            w_norm = w_norm_dispatch(w_norm)\n\
        \            self.k_proj = w_norm(nn.Conv2d(C, 1, 1))\n            self.transform\
        \ = nn.Sequential(\n                w_norm(nn.Linear(C, C_bottleneck)),\n\
        \                nn.LayerNorm(C_bottleneck),\n                nn.ReLU(),\n\
        \                w_norm(nn.Linear(C_bottleneck, C))\n            )\n\n   \
        \     def forward(self, x):\n            # x: [B, C, H, W]\n            context_logits\
        \ = self.k_proj(x)  # [B, 1, H, W]\n            context_weights = F.softmax(context_logits.flatten(1),\
        \ dim=1)  # [B, HW]\n            context = torch.einsum('bci,bi->bc', x.flatten(2),\
        \ context_weights)\n            out = self.transform(context)\n\n        \
        \    return out[..., None, None]\n\n    class GCBlock(nn.Module):\n      \
        \  \"\"\" Global-context block \"\"\"\n        def __init__(self, C, bottleneck_ratio=0.25,\
        \ w_norm='none'):\n            super().__init__()\n            self.gc = GlobalContext(C,\
        \ bottleneck_ratio, w_norm)\n\n        def forward(self, x):\n           \
        \ gc = self.gc(x)\n            return x + gc\n\n    class TLU(nn.Module):\n\
        \        \"\"\" Thresholded Linear Unit \"\"\"\n        def __init__(self,\
        \ num_features):\n            super().__init__()\n            self.num_features\
        \ = num_features\n            self.tau = nn.Parameter(torch.zeros(1, num_features,\
        \ 1, 1))\n\n        def forward(self, x):\n            return torch.max(x,\
        \ self.tau)\n\n        def extra_repr(self):\n            return 'num_features={}'.format(self.num_features)\n\
        \n    # NOTE generalized version\n    class FilterResponseNorm(nn.Module):\n\
        \        \"\"\" Filter Response Normalization \"\"\"\n        def __init__(self,\
        \ num_features, ndim, eps=None, learnable_eps=False):\n            \"\"\"\n\
        \            Args:\n                num_features\n                ndim\n \
        \               eps: if None is given, use the paper value as default.\n \
        \                   from paper, fixed_eps=1e-6 and learnable_eps_init=1e-4.\n\
        \                learnable_eps: turn eps to learnable parameter, which is\
        \ recommended on\n                    fully-connected or 1x1 activation map.\n\
        \            \"\"\"\n            super().__init__()\n            if eps is\
        \ None:\n                if learnable_eps:\n                    eps = 1e-4\n\
        \                else:\n                    eps = 1e-6\n\n            self.num_features\
        \ = num_features\n            self.init_eps = eps\n            self.learnable_eps\
        \ = learnable_eps\n            self.ndim = ndim\n\n            self.mean_dims\
        \ = list(range(2, 2+ndim))\n\n            self.weight = nn.Parameter(torch.ones([1,\
        \ num_features] + [1]*ndim))\n            self.bias = nn.Parameter(torch.zeros([1,\
        \ num_features] + [1]*ndim))\n            if learnable_eps:\n            \
        \    self.eps = nn.Parameter(torch.as_tensor(eps))\n            else:\n  \
        \              self.register_buffer('eps', torch.as_tensor(eps))\n\n     \
        \   def forward(self, x):\n            # normalize\n            nu2 = x.pow(2).mean(self.mean_dims,\
        \ keepdim=True)\n            x = x * torch.rsqrt(nu2 + self.eps.abs())\n\n\
        \            # modulation\n            x = x * self.weight + self.bias\n\n\
        \            return x\n\n        def extra_repr(self):\n            return\
        \ 'num_features={}, init_eps={}, ndim={}'.format(\n                    self.num_features,\
        \ self.init_eps, self.ndim)\n\n    FilterResponseNorm1d = partial(FilterResponseNorm,\
        \ ndim=1, learnable_eps=True)\n    FilterResponseNorm2d = partial(FilterResponseNorm,\
        \ ndim=2)\n\n    def split_dim(x, dim, n_chunks):\n        shape = x.shape\n\
        \        assert shape[dim] % n_chunks == 0\n        return x.view(*shape[:dim],\
        \ n_chunks, shape[dim] // n_chunks, *shape[dim+1:])\n\n    def weights_init(init_type='default'):\n\
        \        \"\"\" Adopted from FUNIT \"\"\"\n        def init_fun(m):\n    \
        \        classname = m.__class__.__name__\n            if (classname.find('Conv')\
        \ == 0 or classname.find('Linear') == 0) and hasattr(m, 'weight'):\n     \
        \           if init_type == 'gaussian':\n                    nn.init.normal_(m.weight.data,\
        \ 0.0, 0.02)\n                elif init_type == 'xavier':\n              \
        \      nn.init.xavier_normal_(m.weight.data, gain=2**0.5)\n              \
        \  elif init_type == 'kaiming':\n                    nn.init.kaiming_normal_(m.weight.data,\
        \ a=0, mode='fan_in')\n                elif init_type == 'orthogonal':\n \
        \                   nn.init.orthogonal_(m.weight.data, gain=2**0.5)\n    \
        \            elif init_type == 'default':\n                    pass\n    \
        \            else:\n                    assert 0, \"Unsupported initialization:\
        \ {}\".format(init_type)\n\n                if hasattr(m, 'bias') and m.bias\
        \ is not None:\n                    nn.init.constant_(m.bias.data, 0.0)\n\n\
        \        return init_fun\n\n    def spectral_norm(module):\n        \"\"\"\
        \ init & apply spectral norm \"\"\"\n        nn.init.xavier_uniform_(module.weight,\
        \ 2 ** 0.5)\n        if hasattr(module, 'bias') and module.bias is not None:\n\
        \            module.bias.data.zero_()\n\n        return nn.utils.spectral_norm(module)\n\
        \n    class BasicConv(nn.Module):\n        def __init__(self, in_planes, out_planes,\
        \ kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True,\
        \ bias=False):\n            super(BasicConv, self).__init__()\n          \
        \  self.out_channels = out_planes\n            self.conv = nn.Conv2d(in_planes,\
        \ out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation,\
        \ groups=groups, bias=bias)\n            self.bn = nn.BatchNorm2d(out_planes,\
        \ eps=1e-5, momentum=0.01, affine=True) if bn else None\n            self.relu\
        \ = nn.ReLU() if relu else None\n\n        def forward(self, x):\n       \
        \     x = self.conv(x)\n            if self.bn is not None:\n            \
        \    x = self.bn(x)\n            if self.relu is not None:\n             \
        \   x = self.relu(x)\n            return x\n\n    class Flatten(nn.Module):\n\
        \n        def forward(self, x):\n            return x.view(x.size(0), -1)\n\
        \n    class ChannelGate(nn.Module):\n        def __init__(self, gate_channels,\
        \ reduction_ratio=16, pool_types=['avg', 'max']):\n            super(ChannelGate,\
        \ self).__init__()\n            self.gate_channels = gate_channels\n     \
        \       self.mlp = nn.Sequential(\n                Flatten(),\n          \
        \      nn.Linear(gate_channels, gate_channels // reduction_ratio),\n     \
        \           nn.ReLU(),\n                nn.Linear(gate_channels // reduction_ratio,\
        \ gate_channels)\n                )\n            self.pool_types = pool_types\n\
        \n        def forward(self, x):\n            channel_att_sum = None\n    \
        \        for pool_type in self.pool_types:\n                if pool_type ==\
        \ 'avg':\n                    avg_pool = F.avg_pool2d(x, (x.size(2), x.size(3)),\
        \ stride=(x.size(2), x.size(3)))\n                    channel_att_raw = self.mlp(avg_pool)\n\
        \                elif pool_type == 'max':\n                    max_pool =\
        \ F.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n\
        \                    channel_att_raw = self.mlp(max_pool)\n              \
        \  elif pool_type == 'lp':\n                    lp_pool = F.lp_pool2d(x, 2,\
        \ (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n               \
        \     channel_att_raw = self.mlp(lp_pool)\n                elif pool_type\
        \ == 'lse':\n                    # LSE pool only\n                    lse_pool\
        \ = logsumexp_2d(x)\n                    channel_att_raw = self.mlp(lse_pool)\n\
        \n                if channel_att_sum is None:\n                    channel_att_sum\
        \ = channel_att_raw\n                else:\n                    channel_att_sum\
        \ = channel_att_sum + channel_att_raw\n\n            scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n\
        \            return x * scale\n\n    def logsumexp_2d(tensor):\n        tensor_flatten\
        \ = tensor.view(tensor.size(0), tensor.size(1), -1)\n        s, _ = torch.max(tensor_flatten,\
        \ dim=2, keepdim=True)\n        outputs = s + (tensor_flatten - s).exp().sum(dim=2,\
        \ keepdim=True).log()\n        return outputs\n\n    class ChannelPool(nn.Module):\n\
        \        def forward(self, x):\n            return torch.cat((torch.max(x,\
        \ 1)[0].unsqueeze(1), torch.mean(x, 1).unsqueeze(1)), dim=1)\n\n    class\
        \ SpatialGate(nn.Module):\n        def __init__(self):\n            super(SpatialGate,\
        \ self).__init__()\n            kernel_size = 7\n            self.compress\
        \ = ChannelPool()\n            self.spatial = BasicConv(2, 1, kernel_size,\
        \ stride=1, padding=(kernel_size-1) // 2, relu=False)\n\n        def forward(self,\
        \ x):\n            x_compress = self.compress(x)\n            x_out = self.spatial(x_compress)\n\
        \            scale = torch.sigmoid(x_out)  # broadcasting\n            return\
        \ x * scale\n\n    class CBAM(nn.Module):\n        def __init__(self, gate_channels,\
        \ reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n    \
        \        super(CBAM, self).__init__()\n            self.ChannelGate = ChannelGate(gate_channels,\
        \ reduction_ratio, pool_types)\n            self.no_spatial = no_spatial\n\
        \            if not no_spatial:\n                self.SpatialGate = SpatialGate()\n\
        \n        def forward(self, x):\n            x_out = self.ChannelGate(x)\n\
        \            if not self.no_spatial:\n                x_out = self.SpatialGate(x_out)\n\
        \            return x_out\n\n    class Flatten(nn.Module):\n        def __init__(self,\
        \ start_dim=1, end_dim=-1):\n            super(Flatten, self).__init__()\n\
        \            self.start_dim = start_dim\n            self.end_dim = end_dim\n\
        \n        def forward(self, input):\n            return input.flatten(self.start_dim,\
        \ self.end_dim)\n\n    def dispatcher(dispatch_fn):\n        def decorated(key,\
        \ *args):\n            if callable(key):\n                return key\n\n \
        \           if key is None:\n                key = 'none'\n\n            return\
        \ dispatch_fn(key, *args)\n        return decorated\n\n    @dispatcher\n \
        \   def norm_dispatch(norm):\n        return {\n            'none': nn.Identity,\n\
        \            'in': partial(nn.InstanceNorm2d, affine=False),  # false as default\n\
        \            'bn': nn.BatchNorm2d,\n            'frn': FilterResponseNorm2d\n\
        \        }[norm.lower()]\n\n    @dispatcher\n    def w_norm_dispatch(w_norm):\n\
        \        # NOTE Unlike other dispatcher, w_norm is function, not class.\n\
        \        return {\n            'spectral': spectral_norm,\n            'none':\
        \ lambda x: x\n        }[w_norm.lower()]\n\n    @dispatcher\n    def activ_dispatch(activ,\
        \ norm=None):\n        if norm_dispatch(norm) == FilterResponseNorm2d:\n \
        \           # use TLU for FRN\n            activ = 'tlu'\n\n        return\
        \ {\n            \"none\": nn.Identity,\n            \"relu\": nn.ReLU,\n\
        \            \"lrelu\": partial(nn.LeakyReLU, negative_slope=0.2),\n     \
        \       \"tlu\": TLU\n        }[activ.lower()]\n\n    @dispatcher\n    def\
        \ pad_dispatch(pad_type):\n        return {\n            \"zero\": nn.ZeroPad2d,\n\
        \            \"replicate\": nn.ReplicationPad2d,\n            \"reflect\"\
        : nn.ReflectionPad2d\n        }[pad_type.lower()]\n\n    class ParamBlock(nn.Module):\n\
        \        def __init__(self, C_out, shape):\n            super().__init__()\n\
        \            w = torch.randn((C_out, *shape))\n            b = torch.randn((C_out,))\n\
        \            self.shape = shape\n            self.w = nn.Parameter(w)\n  \
        \          self.b = nn.Parameter(b)\n\n        def forward(self, x):\n   \
        \         b = self.b.reshape((1, *self.b.shape, 1, 1, 1)).repeat(x.size(0),\
        \ 1, *self.shape)\n            return self.w*x + b\n\n    class LinearBlock(nn.Module):\n\
        \        \"\"\" pre-active linear block \"\"\"\n        def __init__(self,\
        \ C_in, C_out, norm='none', activ='relu', bias=True, w_norm='none',\n    \
        \                dropout=0.):\n            super().__init__()\n          \
        \  activ = activ_dispatch(activ, norm)\n            if norm.lower() == 'bn':\n\
        \                norm = nn.BatchNorm1d\n            elif norm.lower() == 'frn':\n\
        \                norm = FilterResponseNorm1d\n            elif norm.lower()\
        \ == 'none':\n                norm = nn.Identity\n            else:\n    \
        \            raise ValueError(f\"LinearBlock supports BN only (but {norm}\
        \ is given)\")\n            w_norm = w_norm_dispatch(w_norm)\n           \
        \ self.norm = norm(C_in)\n            self.activ = activ()\n            if\
        \ dropout > 0.:\n                self.dropout = nn.Dropout(p=dropout)\n  \
        \          self.linear = w_norm(nn.Linear(C_in, C_out, bias))\n\n        def\
        \ forward(self, x):\n            x = self.norm(x)\n            x = self.activ(x)\n\
        \            if hasattr(self, 'dropout'):\n                x = self.dropout(x)\n\
        \            return self.linear(x)\n\n    class ConvBlock(nn.Module):\n  \
        \      \"\"\" pre-active conv block \"\"\"\n        def __init__(self, C_in,\
        \ C_out, kernel_size=3, stride=1, padding=1, norm='none',\n              \
        \      activ='relu', bias=True, upsample=False, downsample=False, w_norm='none',\n\
        \                    pad_type='zero', dropout=0., size=None):\n          \
        \  # 1x1 conv assertion\n            if kernel_size == 1:\n              \
        \  assert padding == 0\n            super().__init__()\n            self.C_in\
        \ = C_in\n            self.C_out = C_out\n\n            activ = activ_dispatch(activ,\
        \ norm)\n            norm = norm_dispatch(norm)\n            w_norm = w_norm_dispatch(w_norm)\n\
        \            pad = pad_dispatch(pad_type)\n            self.upsample = upsample\n\
        \            self.downsample = downsample\n\n            assert ((norm ==\
        \ FilterResponseNorm2d) == (activ == TLU)), \"Use FRN and TLU together\"\n\
        \n            if norm == FilterResponseNorm2d and size == 1:\n           \
        \     self.norm = norm(C_in, learnable_eps=True)\n            else:\n    \
        \            self.norm = norm(C_in)\n            if activ == TLU:\n      \
        \          self.activ = activ(C_in)\n            else:\n                self.activ\
        \ = activ()\n            if dropout > 0.:\n                self.dropout =\
        \ nn.Dropout2d(p=dropout)\n            self.pad = pad(padding)\n         \
        \   self.conv = w_norm(nn.Conv2d(C_in, C_out, kernel_size, stride, bias=bias))\n\
        \n        def forward(self, x):\n            x = self.norm(x)\n          \
        \  x = self.activ(x)\n            if self.upsample:\n                x = F.interpolate(x,\
        \ scale_factor=2)\n            if hasattr(self, 'dropout'):\n            \
        \    x = self.dropout(x)\n            x = self.conv(self.pad(x))\n       \
        \     if self.downsample:\n                x = F.avg_pool2d(x, 2)\n      \
        \      return x\n\n    class ResBlock(nn.Module):\n        \"\"\" Pre-activate\
        \ ResBlock with spectral normalization \"\"\"\n        def __init__(self,\
        \ C_in, C_out, kernel_size=3, padding=1, upsample=False, downsample=False,\n\
        \                    norm='none', w_norm='none', activ='relu', pad_type='zero',\
        \ dropout=0.,\n                    scale_var=False):\n            assert not\
        \ (upsample and downsample)\n            super().__init__()\n            w_norm\
        \ = w_norm_dispatch(w_norm)\n            self.C_in = C_in\n            self.C_out\
        \ = C_out\n            self.upsample = upsample\n            self.downsample\
        \ = downsample\n            self.scale_var = scale_var\n\n            self.conv1\
        \ = ConvBlock(C_in, C_out, kernel_size, 1, padding, norm, activ,\n       \
        \                         upsample=upsample, w_norm=w_norm, pad_type=pad_type,\n\
        \                                dropout=dropout)\n            self.conv2\
        \ = ConvBlock(C_out, C_out, kernel_size, 1, padding, norm, activ,\n      \
        \                          w_norm=w_norm, pad_type=pad_type, dropout=dropout)\n\
        \n            # XXX upsample / downsample needs skip conv?\n            if\
        \ C_in != C_out or upsample or downsample:\n                self.skip = w_norm(nn.Conv2d(C_in,\
        \ C_out, 1))\n\n        def forward(self, x):\n            \"\"\"\n      \
        \      normal: pre-activ + convs + skip-con\n            upsample: pre-activ\
        \ + upsample + convs + skip-con\n            downsample: pre-activ + convs\
        \ + downsample + skip-con\n            => pre-activ + (upsample) + convs +\
        \ (downsample) + skip-con\n            \"\"\"\n            out = x\n\n   \
        \         out = self.conv1(out)\n            out = self.conv2(out)\n\n   \
        \         if self.downsample:\n                out = F.avg_pool2d(out, 2)\n\
        \n            # skip-con\n            if hasattr(self, 'skip'):\n        \
        \        if self.upsample:\n                    x = F.interpolate(x, scale_factor=2)\n\
        \                x = self.skip(x)\n                if self.downsample:\n \
        \                   x = F.avg_pool2d(x, 2)\n\n            out = out + x\n\
        \            if self.scale_var:\n                out = out / np.sqrt(2)\n\
        \            return out\n\n    class Upsample1x1(nn.Module):\n        \"\"\
        \"Upsample 1x1 to 2x2 using Linear\"\"\"\n        def __init__(self, C_in,\
        \ C_out, norm='none', activ='relu', w_norm='none'):\n            assert norm.lower()\
        \ != 'in', 'Do not use instance norm for 1x1 spatial size'\n            super().__init__()\n\
        \            self.C_in = C_in\n            self.C_out = C_out\n          \
        \  self.proj = ConvBlock(\n                C_in, C_out*4, 1, 1, 0, norm=norm,\
        \ activ=activ, w_norm=w_norm\n            )\n\n        def forward(self, x):\n\
        \            # x: [B, C_in, 1, 1]\n            x = self.proj(x)  # [B, C_out*4,\
        \ 1, 1]\n            B, C = x.shape[:2]\n            return x.view(B, C//4,\
        \ 2, 2)\n\n    class HourGlass(nn.Module):\n        \"\"\"U-net like hourglass\
        \ module\"\"\"\n        def __init__(self, C_in, C_max, size, n_downs, n_mids=1,\
        \ norm='none', activ='relu',\n                    w_norm='none', pad_type='zero'):\n\
        \            \"\"\"\n            Args:\n                C_max: maximum C_out\
        \ of left downsampling block's output\n            \"\"\"\n            super().__init__()\n\
        \            assert size == n_downs ** 2, \"HGBlock assume that the spatial\
        \ size is downsampled to 1x1.\"\n            self.C_in = C_in\n\n        \
        \    ConvBlk = partial(ConvBlock, norm=norm, activ=activ, w_norm=w_norm, pad_type=pad_type)\n\
        \n            self.lefts = nn.ModuleList()\n            c_in = C_in\n    \
        \        for i in range(n_downs):\n                c_out = min(c_in*2, C_max)\n\
        \                self.lefts.append(ConvBlk(c_in, c_out, downsample=True))\n\
        \                c_in = c_out\n\n            # 1x1 conv for mids\n       \
        \     self.mids = nn.Sequential(\n                *[\n                   \
        \ ConvBlk(c_in, c_out, kernel_size=1, padding=0)\n                    for\
        \ _ in range(n_mids)\n                ]\n            )\n\n            self.rights\
        \ = nn.ModuleList()\n            for i, lb in enumerate(self.lefts[::-1]):\n\
        \                c_out = lb.C_in\n                c_in = lb.C_out\n      \
        \          channel_in = c_in*2 if i else c_in  # for channel concat\n    \
        \            if i == 0:\n                    block = Upsample1x1(channel_in,\
        \ c_out, norm=norm, activ=activ, w_norm=w_norm)\n                else:\n \
        \                   block = ConvBlk(channel_in, c_out, upsample=True)\n  \
        \              self.rights.append(block)\n\n        def forward(self, x):\n\
        \            features = []\n            for lb in self.lefts:\n          \
        \      x = lb(x)\n                features.append(x)\n\n            assert\
        \ x.shape[-2:] == torch.Size((1, 1))\n\n            for i, (rb, lf) in enumerate(zip(self.rights,\
        \ features[::-1])):\n                if i:\n                    x = torch.cat([x,\
        \ lf], dim=1)\n                x = rb(x)\n\n            return x\n\n    class\
        \ StyleEncoder(nn.Module):\n        def __init__(self, layers, out_shape):\n\
        \            super().__init__()\n\n            self.layers = nn.Sequential(*layers)\n\
        \            self.out_shape = out_shape\n\n        def forward(self, x):\n\
        \            style_feat = self.layers(x)\n\n            return style_feat\n\
        \n    def style_enc_builder(C_in, C, norm='none', activ='relu', pad_type='reflect',\
        \ skip_scale_var=False):\n\n        ConvBlk = partial(ConvBlock, norm=norm,\
        \ activ=activ, pad_type=pad_type)\n\n        layers = [\n            ConvBlk(C_in,\
        \ C, 3, 1, 1, norm='none', activ='none'),\n            ConvBlk(C*1, C*2, 3,\
        \ 1, 1, downsample=True),\n            GCBlock(C*2),\n            ConvBlk(C*2,\
        \ C*4, 3, 1, 1, downsample=True),\n            CBAM(C*4)\n        ]\n\n  \
        \      out_shape = (C*4, 32, 32)\n\n        return StyleEncoder(layers, out_shape)\n\
        \n    class CharAttar:\n        def __init__(self,num_classes,device,style_path):\n\
        \            self.num_classes = num_classes\n            self.device = device\n\
        \            self.contents_dim = 100\n            self.contents_emb = nn.Embedding(num_classes,\
        \ self.contents_dim)\n            self.style_enc = self.make_style_enc(os.path.join(style_path,\"\
        style_enc.pth\"))\n            self.style_conv = nn.Sequential(\n        \
        \                        nn.Conv2d(128,128,16),\n                        \
        \        nn.SiLU(),\n                            ).to(device)\n\n        def\
        \ make_stroke(self,contents):\n            strokes_list = []\n           \
        \ for content in contents:\n                content_code = ord(content)\n\
        \                first_letter_code = 44032\n                stroke = [0] *\
        \ 68\n                first_consonant_letter = int((content_code - first_letter_code)\
        \ / 588)\n                middle_consonant_letter = int(((content_code - first_letter_code)\
        \ - (first_consonant_letter * 588)) / 28)\n                last_consonant_letter\
        \ = int((content_code - first_letter_code) - (first_consonant_letter * 588)\
        \ - (middle_consonant_letter * 28))\n                stroke[first_consonant_letter]\
        \ = 1\n                stroke[middle_consonant_letter + 19] = 1\n        \
        \        stroke[last_consonant_letter + 19 + 21] = 1\n                strokes_list.append(stroke)\n\
        \            return strokes_list\n\n        def make_style_enc(self,style_enc_path):\n\
        \            C ,C_in = 32, 1\n            sty_encoder = style_enc_builder(C_in,\
        \ C)\n            checkpoint = torch.load(style_enc_path, map_location=self.device)\n\
        \            tmp_dict = {}\n            for k, v in checkpoint.items():\n\
        \                if k in sty_encoder.state_dict():\n                    tmp_dict[k]\
        \ = v\n            sty_encoder.load_state_dict(tmp_dict)\n            # frozen\
        \ sty_encoder\n            for p in sty_encoder.parameters():\n          \
        \      p.requires_grad = False\n            return sty_encoder.to(self.device)\n\
        \n        # def set_charAttr_dim(mode):\n        #     pass\n        def make_charAttr(self,images,contents_index,\
        \ contents,mode):\n            input_length = images.shape[0]\n          \
        \  # contents_index = [int(content_index) for content_index in contents_index]\n\
        \            # style_encoder = style_enc_builder(1,32).to(self.device)\n\n\
        \            contents_emb = None\n            stroke =  None\n           \
        \ style = None\n            contents_p, stroke_p = random.random(), random.random()\n\
        \            if mode == 1:\n\n                if contents_p < 0.3:\n     \
        \               contents_emb = torch.zeros(input_length,self.contents_dim)\n\
        \                else:\n                    contents_emb = torch.FloatTensor(self.contents_emb(contents_index))\n\
        \n                if stroke_p < 0.3:\n                    stroke = torch.zeros(input_length,68)\n\
        \                else:\n                    stroke =  torch.FloatTensor(self.make_stroke(contents))\n\
        \n                style = torch.zeros(input_length,128)\n\n            elif\
        \ mode == 2:\n\n                if contents_p < 0.3:\n                   \
        \ contents_emb = torch.zeros(input_length,self.contents_dim)\n           \
        \     else:\n                    contents_emb = torch.FloatTensor(self.contents_emb(contents_index))\n\
        \n                if stroke_p < 0.3:\n                    stroke = torch.zeros(input_length,68)\n\
        \                else:\n                    stroke = torch.FloatTensor(self.make_stroke(contents))\n\
        \n                if contents_p < 0.3 and stroke_p < 0.3:\n              \
        \      style = torch.zeros(input_length,128)\n                else:\n    \
        \                style = self.style_enc(images)\n                    # style\
        \ = F.adaptive_avg_pool2d(style, (1, 1))\n                    style = self.style_conv(style)\n\
        \                    style = style.view(input_length, -1).cpu()\n\n      \
        \      elif mode == 3: #test\n                contents_emb = torch.FloatTensor(self.contents_emb(contents_index))\n\
        \                stroke = torch.FloatTensor(self.make_stroke(contents))\n\
        \                style = self.style_enc(images)\n                # style =\
        \ F.adaptive_avg_pool2d(style, (1, 1))\n                style = self.style_conv(style)\n\
        \                style = style.view(input_length, -1).cpu()\n\n          \
        \  elif mode == 4:\n                contents_emb = torch.FloatTensor(self.contents_emb(contents_index))\n\
        \                stroke = torch.FloatTensor(self.make_stroke(contents))\n\
        \                style = torch.zeros(input_length,128)\n\n            return\
        \ torch.cat([contents_emb,stroke,style],dim=1)\n\n    if os.path.isdir(result_model_path):\n\
        \        pass\n    else:\n        os.mkdir(result_model_path)\n    os.environ['CUDA_VISIBLE_DEVICES']\
        \ = str(gpu_num)\n    device = torch.device('cuda' if torch.cuda.is_available()\
        \ else 'cpu')\n    transforms = torchvision.transforms.Compose([\n       \
        \     # torchvision.transforms.Resize((input_size,input_size)),\n        \
        \    torchvision.transforms.Grayscale(num_output_channels=1),\n          \
        \  torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize((0.5),\
        \ (0.5))\n        ])\n    dataset = DiffusionDataset(\"diffusion_font_train.csv\"\
        ,transform=transforms)\n    dataloader = DataLoader(dataset, batch_size=batch_size,\
        \ shuffle=True,num_workers=12)\n    model = UNet().to(device)\n    optimizer\
        \ = optim.AdamW(model.parameters(), lr=lr)\n    loss_func = nn.MSELoss()\n\
        \    diffusion = Diffusion(first_beta=1e-4,\n                            end_beta=0.02,\n\
        \                            noise_step=1000,\n                          \
        \  beta_schedule_type='linear',\n                            img_size=input_size,\n\
        \                            device=device)\n\n    for epoch_id in range(n_epochs):\n\
        \        for i, (images, contents_label,filename) in enumerate(dataloader):\n\
        \            images = images.to(device)\n\n            contents = [dataset.label_to_y[int(content_index)]\
        \ for content_index in contents_label]\n            charAttar = CharAttar(num_classes=num_classes,device=device,style_path=style_path)\n\
        \            print(epoch_id)\n\n            charAttr_list = charAttar.make_charAttr(images,\
        \ contents_label,contents,mode=mode).to(device)\n\n            t = diffusion.sample_t(images.shape[0]).to(device)\n\
        \            x_t, noise = diffusion.noise_images(images, t)\n\n          \
        \  predicted_noise = model(x_t, t, charAttr_list)\n            loss = loss_func(noise,\
        \ predicted_noise)\n\n            optimizer.zero_grad()\n            loss.backward()\n\
        \            optimizer.step()\n        if epoch_id % 10 == 0:\n          \
        \  labels = torch.arange(num_classes).long().to(device)\n            # sampled_images\
        \ = diffusion.portion_sampling(model, n=len(dataset.labels),sampleImage_len\
        \ = sampleImage_len,dataset=dataset,mode =mode,charAttar=charAttar,sample_img=sample_img)\n\
        \            # plot_images(sampled_images)\n            torch.save(model,os.path.join(result_model_path,f\"\
        model_{epoch_id}.pt\"))\n            torch.save(model.state_dict(), os.path.join(result_model_path,\
        \ f\"ckpt_{epoch_id}.pt\"))\n            torch.save(optimizer.state_dict(),\
        \ os.path.join(result_model_path, f\"optim_{epoch_id}.pt\"))\n\nimport argparse\n\
        _parser = argparse.ArgumentParser(prog='Model training', description='')\n\
        _parser.add_argument(\"--gpu-num\", dest=\"gpu_num\", type=int, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch-size\", dest=\"\
        batch_size\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --input-size\", dest=\"input_size\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--n-epochs\", dest=\"n_epochs\", type=int, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--num-classes\", dest=\"\
        num_classes\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --mode\", dest=\"mode\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--lr\", dest=\"lr\", type=float, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--result-model-path\", dest=\"result_model_path\",\
        \ type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --style-path\", dest=\"style_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = model_training(**_parsed_args)\n"
      image: python:3.10
      resources:
        limits: {cpu: '8', memory: 16G}
      volumeMounts:
      - {mountPath: pvc, name: test-lee}
    inputs:
      parameters:
      - {name: batch_size}
      - {name: gpu_num}
      - {name: image_size}
      - {name: lr}
      - {name: mode}
      - {name: n_epochs}
      - {name: num_classes}
      - {name: result_model_path}
      - {name: style_path}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Training Korean Diffusion
          Model, pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--gpu-num", {"inputValue": "gpu_num"}, "--batch-size", {"inputValue":
          "batch_size"}, "--input-size", {"inputValue": "input_size"}, "--n-epochs",
          {"inputValue": "n_epochs"}, "--num-classes", {"inputValue": "num_classes"},
          "--mode", {"inputValue": "mode"}, "--lr", {"inputValue": "lr"}, "--result-model-path",
          {"inputValue": "result_model_path"}, "--style-path", {"inputValue": "style_path"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''tqdm==4.65.0'' ''pandas==2.0.0''
          '' pillow==9.4.0'' ''torch==2.0.0+cu118'' ''torchvision==0.15.1+cu118''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''tqdm==4.65.0'' ''pandas==2.0.0'' '' pillow==9.4.0'' ''torch==2.0.0+cu118''
          ''torchvision==0.15.1+cu118'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def model_training(gpu_num, batch_size, input_size, n_epochs, num_classes,
          mode, lr, result_model_path, style_path):\n    import os\n    import tqdm\n    import
          math\n    import random\n    import pandas as pd\n    import numpy as np\n    from
          PIL import Image\n\n    import torch, torchvision\n    from torch import
          optim\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from
          torch.utils.data import DataLoader, TensorDataset,Dataset\n\n    from functools
          import partial\n\n    from PIL import Image\n    class DiffusionDataset(Dataset):\n        def
          __init__(self, csv_path, transform =None):\n            self.transform =
          transform\n            self.csv_data = pd.read_csv(os.path.join(csv_path,\"diffusion_font_train.csv\"))\n            self.x_file_name
          = self.csv_data.iloc[:,0]\n            self.x_path = self.csv_data.iloc[:,1]\n            self.y
          = self.csv_data.iloc[:,2]\n            self.labels = np.unique(self.y)\n            self.y_to_label
          = self.make_y_to_label()\n            self.label_to_y = self.make_label_to_y()\n            self.y_labels
          = self.make_y_labels()\n\n        def make_y_to_label(self):\n            y_to_label_dict
          = {}\n            for label, value in enumerate(self.labels):\n                y_to_label_dict[value]
          = label\n            return y_to_label_dict\n\n        def make_label_to_y(self):\n            label_to_y_dict
          = {}\n            for label, value in enumerate(self.labels):\n                label_to_y_dict[label]
          = value\n            return label_to_y_dict\n\n        def make_y_labels(self):\n            y_labels
          = []\n            for y_ch in self.y:\n                y_labels.append(self.y_to_label[y_ch])\n            return
          y_labels\n\n        def __len__(self):\n            return len(self.x_path)\n\n        def
          __getitem__(self, id_):\n            filename = self.x_file_name[id_]\n            x
          = Image.open(self.x_path[id_])\n            transform_x = self.transform(x)\n            label
          = self.y_labels[id_]\n\n            return transform_x, label, filename\n\n    class
          Diffusion:    \n        def __init__(self, first_beta, end_beta, beta_schedule_type,
          noise_step, img_size, device):\n            self.first_beta = first_beta\n            self.end_beta
          = end_beta\n            self.beta_schedule_type = beta_schedule_type\n\n            self.noise_step
          = noise_step\n\n            self.beta_list = self.beta_schedule().to(device)\n\n            self.alphas
          =  1. - self.beta_list\n            self.alpha_bars = torch.cumprod(self.alphas,
          dim = 0)\n\n            self.img_size = img_size\n            self.device
          = device\n\n        def sample_t(self, batch_size):\n            return
          torch.randint(1,self.noise_step,(batch_size,))\n\n        def beta_schedule(self):\n            if
          self.beta_schedule_type == \"linear\":\n                return torch.linspace(self.first_beta,
          self.end_beta, self.noise_step)\n            elif self.beta_schedule_type
          == \"cosine\":\n                steps = self.noise_step + 1\n                s
          = 0.008\n                x = torch.linspace(0, self.noise_step, steps)\n                alphas_cumprod
          = torch.cos(((x / self.noise_step) + s) / (1 + s) * torch.pi * 0.5) ** 2\n                alphas_cumprod
          = alphas_cumprod / alphas_cumprod[0]\n                betas = 1 - (alphas_cumprod[1:]
          / alphas_cumprod[:-1])\n                return torch.clip(betas, 0.0001,
          0.9999)\n            elif self.beta_schedule_type == \"quadratic\":\n                return
          torch.linspace(self.first_beta ** 0.5, self.end_beta ** 0.5, self.noise_step)
          ** 2\n            elif self.beta_schedule_type == \"sigmoid\":\n                beta
          = torch.linspace(-6,-6,self.noise_step)\n                return torch.sigmoid(beta)
          * (self.end_beta - self.first_beta) + self.first_beta\n\n        def alpha_t(self,
          t):\n            return self.alphas[t][:, None, None, None]\n\n        def
          alpha_bar_t (self,t):\n            return self.alpha_bars[t][:, None, None,
          None]\n\n        def one_minus_alpha_bar(self,t):\n            return (1.
          - self.alpha_bars[t])[:, None, None, None]\n\n        def beta_t(self,t):\n            return
          self.beta_list[t][:, None, None, None]\n\n        def noise_images(self,x,t):\n            epsilon
          = torch.randn_like(x)\n            return torch.sqrt(self.alpha_bar_t(t))
          * x + torch.sqrt(self.one_minus_alpha_bar(t)) * epsilon , epsilon\n\n        def
          indexToChar(self,y):\n            return chr(44032+y)\n        def portion_sampling(self,
          model, n,sampleImage_len,dataset,mode,charAttar,sample_img, cfg_scale=3):\n            example_images
          = []\n            model.eval()\n            with torch.no_grad():\n                x_list
          = torch.randn((sampleImage_len, 1, self.img_size, self.img_size)).to(self.device)\n\n                y_idx
          = list(range(n))[::math.floor(n/sampleImage_len)][:sampleImage_len]\n                contents_index
          = torch.IntTensor(y_idx)\n                contents = [dataset.label_to_y[int(content_index)]
          for content_index in contents_index]\n                charAttr_list = charAttar.make_charAttr(sample_img,
          contents_index, contents,mode=3).to(self.device)\n\n                pbar
          = tqdm(list(reversed(range(1, self.noise_step))),desc=\"sampling\")\n                for
          i in pbar:\n                    dataset = TensorDataset(x_list,charAttr_list)\n                    batch_size=
          18\n                    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=False)\n                    predicted_noise
          = torch.tensor([]).to(self.device)\n                    uncond_predicted_noise
          = torch.tensor([]).to(self.device)\n                    for batch_x, batch_conditions
          in dataloader:\n                        batch_t = (torch.ones(len(batch_x))
          * i).long().to(self.device)\n                        batch_noise = model(batch_x,
          batch_t, batch_conditions)\n                        predicted_noise = torch.cat([predicted_noise,batch_noise],dim=0)\n                        #uncodition\n                        uncond_batch_noise
          = model(batch_x, batch_t, torch.zeros_like(batch_conditions))\n                        uncond_predicted_noise
          = torch.cat([uncond_predicted_noise,uncond_batch_noise],dim = 0)\n\n                    if
          cfg_scale > 0:\n                        predicted_noise = torch.lerp(uncond_predicted_noise,
          predicted_noise, cfg_scale)\n\n                    t = (torch.ones(sampleImage_len)
          * i).long()\n                    a_t = self.alpha_t(t)\n                    aBar_t
          = self.alpha_bar_t(t)\n                    b_t = self.beta_t(t)\n\n                    if
          i > 1:\n                        noise = torch.randn_like(x_list)\n                    else:\n                        noise
          = torch.zeros_like(x_list)\n\n                    x_list = 1 / torch.sqrt(a_t)
          * (\n                            x_list - ((1 - a_t) / (torch.sqrt(1 - aBar_t)))
          * predicted_noise) + torch.sqrt(\n                        b_t) * noise\n            model.train()\n            x_list
          = (x_list.clamp(-1, 1) + 1) / 2\n            x_list = (x_list * 255).type(torch.uint8)\n            return
          x_list\n\n        def test_sampling(self, model,sampleImage_len,charAttr_list,cfg_scale=3):\n            example_images
          = []\n            model.eval()\n            with torch.no_grad():\n                x_list
          = torch.randn((sampleImage_len, 3, self.img_size, self.img_size)).to(self.device)\n                pbar
          = tqdm(list(reversed(range(1, self.noise_step))),desc=\"sampling\")\n                for
          i in pbar:\n                    dataset = TensorDataset(x_list,charAttr_list)\n                    batch_size=
          4\n                    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=False)\n                    predicted_noise
          = torch.tensor([]).to(self.device)\n                    uncond_predicted_noise
          = torch.tensor([]).to(self.device)\n                    for batch_x, batch_conditions
          in dataloader:\n                        batch_t = (torch.ones(len(batch_x))
          * i).long().to(self.device)\n                        batch_noise = model(batch_x,
          batch_t, batch_conditions)\n                        predicted_noise = torch.cat([predicted_noise,batch_noise],dim=0)\n                        #uncodition\n                        uncond_batch_noise
          = model(batch_x, batch_t, torch.zeros_like(batch_conditions))\n                        uncond_predicted_noise
          = torch.cat([uncond_predicted_noise,uncond_batch_noise],dim = 0)\n\n                    if
          cfg_scale > 0:\n                        predicted_noise = torch.lerp(uncond_predicted_noise,
          predicted_noise, cfg_scale)\n\n                    t = (torch.ones(sampleImage_len)
          * i).long()\n                    a_t = self.alpha_t(t)\n                    aBar_t
          = self.alpha_bar_t(t)\n                    b_t = self.beta_t(t)\n\n                    if
          i > 1:\n                        noise = torch.randn_like(x_list)\n                    else:\n                        noise
          = torch.zeros_like(x_list)\n\n                    x_list = 1 / torch.sqrt(a_t)
          * (\n                            x_list - ((1 - a_t) / (torch.sqrt(1 - aBar_t)))
          * predicted_noise) + torch.sqrt(\n                        b_t) * noise\n            model.train()\n            x_list
          = (x_list.clamp(-1, 1) + 1) / 2\n            x_list = (x_list * 255).type(torch.uint8)\n            return
          x_list\n\n    class SelfAttention(nn.Module):\n        def __init__(self,
          channels):\n            super(SelfAttention, self).__init__()\n            self.channels
          = channels\n            self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n            self.ln
          = nn.LayerNorm([channels])\n            self.ff_self = nn.Sequential(\n                nn.LayerNorm([channels]),\n                nn.Linear(channels,
          channels),\n                nn.GELU(),\n                nn.Linear(channels,
          channels),\n            )\n\n        def forward(self, x):\n            size
          = x.shape[-1]\n            x = x.view(-1, self.channels, size * size).swapaxes(1,
          2)\n            x_ln = self.ln(x)\n            attention_value, _ = self.mha(x_ln,
          x_ln, x_ln)\n            attention_value = attention_value + x\n            attention_value
          = self.ff_self(attention_value) + attention_value\n            return attention_value.swapaxes(2,
          1).view(-1, self.channels, size, size)\n\n    class DoubleConv(nn.Module):\n        def
          __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n            super().__init__()\n            self.residual
          = residual\n            if not mid_channels:\n                mid_channels
          = out_channels\n            self.double_conv = nn.Sequential(\n                nn.Conv2d(in_channels,
          mid_channels, kernel_size=3, padding=1, bias=False),\n                nn.GroupNorm(1,
          mid_channels),\n                nn.GELU(),\n                nn.Conv2d(mid_channels,
          out_channels, kernel_size=3, padding=1, bias=False),\n                nn.GroupNorm(1,
          out_channels),\n            )\n\n        def forward(self, x):\n            if
          self.residual:\n                return F.gelu(x + self.double_conv(x))\n            else:\n                return
          self.double_conv(x)\n\n    class Down(nn.Module):\n        def __init__(self,
          in_channels, out_channels, time_dim=256, charAttr_dim=12456):\n            super().__init__()\n            self.maxpool_conv
          = nn.Sequential(\n                nn.MaxPool2d(2),\n                DoubleConv(in_channels,
          in_channels, residual=True),\n                DoubleConv(in_channels, out_channels),\n            )\n\n            self.time_layer
          = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(\n                    time_dim,\n                    out_channels\n                ),\n            )\n\n            self.condition_layer
          = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(\n                    charAttr_dim,\n                    out_channels\n                ),\n            )\n\n        def
          forward(self, x, t,charAttr):\n            x = self.maxpool_conv(x)\n            emb
          = self.time_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n            charAttr_emb
          = self.condition_layer(charAttr)[:, :, None, None].repeat(1, 1, x.shape[-2],
          x.shape[-1])\n            return x + emb + charAttr_emb\n\n    class Up(nn.Module):\n        def
          __init__(self, in_channels, out_channels, time_dim=256, charAttr_dim=12456):\n            super().__init__()\n\n            self.up
          = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n            self.conv
          = nn.Sequential(\n                DoubleConv(in_channels, in_channels, residual=True),\n                DoubleConv(in_channels,
          out_channels, in_channels // 2),\n            )\n\n            self.time_layer
          = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(\n                    time_dim,\n                    out_channels\n                ),\n            )\n            self.condition_layer
          = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(\n                    charAttr_dim,\n                    out_channels\n                ),\n            )\n\n        def
          forward(self, x, skip_x, t, charAttr):\n            x = self.up(x)\n            x
          = torch.cat([skip_x, x], dim=1)\n            x = self.conv(x)\n            time_emb
          = self.time_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n            charAttr_emb
          = self.condition_layer(charAttr)[:, :, None, None].repeat(1, 1, x.shape[-2],
          x.shape[-1])\n            return x + time_emb + charAttr_emb\n\n    class
          UNet(nn.Module):\n        def __init__(self, c_in=1, c_out=1, time_dim=256,
          charAttr_dim = 296, device=\"cuda\"):\n            super().__init__()\n            self.device
          = device\n            self.time_dim = time_dim\n            self.charAttr_dim
          = charAttr_dim\n\n            self.inc = DoubleConv(c_in, 64)\n            self.down1
          = Down(64, 128, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n            self.sa1
          = SelfAttention(128)\n            self.down2 = Down(128, 256,time_dim=self.time_dim,
          charAttr_dim=self.charAttr_dim)\n            self.sa2 = SelfAttention(256)\n            self.down3
          = Down(256, 256, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n            self.sa3
          = SelfAttention(256)\n\n            self.bot1 = DoubleConv(256, 512)\n            self.bot2
          = DoubleConv(512, 512)\n            self.bot3 = DoubleConv(512, 256)\n\n            self.up1
          = Up(512, 128, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n            self.sa4
          = SelfAttention(128)\n            self.up2 = Up(256, 64, time_dim=self.time_dim,
          charAttr_dim=self.charAttr_dim)\n            self.sa5 = SelfAttention(64)\n            self.up3
          = Up(128, 64, time_dim=self.time_dim, charAttr_dim=self.charAttr_dim)\n            self.sa6
          = SelfAttention(64)\n            self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n\n        def
          pos_encoding(self, t, channels):\n            inv_freq = 1.0 / (\n                10000\n                **
          (torch.arange(0, channels, 2, device=self.device).float() / channels)\n            )\n            pos_enc_a
          = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n            pos_enc_b
          = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n            pos_enc
          = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n            return pos_enc\n\n        def
          forward(self, x, time, charAttr):\n            time = time.unsqueeze(-1).type(torch.float)\n            time
          = self.pos_encoding(time, self.time_dim)\n\n            # if y is not None:\n            #     time
          += self.contents_emb(y)\n\n            x1 = self.inc(x)\n            x2
          = self.down1(x1, time, charAttr)\n            x2 = self.sa1(x2)\n            x3
          = self.down2(x2, time, charAttr)\n            x3 = self.sa2(x3)\n            x4
          = self.down3(x3, time, charAttr)\n            x4 = self.sa3(x4)\n\n            x4
          = self.bot1(x4)\n            x4 = self.bot2(x4)\n            x4 = self.bot3(x4)\n\n            x
          = self.up1(x4, x3, time, charAttr)\n            x = self.sa4(x)\n            x
          = self.up2(x, x2, time, charAttr)\n            x = self.sa5(x)\n            x
          = self.up3(x, x1, time, charAttr)\n            x = self.sa6(x)\n            output
          = self.outc(x)\n            return output\n\n    class GlobalContext(nn.Module):\n        \"\"\"
          Global-context \"\"\"\n        def __init__(self, C, bottleneck_ratio=0.25,
          w_norm=''none''):\n            super().__init__()\n            C_bottleneck
          = int(C * bottleneck_ratio)\n            w_norm = w_norm_dispatch(w_norm)\n            self.k_proj
          = w_norm(nn.Conv2d(C, 1, 1))\n            self.transform = nn.Sequential(\n                w_norm(nn.Linear(C,
          C_bottleneck)),\n                nn.LayerNorm(C_bottleneck),\n                nn.ReLU(),\n                w_norm(nn.Linear(C_bottleneck,
          C))\n            )\n\n        def forward(self, x):\n            # x: [B,
          C, H, W]\n            context_logits = self.k_proj(x)  # [B, 1, H, W]\n            context_weights
          = F.softmax(context_logits.flatten(1), dim=1)  # [B, HW]\n            context
          = torch.einsum(''bci,bi->bc'', x.flatten(2), context_weights)\n            out
          = self.transform(context)\n\n            return out[..., None, None]\n\n    class
          GCBlock(nn.Module):\n        \"\"\" Global-context block \"\"\"\n        def
          __init__(self, C, bottleneck_ratio=0.25, w_norm=''none''):\n            super().__init__()\n            self.gc
          = GlobalContext(C, bottleneck_ratio, w_norm)\n\n        def forward(self,
          x):\n            gc = self.gc(x)\n            return x + gc\n\n    class
          TLU(nn.Module):\n        \"\"\" Thresholded Linear Unit \"\"\"\n        def
          __init__(self, num_features):\n            super().__init__()\n            self.num_features
          = num_features\n            self.tau = nn.Parameter(torch.zeros(1, num_features,
          1, 1))\n\n        def forward(self, x):\n            return torch.max(x,
          self.tau)\n\n        def extra_repr(self):\n            return ''num_features={}''.format(self.num_features)\n\n    #
          NOTE generalized version\n    class FilterResponseNorm(nn.Module):\n        \"\"\"
          Filter Response Normalization \"\"\"\n        def __init__(self, num_features,
          ndim, eps=None, learnable_eps=False):\n            \"\"\"\n            Args:\n                num_features\n                ndim\n                eps:
          if None is given, use the paper value as default.\n                    from
          paper, fixed_eps=1e-6 and learnable_eps_init=1e-4.\n                learnable_eps:
          turn eps to learnable parameter, which is recommended on\n                    fully-connected
          or 1x1 activation map.\n            \"\"\"\n            super().__init__()\n            if
          eps is None:\n                if learnable_eps:\n                    eps
          = 1e-4\n                else:\n                    eps = 1e-6\n\n            self.num_features
          = num_features\n            self.init_eps = eps\n            self.learnable_eps
          = learnable_eps\n            self.ndim = ndim\n\n            self.mean_dims
          = list(range(2, 2+ndim))\n\n            self.weight = nn.Parameter(torch.ones([1,
          num_features] + [1]*ndim))\n            self.bias = nn.Parameter(torch.zeros([1,
          num_features] + [1]*ndim))\n            if learnable_eps:\n                self.eps
          = nn.Parameter(torch.as_tensor(eps))\n            else:\n                self.register_buffer(''eps'',
          torch.as_tensor(eps))\n\n        def forward(self, x):\n            # normalize\n            nu2
          = x.pow(2).mean(self.mean_dims, keepdim=True)\n            x = x * torch.rsqrt(nu2
          + self.eps.abs())\n\n            # modulation\n            x = x * self.weight
          + self.bias\n\n            return x\n\n        def extra_repr(self):\n            return
          ''num_features={}, init_eps={}, ndim={}''.format(\n                    self.num_features,
          self.init_eps, self.ndim)\n\n    FilterResponseNorm1d = partial(FilterResponseNorm,
          ndim=1, learnable_eps=True)\n    FilterResponseNorm2d = partial(FilterResponseNorm,
          ndim=2)\n\n    def split_dim(x, dim, n_chunks):\n        shape = x.shape\n        assert
          shape[dim] % n_chunks == 0\n        return x.view(*shape[:dim], n_chunks,
          shape[dim] // n_chunks, *shape[dim+1:])\n\n    def weights_init(init_type=''default''):\n        \"\"\"
          Adopted from FUNIT \"\"\"\n        def init_fun(m):\n            classname
          = m.__class__.__name__\n            if (classname.find(''Conv'') == 0 or
          classname.find(''Linear'') == 0) and hasattr(m, ''weight''):\n                if
          init_type == ''gaussian'':\n                    nn.init.normal_(m.weight.data,
          0.0, 0.02)\n                elif init_type == ''xavier'':\n                    nn.init.xavier_normal_(m.weight.data,
          gain=2**0.5)\n                elif init_type == ''kaiming'':\n                    nn.init.kaiming_normal_(m.weight.data,
          a=0, mode=''fan_in'')\n                elif init_type == ''orthogonal'':\n                    nn.init.orthogonal_(m.weight.data,
          gain=2**0.5)\n                elif init_type == ''default'':\n                    pass\n                else:\n                    assert
          0, \"Unsupported initialization: {}\".format(init_type)\n\n                if
          hasattr(m, ''bias'') and m.bias is not None:\n                    nn.init.constant_(m.bias.data,
          0.0)\n\n        return init_fun\n\n    def spectral_norm(module):\n        \"\"\"
          init & apply spectral norm \"\"\"\n        nn.init.xavier_uniform_(module.weight,
          2 ** 0.5)\n        if hasattr(module, ''bias'') and module.bias is not None:\n            module.bias.data.zero_()\n\n        return
          nn.utils.spectral_norm(module)\n\n    class BasicConv(nn.Module):\n        def
          __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0,
          dilation=1, groups=1, relu=True, bn=True, bias=False):\n            super(BasicConv,
          self).__init__()\n            self.out_channels = out_planes\n            self.conv
          = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,
          padding=padding, dilation=dilation, groups=groups, bias=bias)\n            self.bn
          = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn
          else None\n            self.relu = nn.ReLU() if relu else None\n\n        def
          forward(self, x):\n            x = self.conv(x)\n            if self.bn
          is not None:\n                x = self.bn(x)\n            if self.relu is
          not None:\n                x = self.relu(x)\n            return x\n\n    class
          Flatten(nn.Module):\n\n        def forward(self, x):\n            return
          x.view(x.size(0), -1)\n\n    class ChannelGate(nn.Module):\n        def
          __init__(self, gate_channels, reduction_ratio=16, pool_types=[''avg'', ''max'']):\n            super(ChannelGate,
          self).__init__()\n            self.gate_channels = gate_channels\n            self.mlp
          = nn.Sequential(\n                Flatten(),\n                nn.Linear(gate_channels,
          gate_channels // reduction_ratio),\n                nn.ReLU(),\n                nn.Linear(gate_channels
          // reduction_ratio, gate_channels)\n                )\n            self.pool_types
          = pool_types\n\n        def forward(self, x):\n            channel_att_sum
          = None\n            for pool_type in self.pool_types:\n                if
          pool_type == ''avg'':\n                    avg_pool = F.avg_pool2d(x, (x.size(2),
          x.size(3)), stride=(x.size(2), x.size(3)))\n                    channel_att_raw
          = self.mlp(avg_pool)\n                elif pool_type == ''max'':\n                    max_pool
          = F.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n                    channel_att_raw
          = self.mlp(max_pool)\n                elif pool_type == ''lp'':\n                    lp_pool
          = F.lp_pool2d(x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n                    channel_att_raw
          = self.mlp(lp_pool)\n                elif pool_type == ''lse'':\n                    #
          LSE pool only\n                    lse_pool = logsumexp_2d(x)\n                    channel_att_raw
          = self.mlp(lse_pool)\n\n                if channel_att_sum is None:\n                    channel_att_sum
          = channel_att_raw\n                else:\n                    channel_att_sum
          = channel_att_sum + channel_att_raw\n\n            scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n            return
          x * scale\n\n    def logsumexp_2d(tensor):\n        tensor_flatten = tensor.view(tensor.size(0),
          tensor.size(1), -1)\n        s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n        outputs
          = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n        return
          outputs\n\n    class ChannelPool(nn.Module):\n        def forward(self,
          x):\n            return torch.cat((torch.max(x, 1)[0].unsqueeze(1), torch.mean(x,
          1).unsqueeze(1)), dim=1)\n\n    class SpatialGate(nn.Module):\n        def
          __init__(self):\n            super(SpatialGate, self).__init__()\n            kernel_size
          = 7\n            self.compress = ChannelPool()\n            self.spatial
          = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n\n        def
          forward(self, x):\n            x_compress = self.compress(x)\n            x_out
          = self.spatial(x_compress)\n            scale = torch.sigmoid(x_out)  #
          broadcasting\n            return x * scale\n\n    class CBAM(nn.Module):\n        def
          __init__(self, gate_channels, reduction_ratio=16, pool_types=[''avg'', ''max''],
          no_spatial=False):\n            super(CBAM, self).__init__()\n            self.ChannelGate
          = ChannelGate(gate_channels, reduction_ratio, pool_types)\n            self.no_spatial
          = no_spatial\n            if not no_spatial:\n                self.SpatialGate
          = SpatialGate()\n\n        def forward(self, x):\n            x_out = self.ChannelGate(x)\n            if
          not self.no_spatial:\n                x_out = self.SpatialGate(x_out)\n            return
          x_out\n\n    class Flatten(nn.Module):\n        def __init__(self, start_dim=1,
          end_dim=-1):\n            super(Flatten, self).__init__()\n            self.start_dim
          = start_dim\n            self.end_dim = end_dim\n\n        def forward(self,
          input):\n            return input.flatten(self.start_dim, self.end_dim)\n\n    def
          dispatcher(dispatch_fn):\n        def decorated(key, *args):\n            if
          callable(key):\n                return key\n\n            if key is None:\n                key
          = ''none''\n\n            return dispatch_fn(key, *args)\n        return
          decorated\n\n    @dispatcher\n    def norm_dispatch(norm):\n        return
          {\n            ''none'': nn.Identity,\n            ''in'': partial(nn.InstanceNorm2d,
          affine=False),  # false as default\n            ''bn'': nn.BatchNorm2d,\n            ''frn'':
          FilterResponseNorm2d\n        }[norm.lower()]\n\n    @dispatcher\n    def
          w_norm_dispatch(w_norm):\n        # NOTE Unlike other dispatcher, w_norm
          is function, not class.\n        return {\n            ''spectral'': spectral_norm,\n            ''none'':
          lambda x: x\n        }[w_norm.lower()]\n\n    @dispatcher\n    def activ_dispatch(activ,
          norm=None):\n        if norm_dispatch(norm) == FilterResponseNorm2d:\n            #
          use TLU for FRN\n            activ = ''tlu''\n\n        return {\n            \"none\":
          nn.Identity,\n            \"relu\": nn.ReLU,\n            \"lrelu\": partial(nn.LeakyReLU,
          negative_slope=0.2),\n            \"tlu\": TLU\n        }[activ.lower()]\n\n    @dispatcher\n    def
          pad_dispatch(pad_type):\n        return {\n            \"zero\": nn.ZeroPad2d,\n            \"replicate\":
          nn.ReplicationPad2d,\n            \"reflect\": nn.ReflectionPad2d\n        }[pad_type.lower()]\n\n    class
          ParamBlock(nn.Module):\n        def __init__(self, C_out, shape):\n            super().__init__()\n            w
          = torch.randn((C_out, *shape))\n            b = torch.randn((C_out,))\n            self.shape
          = shape\n            self.w = nn.Parameter(w)\n            self.b = nn.Parameter(b)\n\n        def
          forward(self, x):\n            b = self.b.reshape((1, *self.b.shape, 1,
          1, 1)).repeat(x.size(0), 1, *self.shape)\n            return self.w*x +
          b\n\n    class LinearBlock(nn.Module):\n        \"\"\" pre-active linear
          block \"\"\"\n        def __init__(self, C_in, C_out, norm=''none'', activ=''relu'',
          bias=True, w_norm=''none'',\n                    dropout=0.):\n            super().__init__()\n            activ
          = activ_dispatch(activ, norm)\n            if norm.lower() == ''bn'':\n                norm
          = nn.BatchNorm1d\n            elif norm.lower() == ''frn'':\n                norm
          = FilterResponseNorm1d\n            elif norm.lower() == ''none'':\n                norm
          = nn.Identity\n            else:\n                raise ValueError(f\"LinearBlock
          supports BN only (but {norm} is given)\")\n            w_norm = w_norm_dispatch(w_norm)\n            self.norm
          = norm(C_in)\n            self.activ = activ()\n            if dropout >
          0.:\n                self.dropout = nn.Dropout(p=dropout)\n            self.linear
          = w_norm(nn.Linear(C_in, C_out, bias))\n\n        def forward(self, x):\n            x
          = self.norm(x)\n            x = self.activ(x)\n            if hasattr(self,
          ''dropout''):\n                x = self.dropout(x)\n            return self.linear(x)\n\n    class
          ConvBlock(nn.Module):\n        \"\"\" pre-active conv block \"\"\"\n        def
          __init__(self, C_in, C_out, kernel_size=3, stride=1, padding=1, norm=''none'',\n                    activ=''relu'',
          bias=True, upsample=False, downsample=False, w_norm=''none'',\n                    pad_type=''zero'',
          dropout=0., size=None):\n            # 1x1 conv assertion\n            if
          kernel_size == 1:\n                assert padding == 0\n            super().__init__()\n            self.C_in
          = C_in\n            self.C_out = C_out\n\n            activ = activ_dispatch(activ,
          norm)\n            norm = norm_dispatch(norm)\n            w_norm = w_norm_dispatch(w_norm)\n            pad
          = pad_dispatch(pad_type)\n            self.upsample = upsample\n            self.downsample
          = downsample\n\n            assert ((norm == FilterResponseNorm2d) == (activ
          == TLU)), \"Use FRN and TLU together\"\n\n            if norm == FilterResponseNorm2d
          and size == 1:\n                self.norm = norm(C_in, learnable_eps=True)\n            else:\n                self.norm
          = norm(C_in)\n            if activ == TLU:\n                self.activ =
          activ(C_in)\n            else:\n                self.activ = activ()\n            if
          dropout > 0.:\n                self.dropout = nn.Dropout2d(p=dropout)\n            self.pad
          = pad(padding)\n            self.conv = w_norm(nn.Conv2d(C_in, C_out, kernel_size,
          stride, bias=bias))\n\n        def forward(self, x):\n            x = self.norm(x)\n            x
          = self.activ(x)\n            if self.upsample:\n                x = F.interpolate(x,
          scale_factor=2)\n            if hasattr(self, ''dropout''):\n                x
          = self.dropout(x)\n            x = self.conv(self.pad(x))\n            if
          self.downsample:\n                x = F.avg_pool2d(x, 2)\n            return
          x\n\n    class ResBlock(nn.Module):\n        \"\"\" Pre-activate ResBlock
          with spectral normalization \"\"\"\n        def __init__(self, C_in, C_out,
          kernel_size=3, padding=1, upsample=False, downsample=False,\n                    norm=''none'',
          w_norm=''none'', activ=''relu'', pad_type=''zero'', dropout=0.,\n                    scale_var=False):\n            assert
          not (upsample and downsample)\n            super().__init__()\n            w_norm
          = w_norm_dispatch(w_norm)\n            self.C_in = C_in\n            self.C_out
          = C_out\n            self.upsample = upsample\n            self.downsample
          = downsample\n            self.scale_var = scale_var\n\n            self.conv1
          = ConvBlock(C_in, C_out, kernel_size, 1, padding, norm, activ,\n                                upsample=upsample,
          w_norm=w_norm, pad_type=pad_type,\n                                dropout=dropout)\n            self.conv2
          = ConvBlock(C_out, C_out, kernel_size, 1, padding, norm, activ,\n                                w_norm=w_norm,
          pad_type=pad_type, dropout=dropout)\n\n            # XXX upsample / downsample
          needs skip conv?\n            if C_in != C_out or upsample or downsample:\n                self.skip
          = w_norm(nn.Conv2d(C_in, C_out, 1))\n\n        def forward(self, x):\n            \"\"\"\n            normal:
          pre-activ + convs + skip-con\n            upsample: pre-activ + upsample
          + convs + skip-con\n            downsample: pre-activ + convs + downsample
          + skip-con\n            => pre-activ + (upsample) + convs + (downsample)
          + skip-con\n            \"\"\"\n            out = x\n\n            out =
          self.conv1(out)\n            out = self.conv2(out)\n\n            if self.downsample:\n                out
          = F.avg_pool2d(out, 2)\n\n            # skip-con\n            if hasattr(self,
          ''skip''):\n                if self.upsample:\n                    x = F.interpolate(x,
          scale_factor=2)\n                x = self.skip(x)\n                if self.downsample:\n                    x
          = F.avg_pool2d(x, 2)\n\n            out = out + x\n            if self.scale_var:\n                out
          = out / np.sqrt(2)\n            return out\n\n    class Upsample1x1(nn.Module):\n        \"\"\"Upsample
          1x1 to 2x2 using Linear\"\"\"\n        def __init__(self, C_in, C_out, norm=''none'',
          activ=''relu'', w_norm=''none''):\n            assert norm.lower() != ''in'',
          ''Do not use instance norm for 1x1 spatial size''\n            super().__init__()\n            self.C_in
          = C_in\n            self.C_out = C_out\n            self.proj = ConvBlock(\n                C_in,
          C_out*4, 1, 1, 0, norm=norm, activ=activ, w_norm=w_norm\n            )\n\n        def
          forward(self, x):\n            # x: [B, C_in, 1, 1]\n            x = self.proj(x)  #
          [B, C_out*4, 1, 1]\n            B, C = x.shape[:2]\n            return x.view(B,
          C//4, 2, 2)\n\n    class HourGlass(nn.Module):\n        \"\"\"U-net like
          hourglass module\"\"\"\n        def __init__(self, C_in, C_max, size, n_downs,
          n_mids=1, norm=''none'', activ=''relu'',\n                    w_norm=''none'',
          pad_type=''zero''):\n            \"\"\"\n            Args:\n                C_max:
          maximum C_out of left downsampling block''s output\n            \"\"\"\n            super().__init__()\n            assert
          size == n_downs ** 2, \"HGBlock assume that the spatial size is downsampled
          to 1x1.\"\n            self.C_in = C_in\n\n            ConvBlk = partial(ConvBlock,
          norm=norm, activ=activ, w_norm=w_norm, pad_type=pad_type)\n\n            self.lefts
          = nn.ModuleList()\n            c_in = C_in\n            for i in range(n_downs):\n                c_out
          = min(c_in*2, C_max)\n                self.lefts.append(ConvBlk(c_in, c_out,
          downsample=True))\n                c_in = c_out\n\n            # 1x1 conv
          for mids\n            self.mids = nn.Sequential(\n                *[\n                    ConvBlk(c_in,
          c_out, kernel_size=1, padding=0)\n                    for _ in range(n_mids)\n                ]\n            )\n\n            self.rights
          = nn.ModuleList()\n            for i, lb in enumerate(self.lefts[::-1]):\n                c_out
          = lb.C_in\n                c_in = lb.C_out\n                channel_in =
          c_in*2 if i else c_in  # for channel concat\n                if i == 0:\n                    block
          = Upsample1x1(channel_in, c_out, norm=norm, activ=activ, w_norm=w_norm)\n                else:\n                    block
          = ConvBlk(channel_in, c_out, upsample=True)\n                self.rights.append(block)\n\n        def
          forward(self, x):\n            features = []\n            for lb in self.lefts:\n                x
          = lb(x)\n                features.append(x)\n\n            assert x.shape[-2:]
          == torch.Size((1, 1))\n\n            for i, (rb, lf) in enumerate(zip(self.rights,
          features[::-1])):\n                if i:\n                    x = torch.cat([x,
          lf], dim=1)\n                x = rb(x)\n\n            return x\n\n    class
          StyleEncoder(nn.Module):\n        def __init__(self, layers, out_shape):\n            super().__init__()\n\n            self.layers
          = nn.Sequential(*layers)\n            self.out_shape = out_shape\n\n        def
          forward(self, x):\n            style_feat = self.layers(x)\n\n            return
          style_feat\n\n    def style_enc_builder(C_in, C, norm=''none'', activ=''relu'',
          pad_type=''reflect'', skip_scale_var=False):\n\n        ConvBlk = partial(ConvBlock,
          norm=norm, activ=activ, pad_type=pad_type)\n\n        layers = [\n            ConvBlk(C_in,
          C, 3, 1, 1, norm=''none'', activ=''none''),\n            ConvBlk(C*1, C*2,
          3, 1, 1, downsample=True),\n            GCBlock(C*2),\n            ConvBlk(C*2,
          C*4, 3, 1, 1, downsample=True),\n            CBAM(C*4)\n        ]\n\n        out_shape
          = (C*4, 32, 32)\n\n        return StyleEncoder(layers, out_shape)\n\n    class
          CharAttar:\n        def __init__(self,num_classes,device,style_path):\n            self.num_classes
          = num_classes\n            self.device = device\n            self.contents_dim
          = 100\n            self.contents_emb = nn.Embedding(num_classes, self.contents_dim)\n            self.style_enc
          = self.make_style_enc(os.path.join(style_path,\"style_enc.pth\"))\n            self.style_conv
          = nn.Sequential(\n                                nn.Conv2d(128,128,16),\n                                nn.SiLU(),\n                            ).to(device)\n\n        def
          make_stroke(self,contents):\n            strokes_list = []\n            for
          content in contents:\n                content_code = ord(content)\n                first_letter_code
          = 44032\n                stroke = [0] * 68\n                first_consonant_letter
          = int((content_code - first_letter_code) / 588)\n                middle_consonant_letter
          = int(((content_code - first_letter_code) - (first_consonant_letter * 588))
          / 28)\n                last_consonant_letter = int((content_code - first_letter_code)
          - (first_consonant_letter * 588) - (middle_consonant_letter * 28))\n                stroke[first_consonant_letter]
          = 1\n                stroke[middle_consonant_letter + 19] = 1\n                stroke[last_consonant_letter
          + 19 + 21] = 1\n                strokes_list.append(stroke)\n            return
          strokes_list\n\n        def make_style_enc(self,style_enc_path):\n            C
          ,C_in = 32, 1\n            sty_encoder = style_enc_builder(C_in, C)\n            checkpoint
          = torch.load(style_enc_path, map_location=self.device)\n            tmp_dict
          = {}\n            for k, v in checkpoint.items():\n                if k
          in sty_encoder.state_dict():\n                    tmp_dict[k] = v\n            sty_encoder.load_state_dict(tmp_dict)\n            #
          frozen sty_encoder\n            for p in sty_encoder.parameters():\n                p.requires_grad
          = False\n            return sty_encoder.to(self.device)\n\n        # def
          set_charAttr_dim(mode):\n        #     pass\n        def make_charAttr(self,images,contents_index,
          contents,mode):\n            input_length = images.shape[0]\n            #
          contents_index = [int(content_index) for content_index in contents_index]\n            #
          style_encoder = style_enc_builder(1,32).to(self.device)\n\n            contents_emb
          = None\n            stroke =  None\n            style = None\n            contents_p,
          stroke_p = random.random(), random.random()\n            if mode == 1:\n\n                if
          contents_p < 0.3:\n                    contents_emb = torch.zeros(input_length,self.contents_dim)\n                else:\n                    contents_emb
          = torch.FloatTensor(self.contents_emb(contents_index))\n\n                if
          stroke_p < 0.3:\n                    stroke = torch.zeros(input_length,68)\n                else:\n                    stroke
          =  torch.FloatTensor(self.make_stroke(contents))\n\n                style
          = torch.zeros(input_length,128)\n\n            elif mode == 2:\n\n                if
          contents_p < 0.3:\n                    contents_emb = torch.zeros(input_length,self.contents_dim)\n                else:\n                    contents_emb
          = torch.FloatTensor(self.contents_emb(contents_index))\n\n                if
          stroke_p < 0.3:\n                    stroke = torch.zeros(input_length,68)\n                else:\n                    stroke
          = torch.FloatTensor(self.make_stroke(contents))\n\n                if contents_p
          < 0.3 and stroke_p < 0.3:\n                    style = torch.zeros(input_length,128)\n                else:\n                    style
          = self.style_enc(images)\n                    # style = F.adaptive_avg_pool2d(style,
          (1, 1))\n                    style = self.style_conv(style)\n                    style
          = style.view(input_length, -1).cpu()\n\n            elif mode == 3: #test\n                contents_emb
          = torch.FloatTensor(self.contents_emb(contents_index))\n                stroke
          = torch.FloatTensor(self.make_stroke(contents))\n                style =
          self.style_enc(images)\n                # style = F.adaptive_avg_pool2d(style,
          (1, 1))\n                style = self.style_conv(style)\n                style
          = style.view(input_length, -1).cpu()\n\n            elif mode == 4:\n                contents_emb
          = torch.FloatTensor(self.contents_emb(contents_index))\n                stroke
          = torch.FloatTensor(self.make_stroke(contents))\n                style =
          torch.zeros(input_length,128)\n\n            return torch.cat([contents_emb,stroke,style],dim=1)\n\n    if
          os.path.isdir(result_model_path):\n        pass\n    else:\n        os.mkdir(result_model_path)\n    os.environ[''CUDA_VISIBLE_DEVICES'']
          = str(gpu_num)\n    device = torch.device(''cuda'' if torch.cuda.is_available()
          else ''cpu'')\n    transforms = torchvision.transforms.Compose([\n            #
          torchvision.transforms.Resize((input_size,input_size)),\n            torchvision.transforms.Grayscale(num_output_channels=1),\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize((0.5),
          (0.5))\n        ])\n    dataset = DiffusionDataset(\"diffusion_font_train.csv\",transform=transforms)\n    dataloader
          = DataLoader(dataset, batch_size=batch_size, shuffle=True,num_workers=12)\n    model
          = UNet().to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    loss_func
          = nn.MSELoss()\n    diffusion = Diffusion(first_beta=1e-4,\n                            end_beta=0.02,\n                            noise_step=1000,\n                            beta_schedule_type=''linear'',\n                            img_size=input_size,\n                            device=device)\n\n    for
          epoch_id in range(n_epochs):\n        for i, (images, contents_label,filename)
          in enumerate(dataloader):\n            images = images.to(device)\n\n            contents
          = [dataset.label_to_y[int(content_index)] for content_index in contents_label]\n            charAttar
          = CharAttar(num_classes=num_classes,device=device,style_path=style_path)\n            print(epoch_id)\n\n            charAttr_list
          = charAttar.make_charAttr(images, contents_label,contents,mode=mode).to(device)\n\n            t
          = diffusion.sample_t(images.shape[0]).to(device)\n            x_t, noise
          = diffusion.noise_images(images, t)\n\n            predicted_noise = model(x_t,
          t, charAttr_list)\n            loss = loss_func(noise, predicted_noise)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if
          epoch_id % 10 == 0:\n            labels = torch.arange(num_classes).long().to(device)\n            #
          sampled_images = diffusion.portion_sampling(model, n=len(dataset.labels),sampleImage_len
          = sampleImage_len,dataset=dataset,mode =mode,charAttar=charAttar,sample_img=sample_img)\n            #
          plot_images(sampled_images)\n            torch.save(model,os.path.join(result_model_path,f\"model_{epoch_id}.pt\"))\n            torch.save(model.state_dict(),
          os.path.join(result_model_path, f\"ckpt_{epoch_id}.pt\"))\n            torch.save(optimizer.state_dict(),
          os.path.join(result_model_path, f\"optim_{epoch_id}.pt\"))\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Model training'', description='''')\n_parser.add_argument(\"--gpu-num\",
          dest=\"gpu_num\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch-size\",
          dest=\"batch_size\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-size\",
          dest=\"input_size\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--n-epochs\",
          dest=\"n_epochs\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--num-classes\",
          dest=\"num_classes\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mode\",
          dest=\"mode\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lr\",
          dest=\"lr\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--result-model-path\",
          dest=\"result_model_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--style-path\",
          dest=\"style_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = model_training(**_parsed_args)\n"],
          "image": "python:3.10"}}, "inputs": [{"name": "gpu_num", "type": "Integer"},
          {"name": "batch_size", "type": "Integer"}, {"name": "input_size", "type":
          "Integer"}, {"name": "n_epochs", "type": "Integer"}, {"name": "num_classes",
          "type": "Integer"}, {"name": "mode", "type": "Integer"}, {"name": "lr",
          "type": "Float"}, {"name": "result_model_path", "type": "String"}, {"name":
          "style_path", "type": "String"}], "name": "Model training"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"batch_size": "{{inputs.parameters.batch_size}}",
          "gpu_num": "{{inputs.parameters.gpu_num}}", "input_size": "{{inputs.parameters.image_size}}",
          "lr": "{{inputs.parameters.lr}}", "mode": "{{inputs.parameters.mode}}",
          "n_epochs": "{{inputs.parameters.n_epochs}}", "num_classes": "{{inputs.parameters.num_classes}}",
          "result_model_path": "{{inputs.parameters.result_model_path}}", "style_path":
          "{{inputs.parameters.style_path}}"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.16
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
    volumes:
    - name: test-lee
      persistentVolumeClaim: {claimName: diffusion}
  arguments:
    parameters:
    - {name: style_url}
    - {name: fonts_url}
    - {name: image_size}
    - {name: train_string}
    - {name: fonts_base_path}
    - {name: result_path}
    - {name: csv_path}
    - {name: gpu_num}
    - {name: batch_size}
    - {name: n_epochs}
    - {name: num_classes}
    - {name: mode}
    - {name: lr}
    - {name: result_model_path}
    - {name: style_path}
  serviceAccountName: pipeline-runner
