{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAACTklEQVR4nO2WTUhUURTH/6/n4AxGY04iKJhZMRu/FooEaRChREpQi8CWunDTpmWrMEhBcJMLCyFt0Vp058IwDAShcRiXStQiQnQyXNggzmnRuc9777t3Pnx9wpzVeeec+7vnnPvxrkMIJqcCji8BSoATAY7ePWpVLWSXzelPqiH9+n5MH5IL8AaIP1jY56+NsS7XMKfjHabtV3q6W1MAELrS09cKpO8se0mbS8i2WAvvIiJKmqs+bqIzYgWcBYDLZp+0Crc7bIBzAODkBeCJDWAvDnDkjnRnelzp81malUQbgO8RYbc0kYhW9+WvDyLpxiwR0YE2dd3PqDLZ1qlEzIqZhi3l+zNQZDfKEdV7lCMD+1l4/I2Vp1FrDIAym2NukpW+Qd11CwCvrb2EdTHtpa9sOS5BCbQA3orxFz4KU1GAmTDHNnvjiwFsD4jQu9LGKBiQmaziwOhL2e4BQjkB6Yl6jgs/3FE8HuC8HfD5Rb8ovmX8i4b2AMOKWRym7FIqlUgSADgNHddvXNQX3ztM8eUa2SwAB+0IRSpisdqGxqYzvsEAcNgPIFRz9V5EMTtBHxjKVk6u+fxDmDbYZJEbsuGfgAwZ5ljG3qCAxaAAai4aoK1C5pCVmysczNe5uweg8ohtkmgXSnk5Ky40Oe3PBMAveB+oBV0zuDkjIiLT3/n/e+L4xHytJ4IC2goH/KYSNj0tWs1KamfrfdYUm2cfDOY7C390GeOmd0LBALfzecoUrDZxdFf312MBAMJV8Qpg3tDFwLfy3z8LJcC/APgBWmMFmMlE0cMAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=64x64>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"/home/wonguk/coding/paper_project/hojun/data/62570_갊.png\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "transforms = torchvision.transforms.Compose([\n",
    "        # torchvision.transforms.Resize((input_size,input_size)),\n",
    "        torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "img = transforms(img)\n",
    "img.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 64, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "img_batch = torch.unsqueeze(img,0)\n",
    "img_batch = img_batch.repeat(8,1,1,1).to('cuda')\n",
    "img_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t : torch.Size([8])\n",
    "# char : torch.Size([8, 296])\n",
    "\n",
    "t = torch.randn([8]).to('cuda')\n",
    "char = torch.randn([8,296]).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    " \n",
    "tensorrt_path = '/home/wonguk/coding/paper_project/hojun/light_weight/tensorrt.plan'\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "trt.init_libnvinfer_plugins(None, \"\")\n",
    "with open(tensorrt_path, 'rb') as f:\n",
    "    engine_data = f.read()\n",
    "    engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "device = cuda.Device(0)\n",
    "ctx = device.make_context()\n",
    "\n",
    "\n",
    "# PyTorch 텐서를 NumPy 배열로 변환\n",
    "img_batch_np = img_batch.cpu().numpy()\n",
    "t_np = t.cpu().numpy()\n",
    "char_np = char.cpu().numpy()\n",
    "\n",
    "input_gpu1 = cuda.mem_alloc(img_batch_np.nbytes)\n",
    "input_gpu2 = cuda.mem_alloc(t_np.nbytes)\n",
    "input_gpu3 = cuda.mem_alloc(char_np.nbytes)\n",
    "\n",
    "cuda.memcpy_htod(input_gpu1, img_batch_np)\n",
    "cuda.memcpy_htod(input_gpu2, t_np )\n",
    "cuda.memcpy_htod(input_gpu3, char_np)\n",
    "\n",
    "input_gpu1.free()\n",
    "input_gpu2.free()\n",
    "input_gpu3.free()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/11/2023-00:00:36] [TRT] [E] 1: [slice.cu::launchNaiveSliceImpl::245] Error Code 1: Cuda Runtime (invalid resource handle)\n"
     ]
    }
   ],
   "source": [
    "bindings = [int(input_gpu1), int(input_gpu2), int(input_gpu3)]\n",
    "\n",
    "with engine.create_execution_context() as context:\n",
    "    context.execute_v2(bindings=bindings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/2023-00:41:15] [TRT] [E] 1: [convolutionRunner.cpp::executeConv::465] Error Code 1: Cudnn (CUDNN_STATUS_BAD_PARAM)\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit # 이걸로 gpu 초기화를 진행했는데 추가 에러 발생\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "img = Image.open(\"/home/wonguk/coding/paper_project/hojun/data/62570_갊.png\")\n",
    "transforms = torchvision.transforms.Compose([\n",
    "        # torchvision.transforms.Resize((input_size,input_size)),\n",
    "        torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "img = transforms(img)\n",
    "img_batch = torch.unsqueeze(img,0)\n",
    "img_batch = img_batch.repeat(8,1,1,1).to('cuda')\n",
    "\n",
    "t = torch.randn([8]).to('cuda')\n",
    "char = torch.randn([8,296]).to('cuda')\n",
    "\n",
    "tensorrt_path = '/home/wonguk/coding/paper_project/hojun/light_weight/tensorrt.plan'\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "trt.init_libnvinfer_plugins(None, \"\")\n",
    "with open(tensorrt_path, 'rb') as f:\n",
    "    engine_data = f.read()\n",
    "    engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "# CUDA 메모리 할당\n",
    "input_gpu1 = cuda.mem_alloc(img_batch.element_size() * img_batch.nelement())\n",
    "input_gpu2 = cuda.mem_alloc(t.element_size() * t.nelement())\n",
    "input_gpu3 = cuda.mem_alloc(char.element_size() * char.nelement())\n",
    "\n",
    "# CUDA 메모리에 데이터 복사\n",
    "cuda.memcpy_htod(input_gpu1, img_batch.cpu().numpy().tobytes())\n",
    "cuda.memcpy_htod(input_gpu2, t.cpu().numpy().tobytes())\n",
    "cuda.memcpy_htod(input_gpu3, char.cpu().numpy().tobytes())\n",
    "\n",
    "input_gpu1.free()\n",
    "input_gpu2.free()\n",
    "input_gpu3.free()\n",
    "\n",
    "bindings = [int(input_gpu1), int(input_gpu2), int(input_gpu3)]\n",
    "context.execute_v2(bindings=bindings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
