{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_Gray = Image.open('C:/Paper_Project/Hangul_Characters_Image128_Grayscale/가/62570_가.png')\n",
    "img_Gray_array = np.array(img_Gray)\n",
    "\n",
    "print(img_Gray_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\gih0109/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login 6473a3a8207ad0ecadea31cd0cbc7e6fb3ab0462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "img = cv2.imread('C:/Paper_Project/1.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# context와 x의 예시 shape\n",
    "context_shape = (16, 68, 256)\n",
    "x_shape = (16, 256, 16, 16)\n",
    "\n",
    "\n",
    "# 임의의 값으로 채워진 context와 x 텐서 생성\n",
    "context = torch.rand(context_shape)\n",
    "x = torch.rand(x_shape)\n",
    "\n",
    "# 예시로 nn.Linear를 사용하여 context를 변환\n",
    "# nn.Linear를 사용하여 context 텐서 변환\n",
    "linear_layer = nn.Linear(context_shape[-1], x_shape[1] * x_shape[2] * x_shape[3])\n",
    "context_transformed = linear_layer(context)\n",
    "\n",
    "# 변환된 context를 원하는 크기로 reshape\n",
    "desired_shape = (x_shape[0], x_shape[1], x_shape[2], x_shape[3])\n",
    "context_reshaped = context_transformed.view(desired_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    'vgg19cut': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'N'],\n",
    "}\n",
    "\n",
    "class StyleEncoder2(nn.Module):\n",
    "    def __init__(self, sty_dim=128, cfg=cfg['vgg11']):\n",
    "        super().__init__()\n",
    "        # network layers setting\n",
    "        self.features = self.make_layers(cfg, True)\n",
    "        self.cont = nn.Linear(512, sty_dim)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        flat = x.view(x.size(0), -1)\n",
    "        cont = self.cont(flat)\n",
    "\n",
    "        return cont\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def make_layers(self, cfg, batch_norm=False):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=False)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=False)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Korean_StrokeEmbedding:\n",
    "    def __init__(self,txt_path,classes):\n",
    "        self.emd_stroke_list = self.read_stroke_txt(txt_path)\n",
    "        self.classes = classes\n",
    "\n",
    "    def read_stroke_txt(self, txt_path):\n",
    "        emd_stroke_list = []\n",
    "        read_txt = open(txt_path, 'r')\n",
    "        for emd in read_txt:\n",
    "            emd_stroke_list.append(emd[:-1])\n",
    "        return emd_stroke_list\n",
    "\n",
    "    def embedding(self, indexs):\n",
    "        stroke_embedding = []\n",
    "        for index in indexs:\n",
    "            tmp = []\n",
    "            unicode_diff = ord(self.classes[index]) - 44032# ord('가')\n",
    "            for check in self.emd_stroke_list[unicode_diff]:\n",
    "                tmp.append(int(check))\n",
    "            stroke_embedding.append(tmp)\n",
    "        print(\"stroke list : \")\n",
    "        print(stroke_embedding)\n",
    "        return stroke_embedding\n",
    "\n",
    "class MakeCondition:\n",
    "    # 모든 contents 이름 바꾸기\n",
    "    def __init__(self, num_classes, stroke_text_path, style_enc_path, data_classes, language, device):\n",
    "        self.device = device\n",
    "        self.dataset_classes = data_classes\n",
    "        self.num_classes = num_classes\n",
    "        self.contents_dim = 60 \n",
    "        self.contents_emb = nn.Embedding(num_classes, self.contents_dim)\n",
    "        self.korean_stroke_emb = Korean_StrokeEmbedding(txt_path=stroke_text_path,classes=self.dataset_classes)\n",
    "        # self.style_enc = self.make_style_enc(style_enc_path)\n",
    "        self.style_enc = StyleEncoder2()\n",
    "        self.language = language\n",
    "\n",
    "        self.load_style_weight(style_enc_path)\n",
    "\n",
    "    # def make_style_enc(self,style_enc_path):\n",
    "    #     C ,C_in = 32, 1\n",
    "    #     sty_encoder = style_enc_builder(C_in, C)\n",
    "    #     checkpoint = torch.load(style_enc_path, map_location=self.device)\n",
    "    #     tmp_dict = {}\n",
    "    #     for k, v in checkpoint.items():\n",
    "    #         if k in sty_encoder.state_dict():\n",
    "    #             tmp_dict[k] = v\n",
    "    #     sty_encoder.load_state_dict(tmp_dict)\n",
    "    #     # frozen sty_encoder\n",
    "    #     for p in sty_encoder.parameters():\n",
    "    #         p.requires_grad = False\n",
    "    #     return sty_encoder.to(self.device)\n",
    "        \n",
    "    def load_style_weight(self, style_enc_path):\n",
    "        checkpoint = torch.load(style_enc_path, map_location=self.device)\n",
    "        tmp_dict = {}\n",
    "        for k, v in checkpoint.items():\n",
    "            if k in self.style_enc.state_dict():\n",
    "                tmp_dict[k] = v\n",
    "        self.style_enc.load_state_dict(tmp_dict)\n",
    "        # frozen sty_enc\n",
    "        for p in self.style_enc.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        self.style_enc.to(self.device)\n",
    "\n",
    "    def korean_index_to_uni_diff(self, indexs : list):\n",
    "        char_list = []\n",
    "        for index in indexs:\n",
    "            unicode_diff = ord(self.dataset_classes[index]) - 44032# ord('가')\n",
    "            char_list.append(unicode_diff)\n",
    "        return char_list\n",
    "\n",
    "    # def set_charAttr_dim(mode):\n",
    "    #     pass\n",
    "    def make_condition(self, images, indexs, mode):\n",
    "        input_length = images.shape[0]\n",
    "        # make channel 1 to 3 to input style enc (b, 1, h, w) -> (b, 3, h, w)\n",
    "        images = images.repeat(1, 3, 1, 1)\n",
    "        # contents_index = [int(content_index) for content_index in contents_index]\n",
    "        # style_encoder = style_enc_builder(1,32).to(self.device)\n",
    "        contents = None\n",
    "        stroke =  None\n",
    "        style = None\n",
    "        style_c = 128\n",
    "        style_h, style_w = 16, 16\n",
    "        \n",
    "        contents_p, stroke_p = random.random(), random.random()\n",
    "        if mode == 1:\n",
    "            if contents_p < 0.3:\n",
    "                contents = torch.zeros(input_length,self.contents_dim)\n",
    "            else:\n",
    "                uni_diff_list = torch.LongTensor(self.korean_index_to_uni_diff(indexs))\n",
    "                print(f\"uni diff list: {uni_diff_list}\")\n",
    "                contents = torch.FloatTensor(self.contents_emb(uni_diff_list))\n",
    "\n",
    "            if stroke_p < 0.3:\n",
    "                stroke = torch.zeros(input_length,68)\n",
    "            else:\n",
    "                stroke =  torch.FloatTensor(self.korean_stroke_emb.embedding(indexs))\n",
    "            \n",
    "            if contents_p < 0.3 and stroke_p < 0.3:\n",
    "                # style = torch.zeros(input_length,style_c, style_h , style_w)\n",
    "                style = torch.zeros(input_length, style_c)\n",
    "            else:\n",
    "                style = self.style_enc(images).cpu()\n",
    "                # style = style.view(input_length, style_c, -1).cpu()\n",
    "        elif mode == 2:\n",
    "            if contents_p < 0.3:\n",
    "                contents = torch.zeros(input_length,self.contents_dim)\n",
    "            else:\n",
    "                contents = torch.FloatTensor(self.korean_index_to_uni_diff(indexs))\n",
    "\n",
    "            if stroke_p < 0.3:\n",
    "                stroke = torch.zeros(input_length,68)\n",
    "            else:\n",
    "                stroke =  torch.FloatTensor(self.korean_stroke_emb.embedding(indexs))\n",
    "            \n",
    "            # style = torch.zeros(input_length,style_c, style_h , style_w)\n",
    "            style = torch.zeros(input_length, style_c)\n",
    "\n",
    "\n",
    "        elif mode == 3: #test\n",
    "            uni_diff_list = torch.LongTensor(self.korean_index_to_uni_diff(indexs))\n",
    "            contents = torch.FloatTensor(self.contents_emb(uni_diff_list))\n",
    "            stroke =  torch.FloatTensor(self.korean_stroke_emb.embedding(indexs))\n",
    "            style = self.style_enc(images).cpu()\n",
    "            \n",
    "            # style = style.view(input_length, style_c, -1).cpu()\n",
    "        condition_dict = {}\n",
    "        condition_dict[\"contents\"] = contents.to(self.device)\n",
    "        condition_dict[\"stroke\"] = stroke.to(self.device)\n",
    "        condition_dict['style'] = style.to(self.device)\n",
    "\n",
    "        return condition_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_num = 0\n",
    "image_size = 64\n",
    "input_size = 64\n",
    "batch_size = 12\n",
    "num_classes = 11172\n",
    "lr = 1e-4\n",
    "n_epochs = 402\n",
    "use_amp = True\n",
    "resume_train = True\n",
    "train_dirs = \"H:/data/Hangul_Characters_Image64_radomSampling420_GrayScale\"\n",
    "sample_img_path = f'{train_dirs}/갊/62570_갊.png'\n",
    "stroke_text_path = \"D:/workspace2/KoFont-Diffusion/Ko_diffusion/text_weight/storke_txt.txt\"\n",
    "style_enc_path = \"D:/workspace2/KoFont-Diffusion/Ko_diffusion/text_weight/korean_styenc.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set transform\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    # torchvision.transforms.Resize((input_size,input_size)),\n",
    "    torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "# # #     torchvision.transforms.RandomResizedCrop(input_size, scale=(0.8, 1.0)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5), (0.5))\n",
    "])\n",
    "dataset = torchvision.datasets.ImageFolder(train_dirs,transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len :  range(0, 99122, 10)\n"
     ]
    }
   ],
   "source": [
    "n = range(0,len(dataset),10)\n",
    "print(\"len : \",n)\n",
    "dataset = Subset(dataset, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_condition = MakeCondition(num_classes=num_classes,\n",
    "                                    stroke_text_path=stroke_text_path,\n",
    "                                    style_enc_path=style_enc_path,\n",
    "                                    data_classes=dataset.dataset.classes,\n",
    "                                    language='korean',\n",
    "                                    device=\"cpu\"\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "img, content = next(iter(dataloader))\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uni diff list: tensor([10886])\n",
      "stroke list : \n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "condition_dict = make_condition.make_condition(images=img, indexs=content, mode=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 60])\n"
     ]
    }
   ],
   "source": [
    "print(condition_dict[\"contents\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(condition_dict[\"contents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 68])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition_dict[\"stroke\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stroke = condition_dict[\"stroke\"]\n",
    "stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected TensorOptions(dtype=__int64, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)) (got TensorOptions(dtype=float, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stroke \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(stroke)\n\u001b[0;32m      2\u001b[0m stroke\n",
      "\u001b[1;31mTypeError\u001b[0m: expected TensorOptions(dtype=__int64, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)) (got TensorOptions(dtype=float, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))"
     ]
    }
   ],
   "source": [
    "stroke = torch.LongTensor(stroke)\n",
    "stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_test_layer = nn.Embedding(68, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "test_tensor = torch.LongTensor(test_list)\n",
    "print(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_emb = emb_test_layer(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([68, 128])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_emb.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
