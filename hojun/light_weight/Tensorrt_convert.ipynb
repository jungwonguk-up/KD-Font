{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonguk/anaconda3/envs/onnx/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import tensorrt # python3 -m pip install --upgrade tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"..\")\n",
    "from models.utils import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet().to('cuda')\n",
    "#load weight\n",
    "model.load_state_dict(torch.load('/home/wonguk/coding/paper_project/hojun/light_weight/wieghts/ckpt_290.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAACTklEQVR4nO2WTUhUURTH/6/n4AxGY04iKJhZMRu/FooEaRChREpQi8CWunDTpmWrMEhBcJMLCyFt0Vp058IwDAShcRiXStQiQnQyXNggzmnRuc9777t3Pnx9wpzVeeec+7vnnPvxrkMIJqcCji8BSoATAY7ePWpVLWSXzelPqiH9+n5MH5IL8AaIP1jY56+NsS7XMKfjHabtV3q6W1MAELrS09cKpO8se0mbS8i2WAvvIiJKmqs+bqIzYgWcBYDLZp+0Crc7bIBzAODkBeCJDWAvDnDkjnRnelzp81malUQbgO8RYbc0kYhW9+WvDyLpxiwR0YE2dd3PqDLZ1qlEzIqZhi3l+zNQZDfKEdV7lCMD+1l4/I2Vp1FrDIAym2NukpW+Qd11CwCvrb2EdTHtpa9sOS5BCbQA3orxFz4KU1GAmTDHNnvjiwFsD4jQu9LGKBiQmaziwOhL2e4BQjkB6Yl6jgs/3FE8HuC8HfD5Rb8ovmX8i4b2AMOKWRym7FIqlUgSADgNHddvXNQX3ztM8eUa2SwAB+0IRSpisdqGxqYzvsEAcNgPIFRz9V5EMTtBHxjKVk6u+fxDmDbYZJEbsuGfgAwZ5ljG3qCAxaAAai4aoK1C5pCVmysczNe5uweg8ohtkmgXSnk5Ky40Oe3PBMAveB+oBV0zuDkjIiLT3/n/e+L4xHytJ4IC2goH/KYSNj0tWs1KamfrfdYUm2cfDOY7C390GeOmd0LBALfzecoUrDZxdFf312MBAMJV8Qpg3tDFwLfy3z8LJcC/APgBWmMFmMlE0cMAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=64x64>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"/home/wonguk/coding/paper_project/hojun/data/62570_갊.png\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "transforms = torchvision.transforms.Compose([\n",
    "        # torchvision.transforms.Resize((input_size,input_size)),\n",
    "        torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "img = transforms(img)\n",
    "img.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 64, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_batch = torch.unsqueeze(img,0)\n",
    "img_batch = img_batch.repeat(8,1,1,1).to('cuda')\n",
    "img_batch.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t : torch.Size([8])\n",
    "# char : torch.Size([8, 296])\n",
    "img_batch = torch.randn([1,1,64,64]).to('cuda')\n",
    "t = torch.randn([1]).to('cuda')\n",
    "char = torch.randn([1,296]).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonguk/anaconda3/envs/onnx/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/wonguk/anaconda3/envs/onnx/lib/python3.8/site-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/wonguk/anaconda3/envs/onnx/lib/python3.8/site-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "# convert pytorch to onnx\n",
    "onnx_filename = \"onnx_model.onnx\"\n",
    "input_names = [\"x\", \"t\", \"char\"]\n",
    "output_names = [\"output\"]\n",
    "\n",
    "torch.onnx.export(model,\n",
    "                  (img_batch,t,char),\n",
    "                  onnx_filename,\n",
    "                  export_params = True,\n",
    "                  opset_version = 14, # pytorch 버전 이슈 발생 -> conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia // python  3.10.13\n",
    "                  do_constant_folding = True,\n",
    "                  input_names = input_names,\n",
    "                  output_names = output_names,\n",
    "                  dynamic_axes = {'x' : {0 : 'batch_size'},\n",
    "                                  't' : {0 : 'batch_size'},\n",
    "                                  'char' : {0 : 'batch_size'}\n",
    "                                })\n",
    "\n",
    "# torch.onnx.export(model,\n",
    "#                   (img_batch,t,char),\n",
    "#                   onnx_filename,\n",
    "#                   export_params = True,\n",
    "#                   opset_version = 14, # pytorch 버전 이슈 발생 -> conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia // python  3.10.13\n",
    "#                   do_constant_folding = True,\n",
    "#                   input_names = input_names,\n",
    "#                   output_names = output_names,\n",
    "#                   dynamic_axes = {'x' : {0 : 'batch_size'},\n",
    "#                                   't' : {0 : 'batch_size'},\n",
    "#                                   'char' : {0 : 'batch_size'},\n",
    "#                                 'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# ONNX 모델 불러오기\n",
    "onnx_model = onnx.load('/home/wonguk/coding/paper_project/hojun/light_weight/onnx_model.onnx')\n",
    "ort_session = ort.InferenceSession('/home/wonguk/coding/paper_project/hojun/light_weight/onnx_model.onnx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 296])\n",
      "<built-in method type of Tensor object at 0x7f36c2291b80>\n"
     ]
    }
   ],
   "source": [
    "print(img_batch.shape)\n",
    "print(t.shape)\n",
    "print(char.shape)\n",
    "print(char.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[[-0.10639653,  0.04439037,  0.10102789, ...,  0.12326263,\n",
      "          -0.02602465, -0.16364805],\n",
      "         [ 0.07116662, -0.05104019, -0.15301642, ..., -0.09398307,\n",
      "          -0.03608431, -0.03658168],\n",
      "         [ 0.12347434, -0.08546165, -0.14521451, ..., -0.09615244,\n",
      "          -0.08789815, -0.05091392],\n",
      "         ...,\n",
      "         [ 0.13512728, -0.04711135, -0.14415167, ..., -0.0985157 ,\n",
      "          -0.09971239, -0.03730395],\n",
      "         [ 0.18343157, -0.01006906, -0.14700025, ..., -0.1208332 ,\n",
      "          -0.07512759, -0.13458695],\n",
      "         [ 0.03641973,  0.02900542, -0.03871416, ..., -0.03445272,\n",
      "          -0.13498864, -0.35216713]]],\n",
      "\n",
      "\n",
      "       [[[-0.17673534,  0.19001903,  0.26843455, ...,  0.29452294,\n",
      "           0.1588127 ,  0.16982128],\n",
      "         [ 0.07772186,  0.02625325, -0.03114012, ..., -0.03193667,\n",
      "           0.0700267 ,  0.11363234],\n",
      "         [ 0.265553  ,  0.04567422,  0.06596021, ...,  0.06332278,\n",
      "           0.10093769,  0.19272122],\n",
      "         ...,\n",
      "         [ 0.30636713,  0.04473436,  0.00996856, ...,  0.02813033,\n",
      "           0.0533925 ,  0.18995488],\n",
      "         [ 0.35001478,  0.11560516,  0.03019991, ...,  0.01197914,\n",
      "           0.05189725,  0.06913581],\n",
      "         [ 0.19292447,  0.13121156,  0.12522967, ...,  0.11412045,\n",
      "           0.033762  , -0.00922722]]],\n",
      "\n",
      "\n",
      "       [[[-0.26350465,  0.1173646 ,  0.18327866, ...,  0.19886965,\n",
      "           0.0577128 , -0.18414646],\n",
      "         [ 0.00846053, -0.05948467, -0.18132173, ..., -0.12184459,\n",
      "          -0.01070313, -0.01011789],\n",
      "         [ 0.17187893, -0.047071  , -0.08767214, ..., -0.05759317,\n",
      "           0.01884371,  0.05358484],\n",
      "         ...,\n",
      "         [ 0.22098653, -0.01177733, -0.06240494, ..., -0.00656779,\n",
      "           0.02383969,  0.0923304 ],\n",
      "         [ 0.27246073,  0.04688714, -0.06702852, ..., -0.03118278,\n",
      "           0.03607404,  0.00426024],\n",
      "         [ 0.26850083,  0.11396128,  0.07439514, ...,  0.1172905 ,\n",
      "           0.01774707, -0.1274591 ]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[-0.22370294,  0.08115153,  0.240171  , ...,  0.0998538 ,\n",
      "          -0.22402838, -0.4075088 ],\n",
      "         [ 0.00864507, -0.23581876, -0.2889884 , ..., -0.2757663 ,\n",
      "          -0.27796116, -0.20924345],\n",
      "         [ 0.27545175, -0.15003003, -0.13186795, ..., -0.13471532,\n",
      "          -0.13547474, -0.017795  ],\n",
      "         ...,\n",
      "         [ 0.34560463, -0.14451042, -0.16451953, ..., -0.13828026,\n",
      "          -0.15788212,  0.07675026],\n",
      "         [ 0.28474173, -0.08838344, -0.13670357, ..., -0.1697921 ,\n",
      "          -0.1002332 , -0.0175832 ],\n",
      "         [ 0.33574158,  0.02811357,  0.11390388, ...,  0.142404  ,\n",
      "          -0.0479534 , -0.35590577]]],\n",
      "\n",
      "\n",
      "       [[[-0.17931   ,  0.06045886,  0.15811917, ...,  0.10570283,\n",
      "          -0.03687402, -0.05598943],\n",
      "         [ 0.0709067 , -0.12158956, -0.20464292, ..., -0.16972326,\n",
      "          -0.08209759, -0.01669516],\n",
      "         [ 0.18334469, -0.11172058, -0.11211777, ..., -0.10035782,\n",
      "          -0.03827861,  0.03278534],\n",
      "         ...,\n",
      "         [ 0.24785797, -0.03847932, -0.07036415, ..., -0.03656969,\n",
      "           0.01154916,  0.10004839],\n",
      "         [ 0.30357918,  0.02377898, -0.04556512, ..., -0.02022019,\n",
      "           0.07653263,  0.07781853],\n",
      "         [ 0.1766727 ,  0.07657148,  0.08495915, ...,  0.06463949,\n",
      "           0.03566318, -0.00076694]]],\n",
      "\n",
      "\n",
      "       [[[-0.34842423, -0.05695353,  0.01181571, ...,  0.00599756,\n",
      "          -0.22172154, -0.03626215],\n",
      "         [-0.07439448, -0.17871349, -0.2107803 , ..., -0.13840312,\n",
      "          -0.12190877,  0.0037426 ],\n",
      "         [ 0.13027109, -0.09328993, -0.06145567, ..., -0.00768027,\n",
      "          -0.01892368,  0.05024207],\n",
      "         ...,\n",
      "         [ 0.20985723, -0.05037931, -0.11088875, ..., -0.06392203,\n",
      "          -0.07829921,  0.13988182],\n",
      "         [ 0.24602358,  0.01436507, -0.00639521, ...,  0.0333655 ,\n",
      "           0.0458577 ,  0.14244178],\n",
      "         [ 0.59755594,  0.11934248,  0.11570272, ...,  0.14014715,\n",
      "           0.0784158 ,  0.25416303]]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_data = {\n",
    "    \"x\" : img_batch.cpu().numpy(),\n",
    "    \"t\" : t.cpu().numpy(),\n",
    "    \"char\" : char.cpu().numpy()\n",
    "}\n",
    "output = ort_session.run(None, input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'graph' from 'onnx.tools' (/home/wonguk/anaconda3/envs/onnx/lib/python3.8/site-packages/onnx/tools/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/wonguk/coding/paper_project/hojun/light_weight/test.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/wonguk/coding/paper_project/hojun/light_weight/test.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39monnx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m graph\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wonguk/coding/paper_project/hojun/light_weight/test.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# onnx graph 추출\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wonguk/coding/paper_project/hojun/light_weight/test.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m graph_def \u001b[39m=\u001b[39m onnx_model\u001b[39m.\u001b[39mgraph\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'graph' from 'onnx.tools' (/home/wonguk/anaconda3/envs/onnx/lib/python3.8/site-packages/onnx/tools/__init__.py)"
     ]
    }
   ],
   "source": [
    "from onnx.tools import graph\n",
    "\n",
    "# onnx graph 추출\n",
    "graph_def = onnx_model.graph\n",
    "\n",
    "# 그래프 시각화\n",
    "graph.plot_graph(graph_def, node_names = True) # 이유는 모르지만 시각화가 안됨\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'build_engine',\n",
       " 'build_serialized_network',\n",
       " 'create_builder_config',\n",
       " 'create_network',\n",
       " 'create_optimization_profile',\n",
       " 'error_recorder',\n",
       " 'gpu_allocator',\n",
       " 'is_network_supported',\n",
       " 'logger',\n",
       " 'max_DLA_batch_size',\n",
       " 'max_batch_size',\n",
       " 'max_threads',\n",
       " 'num_DLA_cores',\n",
       " 'platform_has_fast_fp16',\n",
       " 'platform_has_fast_int8',\n",
       " 'platform_has_tf32',\n",
       " 'reset']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorrt.BuilderFlag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35085/1737570757.py:22: DeprecationWarning: Use build_serialized_network instead.\n",
      "  engine = builder.build_engine(network=network,config=builder_config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/05/2023-01:10:24] [TRT] [W] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.4.0\n",
      "[10/05/2023-01:10:45] [TRT] [W] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.4.0\n",
      "[10/05/2023-01:10:45] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[10/05/2023-01:10:45] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n"
     ]
    }
   ],
   "source": [
    "import tensorrt \n",
    " \n",
    "onnx_file_name = '/home/wonguk/coding/paper_project/hojun/light_weight/onnx_model.onnx'\n",
    "tensorrt_file_name = 'tensorrt.plan'\n",
    "fp_16_mode = True\n",
    "TRT_LOGGER = tensorrt.Logger(tensorrt.Logger.WARNING)\n",
    "EXPLICIT_BATCH = 1 << (int)(tensorrt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    " \n",
    "builder = tensorrt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(EXPLICIT_BATCH)\n",
    "parser = tensorrt.OnnxParser(network, TRT_LOGGER)\n",
    "builder_config = builder.create_builder_config()\n",
    "# builder_config.max_workspace_size = (1<<30)\n",
    "# builder_config.set_memory_pool_limit =(1<<30)\n",
    "# builder.fp16_mode = fp16_mode\n",
    " \n",
    "with open(onnx_file_name, 'rb') as model:\n",
    "    if not parser.parse(model.read()):\n",
    "        for error in range(parser.num_errors):\n",
    "            print (parser.get_error(error))\n",
    " \n",
    "engine = builder.build_engine(network=network,config=builder_config)\n",
    "buf = engine.serialize()\n",
    "with open(tensorrt_file_name, 'wb') as f:\n",
    "    f.write(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import wandb\n",
    "import torch, torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from modules.diffusion import Diffusion\n",
    "from modules.utils import plot_images, test_save_images,make_stroke,stroke_to_char\n",
    "from models.utils import UNet\n",
    "from modules.utils import CharAttar\n",
    "batch_size = 8 #####\n",
    "sampleImage_len = 36\n",
    "\n",
    "\n",
    "num_classes = 420\n",
    "input_length = 100\n",
    "contents_dim = 100\n",
    "input_size = 64\n",
    "mode = \"new\"\n",
    "folder_name =\"test_3\"\n",
    "\n",
    "train_dirs = '/home/wonguk/coding/paper_project/hojun/data/Hangul_Characters_Image64_radomSampling420_GrayScale'\n",
    "sample_img_path = '/home/wonguk/coding/paper_project/hojun/data/62570_갊.png'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wandb.init(project=\"onnx_sampling\", config={\n",
    "                \"learning_rate\": 0.0003,\n",
    "                \"architecture\": \"UNET\",\n",
    "                \"dataset\": \"HOJUN_KOREAN_FONT64\",\n",
    "                \"notes\":\"content, yes_stoke, non_style/ 64 x 64, 420 dataset\"\n",
    "                },\n",
    "            name = \"self-attetnion onnx_sampling 나눔손글씨강인한위로_갊\") #####\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(0)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # model = UNet().to(device)\n",
    "    # ckpt = torch.load(\"/home/hojun/Documents/code/Kofont5/KoFont-Diffusion2/hojun/results/models/font_noStrokeStyle_Unet64_image420_3/ckpt_290.pt\")\n",
    "    # model.load_state_dict(ckpt)\n",
    "\n",
    "    diffusion = Diffusion(first_beta=1e-4,\n",
    "                              end_beta=0.02,\n",
    "                              noise_step=1000,\n",
    "                              beta_schedule_type='linear',\n",
    "                              img_size=input_size,\n",
    "                              device=device)\n",
    "    \n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        # torchvision.transforms.Resize((input_size,input_size)),\n",
    "        torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    dataset = torchvision.datasets.ImageFolder(train_dirs,transform=transforms)\n",
    "\n",
    "    # test set\n",
    "    n = range(0,len(dataset),100)\n",
    "    dataset = Subset(dataset, n)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,num_workers=12)\n",
    "\n",
    "    #sample_img\n",
    "    sample_img = Image.open(sample_img_path)\n",
    "    sample_img = transforms(sample_img).to(device)\n",
    "    sample_img = torch.unsqueeze(sample_img,1)\n",
    "    sample_img = sample_img.repeat(sampleImage_len, 1, 1, 1)\n",
    "    \n",
    "    if mode == \"random\":\n",
    "        contents_emb = torch.zeros(input_length,contents_dim)\n",
    "\n",
    "        first= [random.randint(0,18) for _ in range(input_length)]\n",
    "        middle = [random.randint(19,39) for _ in range(input_length)]\n",
    "        last = [random.randint(40,67) for _ in range(input_length)]\n",
    "\n",
    "        strokes = torch.Tensor([[0 for _ in range(68)] for _ in range(input_length)])\n",
    "\n",
    "        for idx in range(input_length):\n",
    "            strokes[idx][first[idx]], strokes[idx][middle[idx]], strokes[idx][last[idx]] = 1, 1, 1\n",
    "        char_list = stroke_to_char(strokes)\n",
    "\n",
    "        style_emb = torch.zeros(input_length,12288)\n",
    "\n",
    "        y = torch.cat([contents_emb, strokes, style_emb], dim=1).to(device)\n",
    "        x = diffusion.test_sampling(model, input_length, y, cfg_scale=3)\n",
    "\n",
    "    elif mode == \"manual\":\n",
    "        char_list = ['가,나,다,라,마,바,사,아,자,차,카,타,파,하']\n",
    "        contents_emb = torch.zeros(input_length, contents_dim)\n",
    "        strokes = make_stroke(char_list)\n",
    "        style_emb = torch.zeros(input_length, 12288)\n",
    "        y = torch.cat([contents_emb, strokes, style_emb], dim=1).to(device)\n",
    "        x = diffusion.test_sampling(model,len(strokes), y, cfg_scale=3)\n",
    "        \n",
    "    elif mode == \"new\":\n",
    "        charAttar = CharAttar(num_classes=num_classes,device=device)\n",
    "        sampled_images = diffusion.portion_sampling(model, n=len(dataset.dataset.classes),sampleImage_len = sampleImage_len,dataset=dataset,mode =mode,charAttar=charAttar,sample_img=sample_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorrt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
