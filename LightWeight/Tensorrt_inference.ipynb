{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAACTklEQVR4nO2WTUhUURTH/6/n4AxGY04iKJhZMRu/FooEaRChREpQi8CWunDTpmWrMEhBcJMLCyFt0Vp058IwDAShcRiXStQiQnQyXNggzmnRuc9777t3Pnx9wpzVeeec+7vnnPvxrkMIJqcCji8BSoATAY7ePWpVLWSXzelPqiH9+n5MH5IL8AaIP1jY56+NsS7XMKfjHabtV3q6W1MAELrS09cKpO8se0mbS8i2WAvvIiJKmqs+bqIzYgWcBYDLZp+0Crc7bIBzAODkBeCJDWAvDnDkjnRnelzp81malUQbgO8RYbc0kYhW9+WvDyLpxiwR0YE2dd3PqDLZ1qlEzIqZhi3l+zNQZDfKEdV7lCMD+1l4/I2Vp1FrDIAym2NukpW+Qd11CwCvrb2EdTHtpa9sOS5BCbQA3orxFz4KU1GAmTDHNnvjiwFsD4jQu9LGKBiQmaziwOhL2e4BQjkB6Yl6jgs/3FE8HuC8HfD5Rb8ovmX8i4b2AMOKWRym7FIqlUgSADgNHddvXNQX3ztM8eUa2SwAB+0IRSpisdqGxqYzvsEAcNgPIFRz9V5EMTtBHxjKVk6u+fxDmDbYZJEbsuGfgAwZ5ljG3qCAxaAAai4aoK1C5pCVmysczNe5uweg8ohtkmgXSnk5Ky40Oe3PBMAveB+oBV0zuDkjIiLT3/n/e+L4xHytJ4IC2goH/KYSNj0tWs1KamfrfdYUm2cfDOY7C390GeOmd0LBALfzecoUrDZxdFf312MBAMJV8Qpg3tDFwLfy3z8LJcC/APgBWmMFmMlE0cMAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=64x64>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"/home/wonguk/coding/paper_project/hojun/data/62570_갊.png\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "transforms = torchvision.transforms.Compose([\n",
    "        # torchvision.transforms.Resize((input_size,input_size)),\n",
    "        torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "img = transforms(img)\n",
    "img.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 64, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "img_batch = torch.unsqueeze(img,0)\n",
    "img_batch = img_batch.repeat(8,1,1,1).to('cuda')\n",
    "img_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t : torch.Size([8])\n",
    "# char : torch.Size([8, 296])\n",
    "\n",
    "t = torch.randn([8]).to('cuda')\n",
    "char = torch.randn([8,296]).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    " \n",
    "tensorrt_path = '/home/wonguk/coding/paper_project/hojun/light_weight/tensorrt.plan'\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "trt.init_libnvinfer_plugins(None, \"\")\n",
    "with open(tensorrt_path, 'rb') as f:\n",
    "    engine_data = f.read()\n",
    "    engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "device = cuda.Device(0)\n",
    "ctx = device.make_context()\n",
    "\n",
    "\n",
    "# PyTorch 텐서를 NumPy 배열로 변환\n",
    "img_batch_np = img_batch.cpu().numpy()\n",
    "t_np = t.cpu().numpy()\n",
    "char_np = char.cpu().numpy()\n",
    "\n",
    "input_gpu1 = cuda.mem_alloc(img_batch_np.nbytes)\n",
    "input_gpu2 = cuda.mem_alloc(t_np.nbytes)\n",
    "input_gpu3 = cuda.mem_alloc(char_np.nbytes)\n",
    "\n",
    "cuda.memcpy_htod(input_gpu1, img_batch_np)\n",
    "cuda.memcpy_htod(input_gpu2, t_np )\n",
    "cuda.memcpy_htod(input_gpu3, char_np)\n",
    "\n",
    "input_gpu1.free()\n",
    "input_gpu2.free()\n",
    "input_gpu3.free()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/11/2023-00:00:36] [TRT] [E] 1: [slice.cu::launchNaiveSliceImpl::245] Error Code 1: Cuda Runtime (invalid resource handle)\n"
     ]
    }
   ],
   "source": [
    "bindings = [int(input_gpu1), int(input_gpu2), int(input_gpu3)]\n",
    "\n",
    "with engine.create_execution_context() as context:\n",
    "    context.execute_v2(bindings=bindings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/2023-00:41:15] [TRT] [E] 1: [convolutionRunner.cpp::executeConv::465] Error Code 1: Cudnn (CUDNN_STATUS_BAD_PARAM)\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit # 이걸로 gpu 초기화를 진행했는데 추가 에러 발생\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "img = Image.open(\"/home/wonguk/coding/paper_project/hojun/data/62570_갊.png\")\n",
    "transforms = torchvision.transforms.Compose([\n",
    "        # torchvision.transforms.Resize((input_size,input_size)),\n",
    "        torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "img = transforms(img)\n",
    "img_batch = torch.unsqueeze(img,0)\n",
    "img_batch = img_batch.repeat(8,1,1,1).to('cuda')\n",
    "\n",
    "t = torch.randn([8]).to('cuda')\n",
    "char = torch.randn([8,296]).to('cuda')\n",
    "\n",
    "tensorrt_path = '/home/wonguk/coding/paper_project/hojun/light_weight/tensorrt.plan'\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "trt.init_libnvinfer_plugins(None, \"\")\n",
    "with open(tensorrt_path, 'rb') as f:\n",
    "    engine_data = f.read()\n",
    "    engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "# CUDA 메모리 할당\n",
    "input_gpu1 = cuda.mem_alloc(img_batch.element_size() * img_batch.nelement())\n",
    "input_gpu2 = cuda.mem_alloc(t.element_size() * t.nelement())\n",
    "input_gpu3 = cuda.mem_alloc(char.element_size() * char.nelement())\n",
    "\n",
    "# CUDA 메모리에 데이터 복사\n",
    "cuda.memcpy_htod(input_gpu1, img_batch.cpu().numpy().tobytes())\n",
    "cuda.memcpy_htod(input_gpu2, t.cpu().numpy().tobytes())\n",
    "cuda.memcpy_htod(input_gpu3, char.cpu().numpy().tobytes())\n",
    "\n",
    "input_gpu1.free()\n",
    "input_gpu2.free()\n",
    "input_gpu3.free()\n",
    "\n",
    "bindings = [int(input_gpu1), int(input_gpu2), int(input_gpu3)]\n",
    "context.execute_v2(bindings=bindings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libGL.so.1: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/root/paper_project/LightWeight/Tensorrt_inference.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737573706963696f75735f68656973656e62657267227d/root/paper_project/LightWeight/Tensorrt_inference.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737573706963696f75735f68656973656e62657267227d/root/paper_project/LightWeight/Tensorrt_inference.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737573706963696f75735f68656973656e62657267227d/root/paper_project/LightWeight/Tensorrt_inference.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737573706963696f75735f68656973656e62657267227d/root/paper_project/LightWeight/Tensorrt_inference.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorrt\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtrt\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cv2/__init__.py:181\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[39mif\u001b[39;00m DEBUG: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mExtra Python code for\u001b[39m\u001b[39m\"\u001b[39m, submodule, \u001b[39m\"\u001b[39m\u001b[39mis loaded\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m DEBUG: \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mOpenCV loader: DONE\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m bootstrap()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cv2/__init__.py:153\u001b[0m, in \u001b[0;36mbootstrap\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mif\u001b[39;00m DEBUG: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRelink everything from native cv2 module to cv2 package\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m py_module \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcv2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m native_module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39mcv2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    155\u001b[0m sys\u001b[39m.\u001b[39mmodules[\u001b[39m\"\u001b[39m\u001b[39mcv2\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m py_module\n\u001b[1;32m    156\u001b[0m \u001b[39msetattr\u001b[39m(py_module, \u001b[39m\"\u001b[39m\u001b[39m_native\u001b[39m\u001b[39m\"\u001b[39m, native_module)\n",
      "File \u001b[0;32m/usr/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "\u001b[0;31mImportError\u001b[0m: libGL.so.1: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "class ImageBatchStream:\n",
    "    def __init__(self, batch_size, image_shape, image_dir):\n",
    "        self.batch_size = batch_size\n",
    "        self.image_shape = image_shape\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = os.listdir(self.image_dir)\n",
    "        self.max_batches = len(self.image_files) // self.batch_size\n",
    "        self.batch = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        if self.batch < self.max_batches:\n",
    "            start = self.batch * self.batch_size\n",
    "            end = min(start + self.batch_size, len(self.image_files))\n",
    "            image_batch = np.zeros((end-start,) + self.image_shape, dtype=np.float32)\n",
    "            for i in range(start, end):\n",
    "                img = cv2.imread(os.path.join(self.image_dir, self.image_files[i]), cv2.IMREAD_GRAYSCALE)\n",
    "                img = cv2.resize(img, self.image_shape[1:])\n",
    "                img = img.astype(np.float32) / 255.\n",
    "                img = np.expand_dims(img, axis=0)\n",
    "                image_batch[i-start] = img\n",
    "            self.batch += 1\n",
    "            return image_batch\n",
    "        else:\n",
    "            self.batch = 0\n",
    "            return None\n",
    "\n",
    "    def reset(self):\n",
    "        self.batch = 0\n",
    "\n",
    "class PythonEntropyCalibrator(trt.IInt8EntropyCalibrator2):\n",
    "    def __init__(self, stream):\n",
    "        trt.IInt8EntropyCalibrator2.__init__(self)\n",
    "\n",
    "        self.stream = stream\n",
    "        self.d_input = cuda.mem_alloc(self.stream.next_batch().nbytes)\n",
    "        self.stream.reset()\n",
    "\n",
    "    def get_batch_size(self):\n",
    "        return self.stream.batch_size\n",
    "\n",
    "    def get_batch(self, names, p_str=None):\n",
    "        batch = self.stream.next_batch()\n",
    "        if batch is not None:\n",
    "            cuda.memcpy_htod(self.d_input, batch)\n",
    "            return [int(self.d_input)]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def read_calibration_cache(self, length):\n",
    "        return None\n",
    "\n",
    "    def write_calibration_cache(self, ptr, size):\n",
    "        cache = ctypes.c_char_p(int(ptr))\n",
    "        with open('calibration_cache.bin', 'wb') as cache_file:\n",
    "            cache_file.write(cache.value)\n",
    "        return None\n",
    "\n",
    "def build_int8_engine(onnx_path, calibrator):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network(1) as network, trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "        builder.max_workspace_size = 1 << 28  # 256MiB\n",
    "        builder.max_batch_size = calibrator.get_batch_size()\n",
    "        builder.int8_mode = True\n",
    "        builder.int8_calibrator = calibrator\n",
    "\n",
    "        with open(onnx_path, 'rb') as model:\n",
    "            if not parser.parse(model.read()):\n",
    "                print('ERROR: Failed to parse the ONNX file.')\n",
    "                for error in range(parser.num_errors):\n",
    "                    print(parser.get_error(error))\n",
    "                return None\n",
    "\n",
    "        engine = builder.build_cuda_engine(network)\n",
    "        return engine\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
