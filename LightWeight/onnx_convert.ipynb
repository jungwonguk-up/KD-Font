{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "print(ort.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import tensorrt # python3 -m pip install --upgrade tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"..\")\n",
    "from utils import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet().to('cuda')\n",
    "#load weight\n",
    "model.load_state_dict(torch.load('/root/paper_project/LightWeight/ckpt_390.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAACTklEQVR4nO2WTUhUURTH/6/n4AxGY04iKJhZMRu/FooEaRChREpQi8CWunDTpmWrMEhBcJMLCyFt0Vp058IwDAShcRiXStQiQnQyXNggzmnRuc9777t3Pnx9wpzVeeec+7vnnPvxrkMIJqcCji8BSoATAY7ePWpVLWSXzelPqiH9+n5MH5IL8AaIP1jY56+NsS7XMKfjHabtV3q6W1MAELrS09cKpO8se0mbS8i2WAvvIiJKmqs+bqIzYgWcBYDLZp+0Crc7bIBzAODkBeCJDWAvDnDkjnRnelzp81malUQbgO8RYbc0kYhW9+WvDyLpxiwR0YE2dd3PqDLZ1qlEzIqZhi3l+zNQZDfKEdV7lCMD+1l4/I2Vp1FrDIAym2NukpW+Qd11CwCvrb2EdTHtpa9sOS5BCbQA3orxFz4KU1GAmTDHNnvjiwFsD4jQu9LGKBiQmaziwOhL2e4BQjkB6Yl6jgs/3FE8HuC8HfD5Rb8ovmX8i4b2AMOKWRym7FIqlUgSADgNHddvXNQX3ztM8eUa2SwAB+0IRSpisdqGxqYzvsEAcNgPIFRz9V5EMTtBHxjKVk6u+fxDmDbYZJEbsuGfgAwZ5ljG3qCAxaAAai4aoK1C5pCVmysczNe5uweg8ohtkmgXSnk5Ky40Oe3PBMAveB+oBV0zuDkjIiLT3/n/e+L4xHytJ4IC2goH/KYSNj0tWs1KamfrfdYUm2cfDOY7C390GeOmd0LBALfzecoUrDZxdFf312MBAMJV8Qpg3tDFwLfy3z8LJcC/APgBWmMFmMlE0cMAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=64x64>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"/root/paper_project/ML/sample_img/62570_갊.png\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "transforms = torchvision.transforms.Compose([\n",
    "        # torchvision.transforms.Resize((input_size,input_size)),\n",
    "        torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "img = transforms(img)\n",
    "img.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 64, 64])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_batch = torch.unsqueeze(img,0)\n",
    "img_batch = img_batch.repeat(8,1,1,1).to('cuda')\n",
    "img_batch.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t : torch.Size([8])\n",
    "# char : torch.Size([8, 296])\n",
    "img_batch = torch.randn([1,1,64,64]).to('cuda')\n",
    "t = torch.randn([1]).to('cuda')\n",
    "char = torch.randn([1,296]).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:2359: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  _verify_batch_size([input.size(0) * input.size(1) // num_groups, num_groups] + list(input.size()[2:]))\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/symbolic_helper.py:716: UserWarning: allowzero=0 by default. In order to honor zero value in shape use allowzero=1\n",
      "  warnings.warn(\"allowzero=0 by default. In order to honor zero value in shape use allowzero=1\")\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "# convert pytorch to onnx\n",
    "onnx_filename = \"onnx_model.onnx\"\n",
    "input_names = [\"x\", \"t\", \"char\"]\n",
    "output_names = [\"output\"]\n",
    "\n",
    "torch.onnx.export(model,\n",
    "                  (img_batch,t,char),\n",
    "                  onnx_filename,\n",
    "                  export_params = True,\n",
    "                  opset_version = 14, # pytorch 버전 이슈 발생 -> conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia // python  3.10.13\n",
    "                  do_constant_folding = True,\n",
    "                  input_names = input_names,\n",
    "                  output_names = output_names,\n",
    "                  dynamic_axes = {'x' : {0 : 'batch_size'},\n",
    "                                  't' : {0 : 'batch_size'},\n",
    "                                  'char' : {0 : 'batch_size'}\n",
    "                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[MatMul_251]\n",
      "Ignore MatMul due to non constant B: /[MatMul_253]\n",
      "Ignore MatMul due to non constant B: /[MatMul_503]\n",
      "Ignore MatMul due to non constant B: /[MatMul_505]\n",
      "Ignore MatMul due to non constant B: /[MatMul_755]\n",
      "Ignore MatMul due to non constant B: /[MatMul_757]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1092]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1094]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1345]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1347]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1598]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1600]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = '/root/paper_project/LightWeight/onnx_model.onnx'\n",
    "model_quant = '/root/paper_project/LightWeight/onnx_model.quant.onnx'\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[MatMul_251]\n",
      "Ignore MatMul due to non constant B: /[MatMul_253]\n",
      "Ignore MatMul due to non constant B: /[MatMul_503]\n",
      "Ignore MatMul due to non constant B: /[MatMul_505]\n",
      "Ignore MatMul due to non constant B: /[MatMul_755]\n",
      "Ignore MatMul due to non constant B: /[MatMul_757]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1092]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1094]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1345]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1347]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1598]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1600]\n"
     ]
    }
   ],
   "source": [
    "model_quant2 = '/root/paper_project/LightWeight/onnx_model2.quant.onnx'\n",
    "quantize_dynamic(model_fp32, model_quant2, weight_type=QuantType.QUInt8, nodes_to_exclude=['Conv_18_quant']) #Qint -> QUInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# ONNX 모델 불러오기\n",
    "# onnx_model = onnx.load('onnx_model.quant.onnx')\n",
    "ort_session = ort.InferenceSession('onnx_model2.quant.onnx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplemented",
     "evalue": "[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name 'Conv_18_quant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplemented\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/root/paper_project/LightWeight/onnx_convert.ipynb Cell 13\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737573706963696f75735f68656973656e62657267227d/root/paper_project/LightWeight/onnx_convert.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39monnxruntime\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mort\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737573706963696f75735f68656973656e62657267227d/root/paper_project/LightWeight/onnx_convert.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# ONNX 모델 불러오기\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737573706963696f75735f68656973656e62657267227d/root/paper_project/LightWeight/onnx_convert.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# onnx_model = onnx.load('onnx_model.quant.onnx')\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737573706963696f75735f68656973656e62657267227d/root/paper_project/LightWeight/onnx_convert.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m ort_session \u001b[39m=\u001b[39m ort\u001b[39m.\u001b[39;49mInferenceSession(\u001b[39m'\u001b[39;49m\u001b[39monnx_model.quant.onnx\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:419\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m disabled_optimizers \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mdisabled_optimizers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisabled_optimizers\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_inference_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    420\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    421\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:463\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    460\u001b[0m     disabled_optimizers \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(disabled_optimizers)\n\u001b[1;32m    462\u001b[0m \u001b[39m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m sess\u001b[39m.\u001b[39;49minitialize_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    465\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess \u001b[39m=\u001b[39m sess\n\u001b[1;32m    466\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess_options \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess\u001b[39m.\u001b[39msession_options\n",
      "\u001b[0;31mNotImplemented\u001b[0m: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name 'Conv_18_quant'"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# ONNX 모델 불러오기\n",
    "# onnx_model = onnx.load('onnx_model.quant.onnx')\n",
    "ort_session = ort.InferenceSession('onnx_model.quant.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 296])\n",
      "<built-in method type of Tensor object at 0x7f772ca3dd60>\n"
     ]
    }
   ],
   "source": [
    "print(img_batch.shape)\n",
    "print(t.shape)\n",
    "print(char.shape)\n",
    "print(char.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[[-0.81995976, -1.9307777 ,  0.53090143, ...,  1.081383  ,\n",
      "           3.168384  , -0.8220036 ],\n",
      "         [-1.1471047 , -1.2511568 , -0.42478916, ..., -0.6691909 ,\n",
      "          -1.5015309 ,  0.40690026],\n",
      "         [-1.605803  ,  1.2688202 , -0.9642651 , ...,  0.6198112 ,\n",
      "           1.6059572 , -1.05593   ],\n",
      "         ...,\n",
      "         [-1.4578067 ,  0.43323582, -1.0288976 , ..., -1.4706377 ,\n",
      "          -0.33515826,  0.49414203],\n",
      "         [-1.5940886 ,  0.3505332 , -0.1956345 , ...,  0.12081496,\n",
      "          -0.485778  , -0.6566819 ],\n",
      "         [ 0.40568703, -1.7919124 , -0.65647477, ..., -0.9184193 ,\n",
      "          -0.80251867, -1.4198717 ]]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_data = {\n",
    "    \"x\" : img_batch.cpu().numpy(),\n",
    "    \"t\" : t.cpu().numpy(),\n",
    "    \"char\" : char.cpu().numpy()\n",
    "}\n",
    "output = ort_session.run(None, input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPU'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'graph' from 'onnx.tools' (/usr/local/lib/python3.8/dist-packages/onnx/tools/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/root/paper_project/hojun/light_weight/onnx_convert.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737573706963696f75735f68656973656e62657267227d/root/paper_project/hojun/light_weight/onnx_convert.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39monnx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m graph\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737573706963696f75735f68656973656e62657267227d/root/paper_project/hojun/light_weight/onnx_convert.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# onnx graph 추출\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737573706963696f75735f68656973656e62657267227d/root/paper_project/hojun/light_weight/onnx_convert.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m graph_def \u001b[39m=\u001b[39m onnx_model\u001b[39m.\u001b[39mgraph\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'graph' from 'onnx.tools' (/usr/local/lib/python3.8/dist-packages/onnx/tools/__init__.py)"
     ]
    }
   ],
   "source": [
    "from onnx.tools import graph\n",
    "\n",
    "# onnx graph 추출\n",
    "graph_def = onnx_model.graph\n",
    "\n",
    "# 그래프 시각화\n",
    "graph.plot_graph(graph_def, node_names = True) # 이유는 모르지만 시각화가 안됨\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35085/1737570757.py:22: DeprecationWarning: Use build_serialized_network instead.\n",
      "  engine = builder.build_engine(network=network,config=builder_config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/05/2023-01:10:24] [TRT] [W] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.4.0\n",
      "[10/05/2023-01:10:45] [TRT] [W] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.4.0\n",
      "[10/05/2023-01:10:45] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[10/05/2023-01:10:45] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n"
     ]
    }
   ],
   "source": [
    "import tensorrt \n",
    " \n",
    "onnx_file_name = '/home/wonguk/coding/paper_project/hojun/light_weight/onnx_model.onnx'\n",
    "tensorrt_file_name = 'tensorrt.plan'\n",
    "fp_16_mode = True\n",
    "TRT_LOGGER = tensorrt.Logger(tensorrt.Logger.WARNING)\n",
    "EXPLICIT_BATCH = 1 << (int)(tensorrt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    " \n",
    "builder = tensorrt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(EXPLICIT_BATCH)\n",
    "parser = tensorrt.OnnxParser(network, TRT_LOGGER)\n",
    "builder_config = builder.create_builder_config()\n",
    "# builder_config.max_workspace_size = (1<<30)\n",
    "# builder_config.set_memory_pool_limit =(1<<30)\n",
    "# builder.fp16_mode = fp16_mode\n",
    " \n",
    "with open(onnx_file_name, 'rb') as model:\n",
    "    if not parser.parse(model.read()):\n",
    "        for error in range(parser.num_errors):\n",
    "            print (parser.get_error(error))\n",
    " \n",
    "engine = builder.build_engine(network=network,config=builder_config)\n",
    "buf = engine.serialize()\n",
    "with open(tensorrt_file_name, 'wb') as f:\n",
    "    f.write(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import wandb\n",
    "import torch, torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from modules.diffusion import Diffusion\n",
    "from modules.utils import plot_images, test_save_images,make_stroke,stroke_to_char\n",
    "from models.utils import UNet\n",
    "from modules.utils import CharAttar\n",
    "batch_size = 8 #####\n",
    "sampleImage_len = 36\n",
    "\n",
    "\n",
    "num_classes = 420\n",
    "input_length = 100\n",
    "contents_dim = 100\n",
    "input_size = 64\n",
    "mode = \"new\"\n",
    "folder_name =\"test_3\"\n",
    "\n",
    "train_dirs = '/home/wonguk/coding/paper_project/hojun/data/Hangul_Characters_Image64_radomSampling420_GrayScale'\n",
    "sample_img_path = '/home/wonguk/coding/paper_project/hojun/data/62570_갊.png'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wandb.init(project=\"onnx_sampling\", config={\n",
    "                \"learning_rate\": 0.0003,\n",
    "                \"architecture\": \"UNET\",\n",
    "                \"dataset\": \"HOJUN_KOREAN_FONT64\",\n",
    "                \"notes\":\"content, yes_stoke, non_style/ 64 x 64, 420 dataset\"\n",
    "                },\n",
    "            name = \"self-attetnion onnx_sampling 나눔손글씨강인한위로_갊\") #####\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(0)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # model = UNet().to(device)\n",
    "    # ckpt = torch.load(\"/home/hojun/Documents/code/Kofont5/KoFont-Diffusion2/hojun/results/models/font_noStrokeStyle_Unet64_image420_3/ckpt_290.pt\")\n",
    "    # model.load_state_dict(ckpt)\n",
    "\n",
    "    diffusion = Diffusion(first_beta=1e-4,\n",
    "                              end_beta=0.02,\n",
    "                              noise_step=1000,\n",
    "                              beta_schedule_type='linear',\n",
    "                              img_size=input_size,\n",
    "                              device=device)\n",
    "    \n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        # torchvision.transforms.Resize((input_size,input_size)),\n",
    "        torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    dataset = torchvision.datasets.ImageFolder(train_dirs,transform=transforms)\n",
    "\n",
    "    # test set\n",
    "    n = range(0,len(dataset),100)\n",
    "    dataset = Subset(dataset, n)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,num_workers=12)\n",
    "\n",
    "    #sample_img\n",
    "    sample_img = Image.open(sample_img_path)\n",
    "    sample_img = transforms(sample_img).to(device)\n",
    "    sample_img = torch.unsqueeze(sample_img,1)\n",
    "    sample_img = sample_img.repeat(sampleImage_len, 1, 1, 1)\n",
    "    \n",
    "    if mode == \"random\":\n",
    "        contents_emb = torch.zeros(input_length,contents_dim)\n",
    "\n",
    "        first= [random.randint(0,18) for _ in range(input_length)]\n",
    "        middle = [random.randint(19,39) for _ in range(input_length)]\n",
    "        last = [random.randint(40,67) for _ in range(input_length)]\n",
    "\n",
    "        strokes = torch.Tensor([[0 for _ in range(68)] for _ in range(input_length)])\n",
    "\n",
    "        for idx in range(input_length):\n",
    "            strokes[idx][first[idx]], strokes[idx][middle[idx]], strokes[idx][last[idx]] = 1, 1, 1\n",
    "        char_list = stroke_to_char(strokes)\n",
    "\n",
    "        style_emb = torch.zeros(input_length,12288)\n",
    "\n",
    "        y = torch.cat([contents_emb, strokes, style_emb], dim=1).to(device)\n",
    "        x = diffusion.test_sampling(model, input_length, y, cfg_scale=3)\n",
    "\n",
    "    elif mode == \"manual\":\n",
    "        char_list = ['가,나,다,라,마,바,사,아,자,차,카,타,파,하']\n",
    "        contents_emb = torch.zeros(input_length, contents_dim)\n",
    "        strokes = make_stroke(char_list)\n",
    "        style_emb = torch.zeros(input_length, 12288)\n",
    "        y = torch.cat([contents_emb, strokes, style_emb], dim=1).to(device)\n",
    "        x = diffusion.test_sampling(model,len(strokes), y, cfg_scale=3)\n",
    "        \n",
    "    elif mode == \"new\":\n",
    "        charAttar = CharAttar(num_classes=num_classes,device=device)\n",
    "        sampled_images = diffusion.portion_sampling(model, n=len(dataset.dataset.classes),sampleImage_len = sampleImage_len,dataset=dataset,mode =mode,charAttar=charAttar,sample_img=sample_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorrt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
